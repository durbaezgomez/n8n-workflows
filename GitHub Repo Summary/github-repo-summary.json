{
  "nodes": [
    {
      "parameters": {
        "model": "claude-3-5-sonnet-20241022",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "typeVersion": 1.2,
      "position": [
        1580,
        320
      ],
      "id": "98f19f79-373c-43f7-a2f1-b99f5db049fb",
      "name": "Anthropic Chat Model1"
    },
    {
      "parameters": {
        "schemaType": "manual",
        "inputSchema": "{\n\t\"primaryTechnologies\": \"list\",\n  \"features\": \"list\",\n  \"use_cases\": \"list\"\n}"
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "typeVersion": 1.2,
      "position": [
        1740,
        320
      ],
      "id": "2f1706b6-2813-4199-a66e-01cb8923b7db",
      "name": "Structured Output Parser"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "cdc04017-0111-4fe9-96c5-2fd8818ed584",
              "name": "repo",
              "value": "={{ $('set_params').last().json.github_repo_name }}",
              "type": "string"
            },
            {
              "id": "92856169-1cab-48a8-b8f5-f3aac94c9b7b",
              "name": "owner",
              "value": "={{ $('set_params').last().json.github_repo_owner }}",
              "type": "string"
            },
            {
              "id": "58b3ede9-828d-4dd8-b3a4-878aaa14d714",
              "name": "url",
              "value": "={{ $('trigger_input').item.json.repo_data.html_url }}",
              "type": "string"
            },
            {
              "id": "d4d6c309-f714-43ed-bad5-9cc996f165a2",
              "name": "github_description",
              "value": "={{ $('set_params').last().json.github_description }}",
              "type": "string"
            },
            {
              "id": "53540cc7-cc88-45e9-a49c-d58276cdb89b",
              "name": "use_cases",
              "value": "={{ $json.output.useCases }}",
              "type": "array"
            },
            {
              "id": "b95cca12-0f5a-47d0-a4dd-cf8a5d186b89",
              "name": "primary_technologies",
              "value": "={{ $json.output.primaryTechnologies }}",
              "type": "array"
            },
            {
              "id": "11e8d2f2-c91b-4ef1-b373-2ea721fa0f6b",
              "name": "features",
              "value": "={{ $json.output.features }}",
              "type": "array"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        1940,
        140
      ],
      "id": "79946939-69ef-42c4-aad9-873419252cd4",
      "name": "set_repo_info_variables"
    },
    {
      "parameters": {
        "batchSize": "={{ $('trigger_input').item.json.config.batch_size }}",
        "options": {}
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        800,
        160
      ],
      "id": "c167a7d4-f1e5-49ea-b0e0-f7905f4bbc38",
      "name": "Loop Over Items"
    },
    {
      "parameters": {
        "model": "claude-3-5-sonnet-20241022",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "typeVersion": 1.2,
      "position": [
        1240,
        620
      ],
      "id": "c8e3aebd-78e0-4385-a0d8-05c22839a685",
      "name": "Anthropic Chat Model"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": false,
            "leftValue": "",
            "typeValidation": "loose",
            "version": 2
          },
          "conditions": [
            {
              "id": "f1c2dd07-96d5-429c-8537-cdfeb4c7d373",
              "leftValue": "={{ $json.fileContents }}",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "notEmpty",
                "singleValue": true
              }
            },
            {
              "id": "d453c8eb-af00-4bd5-b594-ebd54acb6e5d",
              "leftValue": "={{ $json.fileName }}",
              "rightValue": "=/(\\.(log\\d*|md|txt|csv|json|yaml|yml|xml|conf|config|ini|env|lock|bak|backup|cache|temp|tmp|dist|min|map|pot|po|mo|properties|toml|markdown|rst|adoc|asciidoc|wiki|dtd|plist|strings|resx|xaml|nfo|inf|cfg|cnf|diz|wri|rtf|sub|srt|ass|ssa|vtt|err|out|aux|bbl|blg|toc|lof|lot|run\\.xml|synctex\\.gz|fls|fdb_latexmk|snm|nav|vrb|dvi|prefs|opts|meta|manifest|MF|SF|DSA|sql|dump|tsv|tab|dat|info|release|me|1st|install|dox|egg-info|whl|lint|local|example|sample|template|docs?|contributing|changelog|license|authors|contributors|acknowledgments|coc|security|support|maintainers|governance|roadmap|backlog|todo|fixme|i18n|l10n|locales?|messages|translations?|specs?|tests|py|pyc|pyo|pyd|pyw|pyx|pxd|pxi|ipynb|js|jsx|mjs|cjs|ts|tsx|java|class|jar|war|ear|c|h|cpp|hpp|cc|hh|cxx|hxx|cs|vb|fs|fsx|fsi|fsscript|rb|rbw|rake|gemspec|php|phtml|php[3-7]|go|mod|sum|rs|rlib|swift|kt|kts|scala|sc|dart|elm|ex|exs|erl|hrl|hs|lhs|lua|pl|pm|t|pod|r|rdata|rds|rmd|sh|bash|zsh|fish|sql|ddl|dml|html?|xhtml|htm|css|scss|sass|less|styl|vue|svelte|wasm|wat|graphql|gql|plist|xcodeproj|pbxproj|gradle|androidmanifest|git(ignore|attributes|modules)|docker(file|ignore)|cmake|make|mk|vim|nvim|eslintrc|prettierrc|stylelintrc|babel|webpack|rollup|parcel|tf|tfstate|tfvars|y[a]?ml|sublime-[^.]+|vscode|idea|spec|test|mock|fixture|snap|apib|swagger|openapi|mdx|asciidoc|adoc|package\\.json|package-lock\\.json|composer\\.json|composer\\.lock|gemfile|gemfile\\.lock|pipfile|pipfile\\.lock|cargo\\.toml|cargo\\.lock|pom\\.xml|log|pid|sock|patch|diff|bak|backup|old|new|orig|min|source\\.map|dev|prod|staging|test))$/ig",
              "operator": {
                "type": "string",
                "operation": "regex"
              }
            },
            {
              "id": "0c5944dd-0a94-474e-b6ae-8f1d60ac71cc",
              "leftValue": "={{ $json.fileContents.length }}",
              "rightValue": 5000,
              "operator": {
                "type": "number",
                "operation": "lt"
              }
            },
            {
              "id": "cfa89fb4-31ce-40f3-8141-e99b750ea7d9",
              "leftValue": "={{ $json.fileName }}",
              "rightValue": ".gitignore",
              "operator": {
                "type": "string",
                "operation": "notEquals"
              }
            }
          ],
          "combinator": "and"
        },
        "looseTypeValidation": true,
        "options": {
          "ignoreCase": true
        }
      },
      "type": "n8n-nodes-base.filter",
      "typeVersion": 2.2,
      "position": [
        980,
        -80
      ],
      "id": "4a00a161-f5e6-45bd-830a-17e8042ce777",
      "name": "filter_files"
    },
    {
      "parameters": {
        "maxItems": "={{ $('trigger_input').item.json.config.limit || 100000 }}"
      },
      "type": "n8n-nodes-base.limit",
      "typeVersion": 1,
      "position": [
        1180,
        -80
      ],
      "id": "ba40dd75-9d98-4b6d-8789-ddb27c656057",
      "name": "limit_files"
    },
    {
      "parameters": {
        "aggregate": "aggregateAllItemData",
        "options": {}
      },
      "type": "n8n-nodes-base.aggregate",
      "typeVersion": 1,
      "position": [
        1100,
        140
      ],
      "id": "bdc77819-320c-4427-9200-fb3be5ec1c48",
      "name": "aggregate_summaries"
    },
    {
      "parameters": {
        "aggregate": "aggregateAllItemData",
        "options": {}
      },
      "type": "n8n-nodes-base.aggregate",
      "typeVersion": 1,
      "position": [
        860,
        400
      ],
      "id": "73be68d6-18e6-4f05-8e3c-1c6bb7dca78b",
      "name": "aggregate_files"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "93ebcd15-3a9e-422b-9da0-99fcd649fc91",
              "name": "github_repo_owner",
              "value": "={{ $('trigger_input').item.json.repo_data.owner.login }}",
              "type": "string"
            },
            {
              "id": "0f597cbb-8f0c-4a2d-923d-685a91e14c9a",
              "name": "github_repo_name",
              "value": "={{ $('trigger_input').item.json.repo_data.name }}",
              "type": "string"
            },
            {
              "id": "165784cf-b822-47ec-83f4-074fb01660c3",
              "name": "github_description",
              "value": "={{ $('trigger_input').item.json.repo_data.description }}",
              "type": "string"
            },
            {
              "id": "63e080b3-ff97-49bf-883d-18d59b34f81f",
              "name": "github_topics",
              "value": "={{ $('trigger_input').item.json.repo_data.topics }}",
              "type": "array"
            },
            {
              "id": "34a70beb-e4a4-4604-9757-a0d40390a5ee",
              "name": "files",
              "value": "={{ $json.data }}",
              "type": "array"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        1060,
        400
      ],
      "id": "63750753-61cb-45c7-a012-0896ddb4ff5d",
      "name": "set_params"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=```{{ JSON.stringify($json.files) }}```",
        "messages": {
          "messageValues": [
            {
              "message": "=### MAIN GOAL  \n\nYou are an advanced assistant designed to analyze the structure and content of repositories. Your task is to summarize the key information about repositories provided in JSON format. Each input includes a batch of files with their fileName and file content. It also includes the repo name, repo owner, github description and github topics to help you determine what system are the attached files part of.   \n\nFor context, this is the general repository information:\nrepository name: {{ $json.github_repo_name }}\nrepository owner: {{ $json.github_repo_owner }}\ngithub description: {{ $json.github_description }}\ngithub topics: {{ $json.github_topics }}\n\n### SPECIFIC INSTRUCTIONS  \n\nFor each batch of files, perform the following: \n\n1.\tIdentify Programming Languages and Technologies: \t\n\n•\tDetermine the primary programming languages and frameworks used based on the file extensions and filenames.\n•\tRecognize specific tools or technologies mentioned in the filenames (e.g., Dockerfile, package.json, requirements.txt). \n\n2.\tDescribe functionalities these files handle: \n\n•\tAnalyze the files (e.g., README, main.py, index.js, or app.py) to infer the primary purpose of the project. \t\n\n3.\tHighlight Features: \t\n\n•\tList significant features or capabilities the project offers, such as “API integration”, “real-time analytics”, or “customizable dashboards”. \t\n•\tUse filenames and recognized keywords to infer features (e.g., a tests folder suggests the presence of unit testing).  \n\n4. Omit specific technology versions, just general names that can be used as metadata to identify similar repositories.\n\n### INPUT JSON EXAMPLE:   \n\n```   { \"files\": [       {         \"fileName\": \"string\",          \"fileContents\": \"string\"       }     ]   } ```  \n\n\n### OUTPUT JSON EXAMPLE:  \n\n```{    \"primary_technologies\": [\"Python\", \"Docker\"],   \"goals\": \"Build a web application using Python and containerize it for deployment.\",   \"features\": \"unit testing, dependency management with `requirements.txt`, Docker support\"  }  \n\n{   \"primary_technologies\": [\"Python\"],   \"goals\": \"Automate data scraping tasks and save the results in a CSV file\",   \"features\": \"configurable scraper, saving structured data\" } ```\n\n\n### \nEnsure the summary is concise but informative. Focus on helping a developer or team quickly understand the repository’s purpose and capabilities. "
            }
          ]
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.5,
      "position": [
        1240,
        400
      ],
      "id": "a00962a5-ee14-4f4d-89de-7ac1c5bdc6ce",
      "name": "summarize_batch"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ JSON.stringify($json) }}",
        "hasOutputParser": true,
        "messages": {
          "messageValues": [
            {
              "message": "=### MAIN GOAL\nYou are a specialized assistant trained to analyze and summarize information extracted from GitHub repositories. Your task is to create concise, cohesive, and professional descriptions of repositories based on batches of input data. The input data contains detailed information about the repository, including:\n\n\t•\tRepository name and owner\n\t•\tGitHub description\n\t•\tGitHub repository topics\n\t•\tExtracted information about technologies used, features of the code, and potential goals of the project\n\nYour objective is to produce a well-structured description that clearly communicates the repository’s purpose, primary technologies, key features, and potential use cases. Ensure your response is concise, easy to read, and professionally formatted.\n\n\n### GUIDELINES\n\n\t1.\tCombine and Summarize:\n\t•\tAggregate the input data into a single, cohesive description.\n\t•\tPrioritize clarity and avoid repeating similar information.\n\t2.\tHighlight Key Elements:\n\t•\tFocus on the primary technologies used in the repository.\n\t•\tClearly articulate the goals and purpose of the project.\n\t•\tList significant features and potential use cases.\n\n\t3.\tMaintain Professional Tone:\n\t•\tUse clear and concise language.\n\t•\tAvoid overly technical jargon unless it is essential to describe the project.\n\t4.\tOutput Format:\n\t•\tName: The repository name.\n\t•\tDescription: A short summary of the repository’s purpose and goals.\n\t•\tPrimary Technologies: A list of key programming languages, frameworks, or tools. Include here these github topics if there are any: ```github topics: {{ $('set_params').last().json.github_topics }}```. Make sure to avoid duplicating items.\n\t•\tFeatures: A summary of the repository’s main features.\n\t•\tUse Cases: Potential applications or users of the repository.\n\n### EXAMPLES\n\nExample Output:\n`{\n  \"name\": \"bittensor\",\n  \"description\": \"A decentralized framework for building internet-scale neural networks.\",\n  \"primaryTechnologies\": [\n    \"Python\",\n    \"PyTorch\",\n    \"Substrate\",\n    \"Blockchain\"\n  ],\n  \"features\": [\n    \"Peer-to-peer network for training AI models\",\n    \"Support for cryptographic consensus\",\n    \"Interoperability with Polkadot\"\n  ],\n  \"useCases\": [\n    \"Large-scale machine learning\",\n    \"Decentralized AI training\",\n    \"Cryptocurrency-backed neural networks\"\n  ]\n}`\n\n\tGenerate summaries in this format and ensure the result provides a complete understanding of the repository’s value and functionality based on the provided data."
            }
          ]
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.5,
      "position": [
        1580,
        140
      ],
      "id": "9c0f09d4-c254-45c8-910b-892c6eadda82",
      "name": "summarize_repo"
    },
    {
      "parameters": {
        "jsCode": "return $input.all().map((item) => {\n  return { fileName: item.json.fileName,\n          fileContents: item.json.fileContents\n            .replaceAll('\\n','')\n            .replaceAll('#','')\n            .replaceAll('**','')\n         }\n})"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1420,
        -80
      ],
      "id": "b4e8c267-8ae3-449d-9fb1-f8e9acc7efe1",
      "name": "optimize_tokens"
    },
    {
      "parameters": {
        "inputSource": "jsonExample",
        "jsonExample": "  {\n    \"files\": [\n      {\n        \"fileName\": \"manifold-inc-targon-a854502\",\n        \"fileContents\": \"\"\n      },\n      {\n        \"fileName\": \".gitignore\",\n        \"fileContents\": \"*.egg-info\\n__pycache__/\\ntest.py\\ntest_stream.py\\n.venv\\n.env\\n\\n!hub-proxy/docker-compose.yml\\n.DS_Store\\n\\noutput.txt\\nplay.py\\npackages\\n*.pickle\\n*.json\\n*.npy\\nmodels.txt\\n\"\n      },\n      {\n        \"fileName\": \".python-version\",\n        \"fileContents\": \"3.10.12\\n\"\n      },\n      {\n        \"fileName\": \"CHANGELOG.md\",\n        \"fileContents\": \"# 4.4.0\\n\\n- Bittensor 8.5.1\\n  - Enables CR3\\n- Organics speed based scoring\\n  - Organics are now scored based on tps.\\n  - Organics are now sent back to jugo once scored\\n\\n# 4.3.0\\n\\n- Model Rotation\\n  - Validators can now host any number of models. These are rotated out each\\n    interval randomly. Valis are garunteed to send atleast 1 verifiable request\\n    every 3 ticks. Validators are now only limited to the largest model they can\\n    run, not the number of models.\\n- Dask -> Datasets\\n  - Datasets provides huggingface caching for the synthetic dataset. Vali\\n    startup time after warmup reduced from ~5 min to \\\\< 5 seconds. (not\\n    including model rotation time)\\n- Heartbeat\\n  - Validators now self-monitor for system hangs. If no requests are\\n    successfully sent within 5 mintues, validators will auto-restart. This is\\n    opt-in, and can be enabled with an env file. See readme for details, under\\n    the `Validator .env` header.\\n- Bittensor 8.4.5\\n  - Bumped bittensor version to help with stability when connecting to the chain\\n    and getting metagraph / posting weights\\n\"\n      },\n      {\n        \"fileName\": \"LICENSE\",\n        \"fileContents\": \"# The MIT License (MIT)\\n# Copyright © 2021 Yuma Rao\\n# Copyright © 2023 Manifold Labs\\n\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the “Software”), to deal in the Software without restriction, including without limitation\\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\\n# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\\n# the Software.\\n\\n# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\\n# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\\n# DEALINGS IN THE SOFTWARE.\"\n      },\n      {\n        \"fileName\": \"README.md\",\n        \"fileContents\": \"# Targon: A Deterministic Verification of Large Language Models\\n\\nTargon (Bittensor Subnet 4) is a deterministic verification mechanism that is\\nused to incentivize miners to run openai compliant endpoints and serve synthetic\\nand organic queries.\\n\\nNOTICE: Using this software, you must agree to the Terms and Agreements provided\\nin the terms and conditions document. By downloading and running this software,\\nyou implicitly agree to these terms and conditions.\\n\\n# Table of Contents\\n\\n1. [Compute Requirements](#recommended-compute-requirements)\\n1. [Installation](#installation)\\n   - [Install PM2](#install-pm2)\\n   - [Install Targon](#install-targon-on-your-machine)\\n1. [How to Run Targon](#how-to-run-targon)\\n   - [Running VLLM](#vllm)\\n   - [Running a Miner](#running-a-miner)\\n   - [Running a Validator](#running-a-validator)\\n1. [What is a Deterministic Verification Network?](#what-is-a-deterministic-verification-network)\\n   - [Role of a Miner](#role-of-a-miner)\\n   - [Role of a Validator](#role-of-a-validator)\\n1. [Features of Targon](#features-of-targon)\\n   - [Full OpenAI Compliance](#full-openai-compliance)\\n   - [Targon-Hub](#targon-hub)\\n1. [How to Contribute](#how-to-contribute)\\n\\n# Recommended Compute Requirements\\n\\nFor validators we recommend a 8xA100, although a 1xA100 could also be used. We\\nplan on focusing on bringing these costs down in the coming updates.\\n\\nFor miners, A100 or H100s are common choices. Benchmarking is up to the miner to\\ndetermine what GPU works best for their optimizations.\\n\\n#### Minimum Viable Compute Recommendations\\n\\n- **VRAM:** 80 GB\\n- **Storage:** 200 GB\\n- **RAM:** 16 GB\\n- **CPU**: 4 core\\n\\n# Installation\\n\\n## Overview\\n\\nIn order to run Targon, you will need to install PM2 and the Targon package. The\\nfollowing instructions apply only to Ubuntu OSes. For your specific OS, please\\nrefer to the official documentation.\\n\\n### Install PM2 on your machine\\n\\n#### Download NVM\\n\\nTo install or update nvm, you should run the install script. To do that, you may\\neither download and run the script manually, or use the following cURL or Wget\\ncommand:\\n\\n```bash\\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash\\n```\\n\\n#### Add NVM to bash profile\\n\\nRunning either of the above commands downloads a script and runs it. The script\\nclones the nvm repository to ~/.nvm, and attempts to add the source lines from\\nthe snippet below to the correct profile file (~/.bash_profile, ~/.zshrc,\\n~/.profile, or ~/.bashrc).\\n\\n```bash\\nexport NVM_DIR=\\\"$([ -z \\\"${XDG_CONFIG_HOME-}\\\" ] && printf %s \\\"${HOME}/.nvm\\\" || printf %s \\\"${XDG_CONFIG_HOME}/nvm\\\")\\\"\\n[ -s \\\"$NVM_DIR/nvm.sh\\\" ] && \\\\. \\\"$NVM_DIR/nvm.sh\\\" # This loads nvm\\n```\\n\\n#### Install Node\\n\\n```bash\\nnvm install node\\n```\\n\\n#### Install PM2\\n\\n```bash\\nnpm install pm2@latest -g\\n```\\n\\nYou have now installed PM2.\\n\\n### Install Targon on your machine\\n\\n#### Clone the repository\\n\\n```bash\\ngit clone https://github.com/manifold-inc/targon.git\\ncd targon\\n```\\n\\n#### Install dependencies\\n\\n```bash\\npython3 -m pip install -e .\\n```\\n\\nYou have now installed Targon. You can now run a validator or a miner.\\n\\n# How to Run Targon\\n\\n## Running a Miner\\n\\nBefore starting or registering your miner in Targon, first you will want to run\\nVLLM serving different images validators are requesting. You can find a list at\\nhttps://stats.sybil.com/stats/validator under the live tab. The more models you\\nrun, the higher your incentive.\\n\\nVLLM is the recommended engine, however it is not required. If you are using\\nVLLM, make sure yo include the `--return-tokens-as-token-ids` flag, or else your\\nresponses will fail.\\n\\nOnce you have one (or multiple) models running, modify the default miner code to\\nproxy to the proper VLLM instance on each request. Verifiers will include the\\n`X-Targon-Model` header so that the miner node does not need to parse the actual\\nbody.\\n\\nIn the `miner.py` script you will find a function called `list_models`. To serve\\nmultiple models you must:\\n\\n1. Fill this out to respond to validators with any model you currently have\\n   available (below is an example):\\n\\n```\\n    async def list_models(self):\\n        return [\\n            \\\"ExampleName/Meta-Llama-3.1-8B-Instruct\\\",\\n            \\\"ExampleName/mythomax-l2-13b\\\",\\n            \\\"ExampleName/Hermes-3-Llama-3.1-8B\\\",\\n            \\\"ExampleName/Nxcode-CQ-7B-orpo\\\",\\n            \\\"ExampleName/deepseek-coder-33b-instruct\\\",\\n            \\\"ExampleName/Llama-3.1-Nemotron-70B-Instruct-HF\\\",\\n        ]\\n```\\n\\n2. Update the `create_chat_completion` and `create_completion` methods in\\n   neurons/miner.py to route to the appropriate vllm upstream server based on\\n   the model (which is either in the headers or from the request payload's model\\n   param)\\n\\nHere is a hint / incomplete code snippet to get you started:\\n\\n```\\nmodel_port_map = {\\n    'ExampleName/mythomax-l2-13b': 1001,\\n    'ExampleName/Hermes-3-Llama-3.1-8B': 1002,\\n    'ExampleName/Nxcode-CQ-7B-orpo': 1003,\\n    'ExampleName/deepseek-coder-33b-instruct': 1004,\\n    'ExampleName/Llama-3.1-Nemotron-70B-Instruct-HF': 1005\\n}\\nfull_url = f\\\"http://127.0.0.1:{model_port_map.get(body.get('model'), 1000)}{path}\\\"\\n```\\n\\nOnce this is complete, you are ready to continue starting your miner node.\\n\\n### PM2\\n\\nRunning a miner through PM2 will require the vLLM instance to be running.\\n\\n```bash\\npm2 start neurons/miner.py --name miner --interpreter  python3 -- --wallet.name [WALLET_NAME] --netuid 4 --wallet.hotkey [WALLET_HOTKEY] --subtensor.network finney --model-endpoint [MODEL_ENDPOINT] --api_key [API_KEY] --axon.port [AXON PORT] --logging.trace\\n```\\n\\n> Please replace the following with your specific configuration:\\n>\\n> - \\\\[WALLET_NAME\\\\]\\n> - \\\\[WALLET_HOTKEY\\\\]\\n> - \\\\[MODEL_ENDPOINT\\\\]\\n> - \\\\[API_KEY\\\\]\\n> - \\\\[AXON_PORT\\\\]\\n\\nNOTE: Trace logging is very verbose. You can use `--logging.info` instead for\\nless log bloat.\\n\\nAdditionally:\\n\\n```bash\\n--no-force-validator-permit [TRUE/FALSE]\\n\\n```\\n\\nis defaulted to false to force incoming requests to have a permit. Set this to\\ntrue if you are having trouble getting requests from validators on the 'test'\\nnetwork.\\n\\n## Running a Validator\\n\\n### PM2\\n\\nValidators are simply run through pm2, enabling auto restarts and auto updates.\\nA validator should be run on atleast an A100, but the larger the better, as\\nlarger clusters can handle more models. The machine should have\\n[nvidia-smi / cuda](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu)\\ninstalled along with [docker](https://docs.docker.com/engine/install/ubuntu/).\\n\\n**No vllm instance needed**\\n\\nValidator Instance:\\n\\n```bash\\npm2 start neurons/validator.py --name validator --interperter python3 -- --wallet.name [WALLET_NAME]\\n\\n```\\n\\n> Please replace the following with your specific configuration:\\n>\\n> - \\\\[WALLET_NAME\\\\]\\n\\n## Explanation of Args\\n\\n### Shared Args\\n\\n1. **--netuid** ==> Subnet Netuid. *Defaults to 4*\\n1. **--epoch-length** ==> Default epoch length (how often we set weights,\\n   measured in 12 second blocks). *Defaults to 360*\\n1. **--mock** ==> Mock neuron and all network components. *Defaults to False*\\n\\n### Miner Args\\n\\n1. **--neuron.name** ==> Trials for this neuron go in neuron.root/ (wallet-cold\\n   \\\\- wallet-hot) / neuron.name. *Defaults to miner*\\n1. **--force_validator.permit** ==> If set, forces incoming requests to have a\\n   permit. *Defaults to True*\\n1. **--model-endpoint** ==> Endpoint to use for the OpenAi CompatibleClient.\\n   *Defaults to \\\"http://127.0.0.1:8000/v1\\\"*\\n1. **--api-key** ==> API key for OpenAi Compatible API. *Defaults to \\\"12345\\\"*\\n\\n### Validator Args\\n\\n1. **--neuron.name** ==> Trials for this neuron go in neuron.root/ (wallet-cold\\n   \\\\- wallet-hot) / neuron.name. *Defaults to validator*\\n1. **--timeout** ==> The timeout for each forward call in seconds. *Defaults to\\n   8*\\n1. **--vpermit-tao-limit** ==> The maximum number of TAO allowed to query a\\n   validator with a permit. *Defaults to 4096*\\n1. **--cache-file** ==> Pickle file to save score cache to. *Defaults to\\n   cache.pickle*\\n1. **--database.url** ==> Database URL to save Miner Data to Targon Hub.\\n1. **--autoupdate-off** ==> Disable automatic updates to Targon on latest\\n   version on Main if set. *Defaults to True*\\n1. **--models.mode** ==> Mode to use for determining what models to run. Can be\\n   one of:`default`, or `config`.\\n   - `endpoint`: defaults to `https://targon.sybil.com/api/models`. This will\\n     mimic the manifold validator\\n   - `default`: only run NousResearch/Meta-Llama-3.1-8B-Instruct\\n   - `config`: parse a text file named `models.txt` with a list of models\\n     separated by newlines\\n1. **--models.endpoint** ==> Only used when models.mode is `endpoint`. Sets the\\n   api endpoint to ping for list of models. Defaults to targon hub.\\n\\n> Example model config file `models.txt`\\n>\\n> ```\\n> NousResearch/Meta-Llama-3.1-8B-Instruct\\n> NousResearch/Meta-Llama-3.1-70B-Instruct\\n> NousResearch/Meta-Llama-3.1-405B-Instruct\\n> ```\\n\\n## Validator .env\\n\\nSome more robust settings can be applied via a .env file. Eventually, targon\\naims to move all settings to a .env file instead of cli arguments. Currently,\\nthe following .env variables are supported with their defaults.\\n\\n```\\nAUTO_UPDATE=False # Turn off autoupdate. Overrides cli flag.\\nIMAGE_TAG=latest # Verifier image tag. Useful for testing new updates.\\nHEARTBEAT=False # Enable heartbeat. Requires pm2. Set to True to enable heartbeat monitoring.\\nIS_TESTNET=False # If validator should run in testnet.\\n```\\n\\n## Autoupdate\\n\\nAutoupdate is implemented in targon/utils.py. This is to ensure that your\\ncodebase matches the latest version on Main of the Targon Github Repository.\\n\\n### Validator Autoupdate\\n\\nValidator Autoupdate is implemented and defaulted to run once weights have been\\nset. To **disable**, please add the flag to your command line build:\\n\\n```bash\\npm2 start neurons/validator.py --name validator --interperter python3 -- --wallet.name [WALLET_NAME] --autoupdate-off\\n```\\n\\n### Miner Autoupdate\\n\\nMiner Autoupdate is **not** implemented. Miners will need to check the Targon\\nrepository and update themselves as new versions are released. If interested in\\nutilizing the autoupdate feature that Validators use, please follow the steps\\nbelow:\\n\\n*NOTE*: This will not be maintained by the Manifold Labs Team.\\n\\n1. Import the autoupdate function into your miner script (neurons/miner.py) at\\n   the top of the file.\\n\\n```python\\nfrom targon.updater import autoupdate\\n```\\n\\n3. Call the function at a place of your choosing.\\n\\n```python\\n    if self.config.autoupdate:\\n        autoupdate(branch=\\\"main\\\")\\n\\n```\\n\\n4. Relaunch your miner with the changes.\\n\\n# What is A Deterministic Verification Network\\n\\nValidators send queries to miners that are then scored for speed, and verified\\nby comparing the logprobs of the responses to a validators own model.\\n\\n## Role of a Miner\\n\\nA miner is a node that is responsible for generating a output from a query, both\\norganic and synthetic.\\n\\n## Role of a Validator\\n\\nA validator is a node that is responsible for verifying a miner's output. The\\nvalidator will send an openai compliant request to a miner with. The miner will\\nthen send back a response with the output. The validator will then use the log\\nprob values of the response to verify that each miners response is accurate.\\nValidators will keep score of each miners response time and use their averages\\nto assign scores each epoch. Specifically, miner scores are the sum of the\\naverage TPS per model.\\n\\n# Features of Targon\\n\\n## Full OpenAI Compliance\\n\\nValidators can query miners directly using any openai package, and Epistula\\nheaders. Below is boilerplate for querying a miner in python.\\n\\n```py\\nminer = openai.AsyncOpenAI(\\n    base_url=f\\\"http://{axon.ip}:{axon.port}/v1\\\",\\n    api_key=\\\"sn4\\\",\\n    max_retries=0,\\n    timeout=Timeout(12, connect=5, read=5),\\n    http_client=openai.DefaultAsyncHttpxClient(\\n        event_hooks={\\n            \\\"request\\\": [\\n                # This injects Epistula headers right before the request is sent.\\n                # wallet.hotkey is the public / private keypair\\n                #\\n                # You can find this function in the `epistula.py` file in \\n                # the targon repo\\n                create_header_hook(wallet.hotkey, axon.hotkey_ss58)\\n            ]\\n        }\\n    ),\\n)\\n```\\n\\n# How to Contribute\\n\\n## Code Review\\n\\nProject maintainers reserve the right to weigh the opinions of peer reviewers\\nusing common sense judgement and may also weigh based on merit. Reviewers that\\nhave demonstrated a deeper commitment and understanding of the project over time\\nor who have clear domain expertise may naturally have more weight, as one would\\nexpect in all walks of life. Where a patch set affects consensus-critical code,\\nthe bar will be much higher in terms of discussion and peer review requirements,\\nkeeping in mind that mistakes could be very costly to the wider community. This\\nincludes refactoring of consensus-critical code. Where a patch set proposes to\\nchange the Targon subnet, it must have been discussed extensively on the discord\\nserver and other channels, be accompanied by a widely discussed BIP and have a\\ngenerally widely perceived technical consensus of being a worthwhile change\\nbased on the judgement of the maintainers. That being said, Manifold welcomes\\nall PR's for the betterment of the subnet and Bittensor as a whole. We are\\nstriving for improvement at every interval and believe through open\\ncommunication and sharing of ideas will success be attainable.\\n\"\n      },\n      {\n        \"fileName\": \"TERMS.md\",\n        \"fileContents\": \"### Terms of Service for TARGON search\\n\\n**Last Updated: \\\\[01/29/2024\\\\]**\\n\\n#### Introduction\\n\\nWelcome to Targon subnet service (“Service”). These Terms of Service (“Terms”)\\ngovern your access to and use of our services, including any applications,\\nwebsites, software, and content provided on or through targon. By\\nusing our Service, you agree to be bound by these Terms. If you do not agree to\\nthese Terms, do not use our Service.\\n\\n#### 1. Acceptance of Terms\\n\\nBy creating an account or accessing or using our Service, you agree to be bound\\nby these Terms and all applicable laws and regulations.\\n\\n#### 2. Changes to Terms\\n\\nWe reserve the right to modify or replace these Terms at any time at our sole\\ndiscretion. We will provide notice of any changes by updating the date at the\\ntop of these Terms.\\n\\n#### 3. User Responsibilities\\n\\n- **3.1 Content Ownership**: You retain all rights and ownership of your data.\\n  We do not claim any ownership rights to the content you upload to the Service.\\n- **3.2 Acceptable Use**: You agree not to use the Service to store, transmit,\\n  or share any data that violates any applicable law, including data that is\\n  illegal, infringing, or defamatory. You also agree not to use the Service to\\n  store, transmit, or share any “sensitive” personal information, including\\n  protected health information covered under HIPAA, financial information\\n  covered under PCI, and any other information that would fall under the\\n  definition of “sensitive” or “special categories” of data under applicable\\n  law.\\n\\n#### 4. Prohibited Conduct\\n\\nYou agree not to engage in any of the following activities: (a) Violating any\\nlaws or regulations; (b) Infringing the intellectual property or other rights of\\nthird parties; (c) Transmitting viruses or harmful code; (d) Attempting to\\nbreach or compromise any security measures of the Service.\\n\\n#### 5. Termination\\n\\nWe may terminate or suspend your access to the Service immediately, without\\nprior notice or liability, if you breach the Terms.\\n\\n#### 6. Disclaimers\\n\\nThe Service is provided on an “AS IS” and “AS AVAILABLE” basis. We disclaim all\\nwarranties and representations, express or implied, including the implied\\nwarranties of merchantability and fitness for a particular purpose.\\n\\n#### 7. Limitation of Liability\\n\\nWe shall not be liable for any indirect, incidental, special, consequential, or\\npunitive damages, including loss of profits, data, or use, arising out of or in\\nconnection with these Terms or the Service, whether in an action of contract,\\ntort, or otherwise.\\n\\n#### 8. Indemnification\\n\\nYou agree to indemnify, defend, and hold harmless Manifold, its officers,\\ndirectors, employees, agents, licensors, suppliers, and any third-party\\ninformation providers from and against all losses, expenses, damages, and costs,\\nincluding reasonable attorneys' fees, resulting from any violation of these\\nTerms or any activity related to your account (including negligent or wrongful\\nconduct).\\n\\n#### 9. Governing Law\\n\\nThese Terms shall be governed and construed in accordance with the laws of your\\nlocal jurisdiction, without regard to its conflict of law provisions.\\n\\n#### 10. Contact Us\\n\\nIf you have any questions about these Terms, please contact us at\\n\\\\[operations@manifold.inc\\\\].\\n\\n\"\n      },\n      {\n        \"fileName\": \"VERSION\",\n        \"fileContents\": \"4.4.2\\n\"\n      },\n      {\n        \"fileName\": \"curl.txt\",\n        \"fileContents\": \"     time_namelookup:  %{time_namelookup}s\\\\n\\n        time_connect:  %{time_connect}s\\\\n\\n     time_appconnect:  %{time_appconnect}s\\\\n\\n    time_pretransfer:  %{time_pretransfer}s\\\\n\\n       time_redirect:  %{time_redirect}s\\\\n\\n  time_starttransfer:  %{time_starttransfer}s\\\\n\\n                     ----------\\\\n\\n          time_total:  %{time_total}s\\\\n\\n\"\n      },\n      {\n        \"fileName\": \"docker-compose.testnet.yml\",\n        \"fileContents\": \"services:\\n  miner-vllm:\\n    image: manifoldlabs/vllm-openai:latest\\n    runtime: nvidia\\n    command: --model NousResearch/Meta-Llama-3.1-8B-Instruct\\n    ports:\\n      - 9000:8000\\n    ipc: host\\n    volumes:\\n      - hf_cache:/root/.cache/huggingface\\n    deploy:\\n      resources:\\n        reservations:\\n          devices:\\n            - driver: nvidia\\n              device_ids:\\n                - \\\"1\\\"\\n              capabilities: [gpu]\\nvolumes:\\n  hf_cache:\\n\"\n      },\n      {\n        \"fileName\": \"extra\",\n        \"fileContents\": \"\"\n      },\n      {\n        \"fileName\": \"requirements.txt\",\n        \"fileContents\": \"\\nbittensor==7.1.2\\nopenai==1.44.1\\nnanoid==2.0.0\\n\"\n      },\n      {\n        \"fileName\": \"send_request_to_miner.py\",\n        \"fileContents\": \"from time import time\\nimport bittensor as bt\\nfrom typing import Any, List, Optional\\nfrom openai.types.chat import ChatCompletionMessageParam\\nfrom substrateinterface import Keypair\\nfrom bittensor.subtensor import Dict, Union\\nfrom httpx import Timeout\\nfrom openai import DefaultHttpxClient, OpenAI\\nfrom math import ceil\\nimport httpx\\nfrom hashlib import sha256\\nfrom uuid import uuid4\\nimport json\\n\\n\\ndef create_header_hook(hotkey, axon_hotkey, model):\\n    def add_headers(request: httpx.Request):\\n        for key, header in generate_header(hotkey, request.read(), axon_hotkey).items():\\n            request.headers[key] = header\\n        request.headers[\\\"X-Targon-Model\\\"] = model\\n\\n    return add_headers\\n\\n\\ndef generate_header(\\n    hotkey: Keypair,\\n    body: Union[Dict[Any, Any], List[Any], bytes],\\n    signed_for: Optional[str] = None,\\n) -> Dict[str, Any]:\\n    timestamp = round(time() * 1000)\\n    timestampInterval = ceil(timestamp / 1e4) * 1e4\\n    uuid = str(uuid4())\\n    req_hash = None\\n    if isinstance(body, bytes):\\n        req_hash = sha256(body).hexdigest()\\n    else:\\n        req_hash = sha256(json.dumps(body).encode(\\\"utf-8\\\")).hexdigest()\\n\\n    headers = {\\n        \\\"Epistula-Version\\\": str(2),\\n        \\\"Epistula-Timestamp\\\": str(timestamp),\\n        \\\"Epistula-Uuid\\\": uuid,\\n        \\\"Epistula-Signed-By\\\": hotkey.ss58_address,\\n        \\\"Epistula-Request-Signature\\\": \\\"0x\\\"\\n        + hotkey.sign(f\\\"{req_hash}.{uuid}.{timestamp}.{signed_for or ''}\\\").hex(),\\n    }\\n    if signed_for:\\n        headers[\\\"Epistula-Signed-For\\\"] = signed_for\\n        headers[\\\"Epistula-Secret-Signature-0\\\"] = (\\n            \\\"0x\\\" + hotkey.sign(str(timestampInterval - 1) + \\\".\\\" + signed_for).hex()\\n        )\\n        headers[\\\"Epistula-Secret-Signature-1\\\"] = (\\n            \\\"0x\\\" + hotkey.sign(str(timestampInterval) + \\\".\\\" + signed_for).hex()\\n        )\\n        headers[\\\"Epistula-Secret-Signature-2\\\"] = (\\n            \\\"0x\\\" + hotkey.sign(str(timestampInterval + 1) + \\\".\\\" + signed_for).hex()\\n        )\\n    return headers\\n\\n\\ndef make_client(miner_uid):\\n    wallet = bt.wallet()  # Set your wallet config\\n    subtensor = bt.subtensor()\\n    metagraph = subtensor.metagraph(4)\\n    axon_info = metagraph.axons[miner_uid]\\n    client = OpenAI(\\n        base_url=f\\\"http://{axon_info.ip}:{axon_info.port}/v1\\\",\\n        api_key=\\\"sn4\\\",\\n        max_retries=0,\\n        timeout=Timeout(12, connect=5, read=5),\\n        http_client=DefaultHttpxClient(\\n            event_hooks={\\n                \\\"request\\\": [\\n                    create_header_hook(\\n                        wallet.hotkey,\\n                        axon_info.hotkey,\\n                        \\\"NousResearch/Meta-Llama-3.1-8B-Instruct\\\",\\n                    )\\n                ]\\n            }\\n        ),\\n    )\\n    return client\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    messages: List[ChatCompletionMessageParam] = [\\n        {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"},\\n        {\\n            \\\"role\\\": \\\"user\\\",\\n            \\\"content\\\": f\\\"What is the deffinition of the x y problem \\\",\\n        },\\n    ]\\n    model = \\\"NousResearch/Meta-Llama-3.1-8B-Instruct\\\"\\n    MINER_UID = -1\\n    client = make_client(MINER_UID)\\n    response = client.chat.completions.create(\\n        model=model,\\n        stream=True,\\n        logprobs=True,\\n        max_tokens=100,\\n        messages=messages,\\n    )\\n    for chunk in response:\\n        content = chunk.choices[0].delta.content\\n        if content:\\n            print(content, end=\\\"\\\")\\n\"\n      },\n      {\n        \"fileName\": \"justfile\",\n        \"fileContents\": \"set dotenv-load\\n# This file is mostly for testing, but can be used as reference for what commands should look like\\n\\ndefault:\\n  @just --list\\n\\nvalidator:\\n  python3 neurons/validator.py --wallet.name validator --netuid 40 --subtensor.network test --epoch-length 101 --logging.trace --autoupdate-off --mock --models.mode endpoint --models.endpoint https://targon.sybil.com/api/models\\n\\nminer num=\\\"0\\\":\\n  python neurons/miner.py --wallet.name miner --netuid 40 --wallet.hotkey new-miner{{num}} --subtensor.network test --model-endpoint http://localhost:9000/v1 --axon.port 700{{num}} --api_key abc123 --mock --no-force-validator-permit\\n\\nscript script_name opts=\\\"\\\":\\n  python3 scripts/{{script_name}}.py --wallet.name validator --netuid 40 --subtensor.network test --neuron.port 8080 --epoch-length 101 --logging.trace {{opts}}\\n\\nup:\\n  docker compose -f docker-compose.testnet.yml build\\n  docker compose -f docker-compose.testnet.yml up -d\\n\\nbuild_verifier tag='latest':\\n  cd verifier && docker build -t manifoldlabs/sn4-verifier:{{tag}} .\\n\\nrun_verifier model port gpu tag:\\n  docker run -p {{port}}:80 -e MODEL={{model}} -e GPU_MEMORY_UTIL=.9 --runtime=nvidia --ipc=host --gpus='\\\"device={{gpu}}\\\"' -v ~/.cache/huggingface:/root/.cache/huggingface -d --name dev_image manifoldlabs/sn4-verifier:{{tag}}\\n\\nrun_verifier_prod model port gpu gpus name memory_util='.9' tag='latest':\\n  docker run -p {{port}}:80 -e MODEL={{model}} -e TENSOR_PARALLEL={{gpus}} -e GPU_MEMORY_UTIL={{memory_util}} -l model={{model}} -l port={{port}} --runtime=nvidia --ipc=host --gpus='\\\"device={{gpu}}\\\"' -d --name {{name}} manifoldlabs/sn4-verifier:{{tag}}\\n\\npush_verifier: build_verifier\\n  docker push manifoldlabs/sn4-verifier:latest\\n\"\n      },\n      {\n        \"fileName\": \"neurons\",\n        \"fileContents\": \"\"\n      },\n      {\n        \"fileName\": \"base.py\",\n        \"fileContents\": \"import argparse\\nfrom threading import Thread\\nfrom typing import Callable, List\\nimport bittensor as bt\\nimport copy\\n\\nfrom nest_asyncio import asyncio\\nfrom substrateinterface import SubstrateInterface\\nfrom targon import (\\n    add_args,\\n    add_validator_args,\\n    validate_config_and_neuron_path,\\n)\\nfrom targon.config import add_miner_args\\nfrom enum import Enum\\nimport signal\\n\\nfrom targon import (\\n    __spec_version__ as spec_version,\\n)\\nfrom targon.metagraph import run_block_callback_thread\\nfrom targon.utils import ExitContext\\nfrom bittensor.core.settings import SS58_FORMAT, TYPE_REGISTRY\\n\\n\\nclass NeuronType(Enum):\\n    Validator = \\\"VALIDATOR\\\"\\n    Miner = \\\"MINER\\\"\\n\\n\\nclass BaseNeuron:\\n    config: \\\"bt.config\\\"\\n    neuron_type: NeuronType\\n    exit_context = ExitContext()\\n    next_sync_block = None\\n    block_callbacks: List[Callable] = []\\n    substrate_thread: Thread\\n\\n    def check_registered(self):\\n        if not self.subtensor.is_hotkey_registered(\\n            netuid=self.config.netuid,\\n            hotkey_ss58=self.wallet.hotkey.ss58_address,\\n        ):\\n            bt.logging.error(\\n                f\\\"Wallet: {self.wallet} is not registered on netuid {self.config.netuid}.\\\"\\n                f\\\" Please register the hotkey using `btcli subnets register` before trying again\\\"\\n            )\\n            exit()\\n\\n    def maybe_sync_metagraph(self, block):\\n        assert self.config.neuron\\n        if block % self.config.epoch_length:\\n            return False\\n\\n        # Ensure miner or validator hotkey is still registered on the network.\\n        self.check_registered()\\n        bt.logging.info(\\\"Resyncing Metagraph\\\")\\n        self.metagraph.sync(subtensor=self.subtensor)\\n        return True\\n\\n    def run_callbacks(self, block):\\n        for callback in self.block_callbacks:\\n            callback(block)\\n\\n    def __init__(self, config=None):\\n        # Add parser args\\n        bt.logging.info(f\\\"Targon version {spec_version}\\\")\\n        parser = argparse.ArgumentParser()\\n        bt.wallet.add_args(parser)\\n        bt.subtensor.add_args(parser)\\n        bt.logging.add_args(parser)\\n        bt.axon.add_args(parser)\\n        add_args(parser)\\n        if self.neuron_type == NeuronType.Validator:\\n            add_validator_args(parser)\\n        if self.neuron_type == NeuronType.Miner:\\n            add_miner_args(parser)\\n        self.config = bt.config(parser)\\n        if config:\\n            base_config = copy.deepcopy(config)\\n            self.config.merge(base_config)\\n        validate_config_and_neuron_path(self.config)\\n\\n        ## Add kill signals\\n        signal.signal(signal.SIGINT, self.exit_context.startExit)\\n        signal.signal(signal.SIGTERM, self.exit_context.startExit)\\n\\n        ## Typesafety\\n        assert self.config.logging\\n        assert self.config.neuron\\n        assert self.config.netuid\\n        assert self.config.axon\\n        assert self.config.subtensor\\n\\n        ## LOGGING\\n        bt.logging(config=self.config, logging_dir=self.config.neuron.full_path)\\n        bt.logging.set_info()\\n        if self.config.logging.debug:\\n            bt.logging.set_debug(True)\\n        if self.config.logging.trace:\\n            bt.logging.set_trace(True)\\n\\n        ## BITTENSOR INITIALIZATION\\n        self.wallet = bt.wallet(config=self.config)\\n        self.subtensor = bt.subtensor(config=self.config)\\n        self.metagraph = self.subtensor.metagraph(self.config.netuid)\\n\\n        self.loop = asyncio.get_event_loop()\\n        bt.logging.debug(f\\\"Wallet: {self.wallet}\\\")\\n        bt.logging.debug(f\\\"Subtensor: {self.subtensor}\\\")\\n        bt.logging.debug(f\\\"Metagraph: {self.metagraph}\\\")\\n\\n        ## CHECK IF REGG'D\\n        self.check_registered()\\n        self.uid = self.metagraph.hotkeys.index(self.wallet.hotkey.ss58_address)\\n\\n        ## Substrate, Subtensor and Metagraph\\n        self.substrate = SubstrateInterface(\\n            ss58_format=SS58_FORMAT,\\n            use_remote_preset=True,\\n            url=self.config.subtensor.chain_endpoint,\\n            type_registry=TYPE_REGISTRY,\\n        )\\n        self.block_callbacks.append(self.maybe_sync_metagraph)\\n        self.substrate_thread = run_block_callback_thread(self.substrate, self.run_callbacks)\\n\"\n      },\n      {\n        \"fileName\": \"miner.py\",\n        \"fileContents\": \"import traceback\\nimport time\\nfrom bittensor.core.axon import FastAPIThreadedServer\\nfrom bittensor.core.extrinsics.serving import serve_extrinsic\\nfrom fastapi import APIRouter, Depends, FastAPI, HTTPException, Request\\nimport httpx\\nimport netaddr\\nimport requests\\nfrom starlette.background import BackgroundTask\\nfrom starlette.responses import StreamingResponse\\n\\nfrom neurons.base import BaseNeuron, NeuronType\\nfrom targon.epistula import verify_signature\\nfrom targon.utils import print_info\\nimport uvicorn\\nimport bittensor as bt\\n\\n\\nclass Miner(BaseNeuron):\\n    neuron_type = NeuronType.Miner\\n    fast_api: FastAPIThreadedServer\\n\\n    def shutdown(self):\\n        if self.fast_api:\\n            self.fast_api.stop()\\n\\n    def log_on_block(self, block):\\n        print_info(\\n            self.metagraph,\\n            self.wallet.hotkey.ss58_address,\\n            block,\\n        )\\n\\n    def __init__(self, config=None):\\n        super().__init__(config)\\n        bt.logging.set_info()\\n        ## Typesafety\\n        assert self.config.netuid\\n        assert self.config.logging\\n        assert self.config.model_endpoint\\n\\n        # Register log callback\\n        self.block_callbacks.append(self.log_on_block)\\n\\n        ## BITTENSOR INITIALIZATION\\n        bt.logging.info(\\n            \\\"\\\\N{grinning face with smiling eyes}\\\", \\\"Successfully Initialized!\\\"\\n        )\\n        bt.logging.info(self.config.model_endpoint)\\n        self.client = httpx.AsyncClient(\\n            base_url=self.config.model_endpoint,\\n            headers={\\\"Authorization\\\": f\\\"Bearer {self.config.api_key}\\\"},\\n        )\\n\\n    async def create_chat_completion(self, request: Request):\\n        bt.logging.info(\\n            \\\"\\\\u2713\\\",\\n            f\\\"Getting Chat Completion request from {request.headers.get('Epistula-Signed-By', '')[:8]}!\\\",\\n        )\\n        req = self.client.build_request(\\n            \\\"POST\\\", \\\"/chat/completions\\\", content=await request.body()\\n        )\\n        r = await self.client.send(req, stream=True)\\n        return StreamingResponse(\\n            r.aiter_raw(), background=BackgroundTask(r.aclose), headers=r.headers\\n        )\\n\\n    async def create_completion(self, request: Request):\\n        bt.logging.info(\\n            \\\"\\\\u2713\\\",\\n            f\\\"Getting Completion request from {request.headers.get('Epistula-Signed-By', '')[:8]}!\\\",\\n        )\\n        req = self.client.build_request(\\n            \\\"POST\\\", \\\"/completions\\\", content=await request.body()\\n        )\\n        r = await self.client.send(req, stream=True)\\n        return StreamingResponse(\\n            r.aiter_raw(), background=BackgroundTask(r.aclose), headers=r.headers\\n        )\\n\\n    async def receive_models(self, request: Request):\\n        models = await request.json()\\n        bt.logging.info(\\n            \\\"\\\\u2713\\\",\\n            f\\\"Received model list from {request.headers.get('Epistula-Signed-By', '')[:8]}: {models}\\\",\\n        )\\n\\n        #\\n        # Add extra logic here for how your miner should handle the model list.\\n        #\\n\\n        return \\\"\\\"\\n\\n    async def list_models(self):\\n        #\\n        # Return models the miner is running\\n        #\\n\\n        return []\\n\\n    async def determine_epistula_version_and_verify(self, request: Request):\\n        version = request.headers.get(\\\"Epistula-Version\\\")\\n        if version == \\\"2\\\":\\n            await self.verify_request(request)\\n            return\\n        raise HTTPException(status_code=400, detail=\\\"Unknown Epistula version\\\")\\n\\n    async def verify_request(\\n        self,\\n        request: Request,\\n    ):\\n        # We do this as early as possible so that now has a lesser chance\\n        # of causing a stale request\\n        now = round(time.time() * 1000)\\n\\n        # We need to check the signature of the body as bytes\\n        # But use some specific fields from the body\\n        signed_by = request.headers.get(\\\"Epistula-Signed-By\\\")\\n        signed_for = request.headers.get(\\\"Epistula-Signed-For\\\")\\n        if signed_for != self.wallet.hotkey.ss58_address:\\n            raise HTTPException(\\n                status_code=400, detail=\\\"Bad Request, message is not intended for self\\\"\\n            )\\n        if signed_by not in self.metagraph.hotkeys:\\n            raise HTTPException(status_code=401, detail=\\\"Signer not in metagraph\\\")\\n\\n        uid = self.metagraph.hotkeys.index(signed_by)\\n        stake = self.metagraph.S[uid].item()\\n        if not self.config.no_force_validator_permit and stake < 10000:\\n            bt.logging.warning(\\n                f\\\"Blacklisting request from {signed_by} [uid={uid}], not enough stake -- {stake}\\\"\\n            )\\n            raise HTTPException(status_code=401, detail=\\\"Stake below minimum: {stake}\\\")\\n\\n        # If anything is returned here, we can throw\\n        body = await request.body()\\n        err = verify_signature(\\n            request.headers.get(\\\"Epistula-Request-Signature\\\"),\\n            body,\\n            request.headers.get(\\\"Epistula-Timestamp\\\"),\\n            request.headers.get(\\\"Epistula-Uuid\\\"),\\n            signed_for,\\n            signed_by,\\n            now,\\n        )\\n        if err:\\n            bt.logging.error(err)\\n            raise HTTPException(status_code=400, detail=err)\\n\\n    def run(self):\\n        assert self.config.netuid\\n        assert self.config.subtensor\\n        assert self.config.axon\\n\\n        # Serve passes the axon information to the network + netuid we are hosting on.\\n        # This will auto-update if the axon port of external ip have changed.\\n        external_ip = self.config.axon.external_ip or self.config.axon.ip\\n        if not external_ip or external_ip == \\\"[::]\\\":\\n            try:\\n                external_ip = requests.get(\\\"https://checkip.amazonaws.com\\\").text.strip()\\n                netaddr.IPAddress(external_ip)\\n            except Exception:\\n                bt.logging.error(\\\"Failed to get external IP\\\")\\n\\n        bt.logging.info(\\n            f\\\"Serving miner endpoint {external_ip}:{self.config.axon.port} on network: {self.config.subtensor.chain_endpoint} with netuid: {self.config.netuid}\\\"\\n        )\\n\\n        serve_success = serve_extrinsic(\\n            subtensor=self.subtensor,\\n            wallet=self.wallet,\\n            ip=external_ip,\\n            port=self.config.axon.port,\\n            protocol=4,\\n            netuid=self.config.netuid,\\n        )\\n        if not serve_success:\\n            bt.logging.error(\\\"Failed to serve endpoint\\\")\\n            return\\n\\n        # Start  starts the miner's endpoint, making it active on the network.\\n        # change the config in the axon\\n        app = FastAPI()\\n        router = APIRouter()\\n        router.add_api_route(\\n            \\\"/v1/chat/completions\\\",\\n            self.create_chat_completion,\\n            dependencies=[Depends(self.determine_epistula_version_and_verify)],\\n            methods=[\\\"POST\\\"],\\n        )\\n        router.add_api_route(\\n            \\\"/v1/completions\\\",\\n            self.create_completion,\\n            dependencies=[Depends(self.determine_epistula_version_and_verify)],\\n            methods=[\\\"POST\\\"],\\n        )\\n        router.add_api_route(\\n            \\\"/models\\\",\\n            self.receive_models,\\n            dependencies=[Depends(self.determine_epistula_version_and_verify)],\\n            methods=[\\\"POST\\\"],\\n        )\\n        router.add_api_route(\\n            \\\"/models\\\",\\n            self.list_models,\\n            dependencies=[Depends(self.determine_epistula_version_and_verify)],\\n            methods=[\\\"GET\\\"],\\n        )\\n        app.include_router(router)\\n        fast_config = uvicorn.Config(\\n            app,\\n            host=\\\"0.0.0.0\\\",\\n            port=self.config.axon.port,\\n            log_level=\\\"info\\\",\\n            loop=\\\"asyncio\\\",\\n        )\\n        self.fast_api = FastAPIThreadedServer(config=fast_config)\\n        self.fast_api.start()\\n\\n        bt.logging.info(f\\\"Miner starting at block: {self.subtensor.block}\\\")\\n\\n        # This loop maintains the miner's operations until intentionally stopped.\\n        try:\\n            while not self.exit_context.isExiting:\\n                time.sleep(1)\\n        except Exception as e:\\n            bt.logging.error(str(e))\\n            bt.logging.error(traceback.format_exc())\\n        self.shutdown()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    try:\\n        miner = Miner()\\n        miner.run()\\n    except Exception as e:\\n        bt.logging.error(str(e))\\n        bt.logging.error(traceback.format_exc())\\n    exit()\\n\"\n      },\n      {\n        \"fileName\": \"validator.py\",\n        \"fileContents\": \"import json\\nimport random\\nimport asyncio\\nimport sys\\nfrom threading import Thread\\nfrom time import sleep\\n\\nfrom asyncpg.connection import asyncpg\\nfrom bittensor.core.settings import SS58_FORMAT, TYPE_REGISTRY\\nimport httpx\\nfrom substrateinterface import SubstrateInterface\\nfrom neurons.base import BaseNeuron, NeuronType\\nfrom targon.cache import load_cache\\nfrom targon.config import (\\n    AUTO_UPDATE,\\n    HEARTBEAT,\\n    IS_TESTNET,\\n    SLIDING_WINDOW,\\n    get_models_from_config,\\n    get_models_from_endpoint,\\n)\\nfrom targon.dataset import download_dataset\\nfrom targon.docker import load_docker, sync_output_checkers\\nfrom targon.epistula import generate_header\\nfrom targon.jugo import score_organics, send_organics_to_jugo, send_stats_to_jugo\\nfrom targon.math import get_weights\\nfrom targon.metagraph import (\\n    create_set_weights,\\n    get_miner_uids,\\n    resync_hotkeys,\\n    run_block_callback_thread,\\n)\\nfrom targon.request import check_tokens, generate_request, handle_inference\\nfrom targon.updater import autoupdate\\nfrom targon.utils import (\\n    fail_with_none,\\n    print_info,\\n)\\nfrom targon.types import Endpoints, InferenceStats\\nimport traceback\\nimport bittensor as bt\\n\\nfrom typing import Any, Dict, List, Optional, Tuple\\nfrom targon import (\\n    __version__,\\n    __spec_version__ as spec_version,\\n)\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\n\\nclass Validator(BaseNeuron):\\n    neuron_type = NeuronType.Validator\\n    miner_tps: Dict[int, Dict[str, List[Optional[float]]]]\\n    miner_models: Dict[int, List[str]]\\n    db: Optional[asyncpg.Connection]\\n    verification_ports: Dict[str, Dict[str, Any]]\\n    models: List[str]\\n    lock_waiting = False\\n    lock_halt = False\\n    is_runing = False\\n    organics = {}\\n    last_bucket_id = None\\n    heartbeat_thread: Thread\\n    step = 0\\n    dataset = None\\n\\n    def __init__(self, config=None, run_init=True):\\n        super().__init__(config)\\n        ## Typesafety\\n        self.set_weights = create_set_weights(spec_version, self.config.netuid)\\n\\n        ## CHECK IF REGG'D\\n        if not self.metagraph.validator_permit[self.uid] and not IS_TESTNET:\\n            bt.logging.error(\\\"Validator does not have vpermit\\\")\\n            exit()\\n        if run_init:\\n            self.init()\\n\\n    def init(self):\\n        assert self.config.netuid\\n        assert self.config.cache_file\\n        assert self.config.vpermit_tao_limit\\n        assert self.config.database\\n        assert self.config.subtensor\\n        ## LOAD DOCKER\\n        self.client = load_docker()\\n\\n        ## SET MISC PARAMS\\n        self.next_forward_block = None\\n        self.last_posted_weights = self.metagraph.last_update[self.uid]\\n        bt.logging.info(f\\\"Last updated at block {self.last_posted_weights}\\\")\\n\\n        ## LOAD MINER SCORES CACHE\\n        miners = get_miner_uids(self.metagraph, self.uid, self.config.vpermit_tao_limit)\\n        self.miner_tps = load_cache(\\n            self.config.cache_file, self.subtensor.block, miners\\n        )\\n\\n        ## LOAD DATASET\\n        bt.logging.info(\\\"⌛️\\\", \\\"Loading dataset\\\")\\n        self.dataset = download_dataset()\\n\\n        ## CONNECT TO ORGANICS DB\\n        try:\\n            self.db = None\\n            if self.config.database.url:\\n                self.db = self.loop.run_until_complete(\\n                    asyncpg.connect(self.config.database.url)\\n                )\\n        except Exception as e:\\n            bt.logging.error(f\\\"Failed to initialize organics database: {e}\\\")\\n\\n        ## REGISTER BLOCK CALLBACKS\\n        self.block_callbacks.extend(\\n            [\\n                self.log_on_block,\\n                self.set_weights_on_interval,\\n                self.sync_output_checkers_on_interval,\\n                self.resync_hotkeys_on_interval,\\n                self.send_models_to_miners_on_interval,\\n                self.score_organics_on_block,\\n            ]\\n        )\\n\\n        # Setup heartbeat thread\\n        if HEARTBEAT:\\n            self.heartbeat_thread = Thread(name=\\\"heartbeat\\\", target=self.heartbeat)\\n            self.heartbeat_thread.start()\\n\\n        ## DONE\\n        bt.logging.info(\\n            \\\"\\\\N{grinning face with smiling eyes}\\\", \\\"Successfully Initialized!\\\"\\n        )\\n\\n    def heartbeat(self):\\n        bt.logging.info(\\\"Starting Heartbeat\\\")\\n        last_step = self.step\\n        stuck_count = 0\\n        while True:\\n            sleep(60)\\n            if last_step == self.step:\\n                stuck_count += 1\\n            if last_step != self.step:\\n                stuck_count = 0\\n            if stuck_count >= 5:\\n                bt.logging.error(\\n                    \\\"Heartbeat detecting main process hang, attempting restart\\\"\\n                )\\n                autoupdate(force=True)\\n                sys.exit(0)\\n            last_step = self.step\\n            bt.logging.info(\\\"Heartbeat\\\")\\n\\n    def send_models_to_miners_on_interval(self, block):\\n        assert self.config.vpermit_tao_limit\\n        if block % self.config.epoch_length:\\n            return\\n\\n        if block != 0 and not self.is_runing:\\n            return\\n        miner_uids = get_miner_uids(\\n            self.metagraph, self.uid, self.config.vpermit_tao_limit\\n        )\\n        self.miner_models = {}\\n        bt.logging.info(\\\"Broadcasting models to all miners\\\")\\n        body = self.models\\n        for uid in miner_uids:\\n            bt.logging.info(f\\\"Broadcasting models {uid}\\\")\\n            axon_info = self.metagraph.axons[uid]\\n            headers = generate_header(self.wallet.hotkey, body, axon_info.hotkey)\\n            headers[\\\"Content-Type\\\"] = \\\"application/json\\\"\\n            try:\\n                httpx.post(\\n                    f\\\"http://{axon_info.ip}:{axon_info.port}/models\\\",\\n                    headers=headers,\\n                    json=body,\\n                    timeout=3,\\n                )\\n                headers = generate_header(self.wallet.hotkey, b\\\"\\\", axon_info.hotkey)\\n                res = httpx.get(\\n                    f\\\"http://{axon_info.ip}:{axon_info.port}/models\\\",\\n                    headers=headers,\\n                    timeout=3,\\n                )\\n                if res.status_code != 200 or not isinstance(models := res.json(), list):\\n                    models = []\\n                self.miner_models[uid] = list(set(models))\\n            except Exception:\\n                self.miner_models[uid] = []\\n        bt.logging.info(\\\"Miner models: \\\" + str(self.miner_models))\\n\\n    def resync_hotkeys_on_interval(self, block):\\n        if not self.is_runing:\\n            return\\n        if block % self.config.epoch_length:\\n            return\\n        resync_hotkeys(self.metagraph, self.miner_tps)\\n\\n    def sync_output_checkers_on_interval(self, block):\\n        if not self.is_runing:\\n            return\\n        if block % self.config.epoch_length:\\n            return\\n        self.lock_halt = True\\n        while not self.lock_waiting:\\n            sleep(1)\\n        self.models = self.get_models()\\n        self.verification_ports = sync_output_checkers(self.client, self.models)\\n        self.lock_halt = False\\n\\n    def score_organics_on_block(self, block):\\n        if not self.is_runing:\\n            return\\n        if block % 20:\\n            return\\n        res = asyncio.run(\\n            score_organics(self.last_bucket_id, self.verification_ports, self.wallet)\\n        )\\n        if res == None:\\n            return\\n        bucket_id, organics, organic_stats = res\\n        self.last_bucket_id = bucket_id\\n        if organics == None or organic_stats == None:\\n            return\\n        self.organics = organics\\n        asyncio.run(send_organics_to_jugo(self.wallet, organic_stats))\\n\\n    def set_weights_on_interval(self, block):\\n        if block % self.config.epoch_length:\\n            return\\n        self.lock_halt = True\\n        while not self.lock_waiting:\\n            sleep(1)\\n        self.set_weights(\\n            self.wallet,\\n            self.metagraph,\\n            self.subtensor,\\n            get_weights(\\n                self.miner_models,\\n                self.miner_tps,\\n                self.organics,\\n                self.models,\\n            ),\\n        )\\n\\n        # Only keep last 30 scores\\n        for uid in self.miner_tps:\\n            for model in self.miner_tps[uid]:\\n                self.miner_tps[uid][model] = self.miner_tps[uid][model][-SLIDING_WINDOW:]\\n        self.lock_halt = False\\n\\n    def log_on_block(self, block):\\n        blocks_till = self.config.epoch_length - (block % self.config.epoch_length)\\n        print_info(\\n            self.metagraph,\\n            self.wallet.hotkey.ss58_address,\\n            block,\\n        )\\n        bt.logging.info(\\n            f\\\"Forward Block: {self.subtensor.block} | Blocks till Set Weights: {blocks_till}\\\"\\n        )\\n\\n    def run(self):\\n        assert self.config.subtensor\\n        assert self.config.neuron\\n        assert self.config.database\\n        assert self.config.vpermit_tao_limit\\n        bt.logging.info(\\n            f\\\"Running validator on network: {self.config.subtensor.chain_endpoint} with netuid: {self.config.netuid}\\\"\\n        )\\n\\n        bt.logging.info(f\\\"Validator starting at block: {self.subtensor.block}\\\")\\n\\n        # This loop maintains the validator's operations until intentionally stopped.\\n        miner_subset = 36\\n\\n        # Ensure everything is setup\\n        self.models = self.get_models()\\n        self.verification_ports = sync_output_checkers(self.client, self.models)\\n        resync_hotkeys(self.metagraph, self.miner_tps)\\n        self.send_models_to_miners_on_interval(0)\\n\\n        self.is_runing = True\\n        while not self.exit_context.isExiting:\\n            self.step += 1\\n            if self.config.autoupdate and not AUTO_UPDATE:\\n                autoupdate(branch=\\\"main\\\")\\n            # Make sure our substrate thread is alive\\n            if not self.substrate_thread.is_alive():\\n                self.substrate = SubstrateInterface(\\n                    ss58_format=SS58_FORMAT,\\n                    use_remote_preset=True,\\n                    url=self.config.subtensor.chain_endpoint,\\n                    type_registry=TYPE_REGISTRY,\\n                )\\n                self.substrate_thread = run_block_callback_thread(\\n                    self.substrate, self.run_callbacks\\n                )\\n\\n            # Mutex for setting weights\\n            if self.lock_halt:\\n                self.lock_waiting = True\\n                while self.lock_halt:\\n                    sleep(1)\\n                self.lock_waiting = False\\n\\n            # Random model, but every three is a model we are verifying for sure\\n            model_name = random.choice(self.models)\\n            if self.step % 3 == 0:\\n                model_name = random.choice(list(self.verification_ports.keys()))\\n\\n            endpoint_model = list(self.verification_ports.keys())[0]\\n            if self.verification_ports.get(model_name) != None:\\n                endpoint = random.choice(\\n                    self.verification_ports[model_name][\\\"endpoints\\\"]\\n                )\\n                generator_model_name = model_name\\n            else:\\n                endpoint = random.choice(\\n                    self.verification_ports[endpoint_model][\\\"endpoints\\\"]\\n                )\\n                generator_model_name = endpoint_model\\n            uids = get_miner_uids(\\n                self.metagraph, self.uid, self.config.vpermit_tao_limit\\n            )\\n            random.shuffle(uids)\\n            miner_uids = []\\n            for uid in uids:\\n                if len(miner_uids) > miner_subset:\\n                    break\\n\\n                # Make sure tps array exists\\n                if self.miner_tps[uid].get(model_name) is None:\\n                    self.miner_tps[uid][model_name] = []\\n\\n                if model_name not in self.miner_models.get(uid, []):\\n                    self.miner_tps[uid][model_name].append(None)\\n                    continue\\n                miner_uids.append(uid)\\n\\n            # Skip if no miners running this model\\n            if not len(miner_uids):\\n                bt.logging.info(\\\"No miners for this model\\\")\\n                continue\\n\\n            res = self.loop.run_until_complete(\\n                self.query_miners(\\n                    miner_uids, model_name, endpoint, generator_model_name\\n                )\\n            )\\n            self.save_scores()\\n            if res is not None:\\n                self.loop.run_until_complete(\\n                    send_stats_to_jugo(\\n                        self.metagraph,\\n                        self.subtensor,\\n                        self.wallet,\\n                        *res,\\n                        spec_version,\\n                        self.models,\\n                        self.miner_tps,\\n                    )\\n                )\\n\\n        # Exiting\\n        self.shutdown()\\n\\n    async def verify_response(self, uid, request, endpoint, stat: InferenceStats):\\n        if stat.error or stat.cause:\\n            return uid, stat\\n        # We do this out of the handle_inference loop to not block other requests\\n        verification_port = self.verification_ports.get(\\n            request[\\\"model\\\"], {\\\"port\\\": None}\\n        ).get(\\\"port\\\")\\n        if verification_port is None:\\n            bt.logging.error(\\n                \\\"Send request to a miner without verification port for model\\\"\\n            )\\n            return uid, None\\n        verified = await check_tokens(\\n            request, stat.tokens, uid, endpoint=endpoint, port=verification_port\\n        )\\n        if verified is None:\\n            return uid, None\\n        stat.verified = (\\n            verified.get(\\\"verified\\\", False) if verified is not None else False\\n        )\\n        if stat.error is None and not stat.verified:\\n            stat.error = verified.get(\\\"error\\\")\\n            stat.cause = verified.get(\\\"cause\\\")\\n        return uid, stat\\n\\n    async def query_miners(\\n        self,\\n        miner_uids: List[int],\\n        model_name: str,\\n        endpoint: Endpoints,\\n        generator_model_name: str,\\n    ):\\n        assert self.config.database\\n\\n        verification_port: Optional[int] = self.verification_ports.get(\\n            generator_model_name, {\\\"port\\\": None}\\n        ).get(\\\"port\\\")\\n        if verification_port is None:\\n            bt.logging.error(\\n                f\\\"No generator / verifier found for {generator_model_name}\\\"\\n            )\\n            return None\\n        request = generate_request(\\n            self.dataset, generator_model_name, endpoint, verification_port\\n        )\\n        if not request:\\n            bt.logging.info(\\\"No request was generated\\\")\\n            return None\\n\\n        bt.logging.info(f\\\"{model_name} - {endpoint}: {request}\\\")\\n\\n        # We do these in separate groups for better response timings\\n        tasks = []\\n        try:\\n            for uid in miner_uids:\\n                tasks.append(\\n                    asyncio.create_task(\\n                        handle_inference(\\n                            self.metagraph, self.wallet, request, uid, endpoint\\n                        )\\n                    )\\n                )\\n            responses: List[Tuple[int, InferenceStats]] = await asyncio.gather(*tasks)\\n\\n            # Skip scoring if we arent running that model\\n            if generator_model_name != model_name:\\n                return None\\n\\n            tasks = []\\n            for uid, stat in responses:\\n                tasks.append(\\n                    asyncio.create_task(\\n                        self.verify_response(uid, request, endpoint, stat)\\n                    )\\n                )\\n            stats: List[Tuple[int, Optional[InferenceStats]]] = await asyncio.gather(\\n                *tasks\\n            )\\n        except Exception:\\n            bt.logging.error(f\\\"Failed sending requests: {traceback.format_exc()}\\\")\\n            stats = []\\n        processed_stats = []\\n        for uid, stat in stats:\\n            if not stat:\\n                continue\\n            processed_stats.append((uid, stat))\\n            bt.logging.info(f\\\"{uid}: {stat.verified} | {stat.total_time}\\\")\\n            if not stat.verified and stat.error:\\n                bt.logging.info(str(stat.cause))\\n\\n            # UID is not in our miner tps list\\n            if self.miner_tps.get(uid) is None:\\n                self.miner_tps[uid] = {request[\\\"model\\\"]: []}\\n            # This uid doesnt have reccords of this model\\n            if self.miner_tps[uid].get(request[\\\"model\\\"]) is None:\\n                self.miner_tps[uid][request[\\\"model\\\"]] = []\\n\\n            if stat.verified and stat.total_time != 0:\\n                self.miner_tps[uid][request[\\\"model\\\"]].append(stat.tps)\\n                continue\\n            self.miner_tps[uid][request[\\\"model\\\"]].append(None)\\n        return (processed_stats, request, endpoint)\\n\\n    @fail_with_none(\\\"Failed writing to cache file\\\")\\n    def save_scores(self):\\n        assert self.config.cache_file\\n        if self.exit_context.isExiting:\\n            return\\n        try:\\n            with open(self.config.cache_file, \\\"w\\\") as file:\\n                bt.logging.info(\\\"Caching scores...\\\")\\n                json.dump(\\n                    {\\n                        \\\"miner_tps\\\": self.miner_tps,\\n                        \\\"block_saved\\\": self.subtensor.block,\\n                        \\\"version\\\": spec_version,\\n                    },\\n                    file,\\n                )\\n                file.flush()\\n                bt.logging.info(\\\"Cached\\\")\\n        except Exception as e:\\n            bt.logging.error(f\\\"Failed writing to cache file: {e}\\\")\\n\\n    def shutdown(self):\\n        if self.db:\\n            bt.logging.info(\\\"Closing organics db connection\\\")\\n            self.loop.run_until_complete(self.db.close())\\n\\n    def get_models(self) -> List[str]:\\n        \\\"\\\"\\\"\\n        List of models and sizes of models\\n        Miners are scored based\\n        - How large is the model\\n        - How many models are we testing them on\\n        - How fast\\n\\n        Ask miners what models they are running\\n        score based on what models valis want\\n        Let valis inspect what most popular models are\\n        - Top valis manually decide via model leasing\\n        - Minor valis follow along for consensus\\n        \\\"\\\"\\\"\\n        assert self.config.models\\n\\n        match self.config.models.mode:\\n            case \\\"config\\\":\\n                models = get_models_from_config()\\n                if not models:\\n                    raise Exception(\\\"No models\\\")\\n                return models\\n            case _:\\n                models = get_models_from_endpoint(self.config.models.endpoint)\\n                if not models:\\n                    raise Exception(\\\"No models\\\")\\n                return models\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    try:\\n        validator = Validator()\\n        validator.run()\\n    except Exception as e:\\n        bt.logging.error(str(e))\\n        bt.logging.error(traceback.format_exc())\\n    exit()\\n\"\n      },\n      {\n        \"fileName\": \"requirements.txt\",\n        \"fileContents\": \"bittensor==8.5.1\\npandas==2.2.2\\ndatasets==3.2.0\\nhuggingface_hub==0.27.0\\nnumpy==2.0.1\\nsentencepiece==0.2.0\\nplotext==5.2.8\\nopenai==1.44.1\\nasyncpg==0.29.0\\nfastparquet==2024.5.0\\nnanoid==2.0.0\\naccelerate==0.34.2\\ndocker==7.1.0\\npython-dotenv==1.0.1\\ntransformers==4.46.2\\n\"\n      },\n      {\n        \"fileName\": \"scripts\",\n        \"fileContents\": \"\"\n      },\n      {\n        \"fileName\": \"check_response.py\",\n        \"fileContents\": \"import asyncio\\nfrom enum import Enum\\nfrom requests import post\\n\\n\\nclass Endpoints(Enum):\\n    CHAT = \\\"CHAT\\\"\\n    COMPLETION = \\\"COMPLETION\\\"\\n\\n\\nasync def check_tokens(\\n    request, responses, uid, endpoint: Endpoints, port: int, url=\\\"http://localhost\\\"\\n):\\n    try:\\n        result = post(\\n            f\\\"{url}:{port}/verify\\\",\\n            headers={\\\"Content-Type\\\": \\\"application/json\\\"},\\n            json={\\n                \\\"model\\\": request.get(\\\"model\\\"),\\n                \\\"request_type\\\": endpoint.value,\\n                \\\"request_params\\\": request,\\n                \\\"output_sequence\\\": responses,\\n            },\\n        ).json()\\n        if err := result.get(\\\"error\\\") is not None:\\n            print(str(err))\\n            return None\\n        return result\\n    except Exception as e:\\n        print(f\\\"{uid}: \\\" + str(e))\\n        return None\\n\\n\\nasync def main():\\n    request = {}\\n    responses = []\\n    uid = -1\\n    endpoint = Endpoints.COMPLETION\\n    port = 7777\\n    res = await check_tokens(request, responses, uid, endpoint, port)\\n    print(res)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    asyncio.run(main())\\n\"\n      },\n      {\n        \"fileName\": \"check_response_2.py\",\n        \"fileContents\": \"import asyncio\\nfrom enum import Enum\\nfrom requests import post\\n\\n\\nclass Endpoints(Enum):\\n    CHAT = \\\"CHAT\\\"\\n    COMPLETION = \\\"COMPLETION\\\"\\n\\n\\nasync def check_tokens(\\n    request, responses, uid, endpoint: Endpoints, port: int, url=\\\"http://localhost\\\"\\n):\\n    try:\\n        result = post(\\n            f\\\"{url}:{port}/verify\\\",\\n            headers={\\\"Content-Type\\\": \\\"application/json\\\"},\\n            json={\\n                \\\"model\\\": request.get(\\\"model\\\"),\\n                \\\"request_type\\\": endpoint.value,\\n                \\\"request_params\\\": request,\\n                \\\"output_sequence\\\": responses,\\n            },\\n        ).json()\\n        if err := result.get(\\\"error\\\") is not None:\\n            print(str(err))\\n            return None\\n        return result\\n    except Exception as e:\\n        print(f\\\"{uid}: \\\" + str(e))\\n        return None\\n\\n\\nasync def main():\\n    request = {}\\n    responses = []\\n    uid = -1\\n    endpoint = Endpoints.COMPLETION\\n    port = 7777\\n    res = await check_tokens(request, responses, uid, endpoint, port)\\n    print(res)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    asyncio.run(main())\\n\"\n      },\n      {\n        \"fileName\": \"get_responses.py\",\n        \"fileContents\": \"import json\\nfrom neurons.validator import Validator\\nfrom targon.dataset import download_dataset\\nfrom targon.types import Endpoints\\n\\nminers = []\\nmodel_name = \\\"NTQAI/Nxcode-CQ-7B-orpo\\\"\\nendpoint = Endpoints.CHAT\\n\\nif __name__ == \\\"__main__\\\":\\n    validator = Validator(run_init=False)\\n    validator.dataset = download_dataset(True)\\n    res = validator.loop.run_until_complete(\\n        validator.query_miners(miners, model_name, endpoint)\\n    )\\n    with open(\\\"results.json\\\", \\\"w\\\") as file:\\n        json.dump(\\n            res,\\n            file,\\n        )\\n        file.flush()\\n    exit()\\n\"\n      },\n      {\n        \"fileName\": \"test_miner.py\",\n        \"fileContents\": \"from typing import List\\nfrom httpx import Timeout\\nimport traceback\\nimport httpx\\nimport openai\\nfrom openai.types.chat import ChatCompletionMessageParam\\nfrom neurons.validator import Validator\\nfrom targon.epistula import generate_header\\nfrom targon.protocol import Endpoints\\n\\n\\nMINER_UID = -1\\n\\nmessages: List[ChatCompletionMessageParam] = [\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"},\\n    {\\n        \\\"role\\\": \\\"user\\\",\\n        \\\"content\\\": f\\\"What is the deffinition of the x y problem \\\",\\n    },\\n]\\nmodel = \\\"NousResearch/Meta-Llama-3.1-8B-Instruct\\\"\\nprompt = \\\"def print_hello_world():\\\"\\n\\n\\ndef create_header_hook(hotkey, axon_hotkey):\\n    def add_headers(request: httpx.Request):\\n        for key, header in generate_header(hotkey, request.read(), axon_hotkey).items():\\n            request.headers[key] = header\\n\\n    return add_headers\\n\\n\\ndef main():\\n    try:\\n        validator = Validator(load_dataset=False)\\n        axon_info = validator.metagraph.axons[MINER_UID]\\n        miner = openai.OpenAI(\\n            base_url=f\\\"http://{axon_info.ip}:{axon_info.port}/v1\\\",\\n            api_key=\\\"sn4\\\",\\n            max_retries=0,\\n            timeout=Timeout(12, connect=5, read=5),\\n            http_client=openai.DefaultHttpxClient(\\n                event_hooks={\\n                    \\\"request\\\": [\\n                        create_header_hook(validator.wallet.hotkey, axon_info.hotkey)\\n                    ]\\n                }\\n            ),\\n        )\\n        res = miner.chat.completions.create(\\n            messages=messages, model=model, stream=True, logprobs=True, max_tokens=200\\n        )\\n        tokens = []\\n        for chunk in res:\\n            if chunk.choices[0].delta.content is None:\\n                continue\\n            choice = chunk.choices[0]\\n            if choice.model_extra is None:\\n                continue\\n            token_ids = choice.model_extra.get(\\\"token_ids\\\") or []\\n            token_id = token_ids[0] if len(token_ids) > 0 else -1\\n            tokens.append(\\n                (\\n                    choice.delta.content or \\\"\\\",\\n                    token_id,\\n                )\\n            )\\n            print(choice.delta.content, token_id)\\n        print(\\n            validator.check_tokens({\\\"messages\\\": messages[:20]}, tokens, Endpoints.CHAT)\\n        )\\n        print(\\n            validator.check_tokens({\\\"messages\\\": messages[:20]}, tokens, Endpoints.CHAT)\\n        )\\n        print(\\n            validator.check_tokens({\\\"messages\\\": messages[:20]}, tokens, Endpoints.CHAT)\\n        )\\n        print(\\n            validator.check_tokens({\\\"messages\\\": messages[:20]}, tokens, Endpoints.CHAT)\\n        )\\n        print(validator.check_tokens({\\\"messages\\\": messages}, tokens, Endpoints.CHAT))\\n        print(validator.check_tokens({\\\"messages\\\": messages}, tokens, Endpoints.CHAT))\\n        print(validator.check_tokens({\\\"messages\\\": messages}, tokens, Endpoints.CHAT))\\n        print(validator.check_tokens({\\\"messages\\\": messages}, tokens, Endpoints.CHAT))\\n        print(validator.check_tokens({\\\"messages\\\": messages}, tokens, Endpoints.CHAT))\\n    except Exception as e:\\n        print(e)\\n        print(traceback.format_exc())\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\"\n      },\n      {\n        \"fileName\": \"view_weights.py\",\n        \"fileContents\": \"from neurons.validator import Validator\\nimport plotext as plt\\n\\nMINER_UIDS = []\\n\\nif __name__ == \\\"__main__\\\":\\n    validator = Validator()\\n    uids, weights = validator.get_weights()\\n    weights = sorted(weights)\\n    plt.scatter(weights)\\n    plt.title(\\\"Weights\\\")  # to apply a title\\n    plt.show()\\n\"\n      },\n      {\n        \"fileName\": \"setup.py\",\n        \"fileContents\": \"from setuptools import setup, find_packages\\n\\n# Define the version directly here instead of importing\\n__version__ = [line.strip() for line in open(\\\"VERSION\\\").readlines()][0]\\n\\nsetup(\\n    name=\\\"targon\\\",\\n    version=__version__,\\n    author=\\\"Manifold Labs\\\",\\n    author_email=\\\"devs@manifold.inc\\\",\\n    description=\\\"The code for SN4 on bittensor\\\",\\n    long_description_content_type=\\\"text/markdown\\\",\\n    url=\\\"http://manifold.inc\\\",\\n    packages=find_packages(),\\n    install_requires=[line.strip() for line in open(\\\"requirements.txt\\\").readlines()],\\n    classifiers=[\\n        \\\"Programming Language :: Python :: 3\\\",\\n        \\\"License :: OSI Approved :: MIT License\\\",\\n        \\\"Operating System :: OS Independent\\\",\\n    ],\\n    python_requires=\\\">=3.10\\\",\\n)\\n\"\n      },\n      {\n        \"fileName\": \"targon\",\n        \"fileContents\": \"\"\n      },\n      {\n        \"fileName\": \"__init__.py\",\n        \"fileContents\": \"from .config import *\\nfrom .dataset import *\\n\\n__version__ = \\\"4.4.2\\\"\\n\\nversion_split = __version__.split(\\\".\\\")\\n__spec_version__ = (\\n    (100000 * int(version_split[0]))\\n    + (1000 * int(version_split[1]))\\n    + (10 * int(version_split[2]))\\n)\\n\"\n      },\n      {\n        \"fileName\": \"cache.py\",\n        \"fileContents\": \"import json\\nimport traceback\\nfrom typing import Any, Dict, List\\nimport bittensor as bt\\n\\n\\ndef load_cache(file_name: str, block: int, miners: List[int]):\\n    miner_tps = {}\\n    try:\\n        with open(file_name, \\\"r\\\") as file:\\n            loaded_data: Dict[str, Any] = json.load(file)\\n            # Only load cache if fresh\\n            if loaded_data.get(\\\"version\\\", 0) < 400000:\\n                raise Exception(\\\"Cache file from older targon version\\\")\\n            if loaded_data.get(\\\"block_saved\\\", 0) > block - 360:\\n                miner_cache: Dict[str, Any] = loaded_data.get(\\\"miner_tps\\\", {})\\n                miner_tps = dict([(int(k), v) for k, v in miner_cache.items()])\\n    except IOError:\\n        bt.logging.info(\\\"No cache file found\\\")\\n    except EOFError:\\n        bt.logging.warning(\\\"Curropted pickle file\\\")\\n    except Exception as e:\\n        bt.logging.error(f\\\"Failed reading cache file: {e}\\\")\\n        bt.logging.error(traceback.format_exc())\\n\\n    for miner in miners:\\n        if miner_tps.get(miner) is None:\\n            miner_tps[miner] = {}\\n    bt.logging.info(\\\"Loading cached data\\\")\\n    return miner_tps\\n\"\n      },\n      {\n        \"fileName\": \"config.py\",\n        \"fileContents\": \"# The MIT License (MIT)\\n# Copyright © 2023 Yuma Rao\\n# Copyright © 2023 Opentensor Foundation\\n# Copyright © 2024 Manifold Labs\\n\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the “Software”), to deal in the Software without restriction, including without limitation\\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\\n# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\\n# the Software.\\n\\n# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\\n# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\\n# DEALINGS IN THE SOFTWARE.\\n\\nimport os\\nimport bittensor as bt\\n\\nimport requests\\nimport dotenv\\n\\n\\ndef str2bool(v):\\n    return v.lower() in (\\\"yes\\\", \\\"true\\\", \\\"t\\\", \\\"1\\\")\\n\\n\\ndotenv.load_dotenv()\\nAUTO_UPDATE = not str2bool(os.getenv(\\\"NO_AUTO_UPDATE\\\", \\\"False\\\"))\\nIMAGE_TAG = os.getenv(\\\"IMAGE_TAG\\\", \\\"latest\\\")\\nHEARTBEAT = str2bool(os.getenv(\\\"HEARTBEAT\\\", \\\"False\\\"))\\nIS_TESTNET = str2bool(os.getenv(\\\"IS_TESTNET\\\", \\\"False\\\"))\\n\\nSLIDING_WINDOW = 30\\n\\n\\ndef validate_config_and_neuron_path(config):\\n    r\\\"\\\"\\\"Checks/validates the config namespace object.\\\"\\\"\\\"\\n    full_path = os.path.expanduser(\\n        \\\"{}/{}/{}/netuid{}/{}\\\".format(\\n            config.logging.logging_dir,\\n            config.wallet.name,\\n            config.wallet.hotkey,\\n            config.netuid,\\n            config.neuron.name,\\n        )\\n    )\\n    bt.logging.info(f\\\"Logging path: {full_path}\\\")\\n    config.neuron.full_path = os.path.expanduser(full_path)\\n    if not os.path.exists(config.neuron.full_path):\\n        os.makedirs(config.neuron.full_path, exist_ok=True)\\n    return config\\n\\n\\ndef add_args(parser):\\n    \\\"\\\"\\\"\\n    Adds relevant arguments to the parser for operation.\\n    \\\"\\\"\\\"\\n    # Netuid Arg: The netuid of the subnet to connect to.\\n    parser.add_argument(\\\"--netuid\\\", type=int, help=\\\"Subnet netuid\\\", default=4)\\n\\n    parser.add_argument(\\n        \\\"--neuron.name\\\",\\n        type=str,\\n        help=\\\"Neuron Name\\\",\\n        default=\\\"targon\\\",\\n    )\\n\\n    parser.add_argument(\\n        \\\"--epoch-length\\\",\\n        type=int,\\n        dest=\\\"epoch_length\\\",\\n        help=\\\"The default epoch length (how often we set weights, measured in 12 second blocks).\\\",\\n        default=360,\\n    )\\n\\n    parser.add_argument(\\n        \\\"--mock\\\",\\n        action=\\\"store_true\\\",\\n        help=\\\"Run in mock mode\\\",\\n        default=False,\\n    )\\n\\n    parser.add_argument(\\n        \\\"--autoupdate-off\\\",\\n        action=\\\"store_false\\\",\\n        dest=\\\"autoupdate\\\",\\n        help=\\\"Disable automatic updates to Targon on latest version on Main.\\\",\\n        default=True,\\n    )\\n\\n\\ndef add_miner_args(parser):\\n    \\\"\\\"\\\"Add miner specific arguments to the parser.\\\"\\\"\\\"\\n\\n    parser.add_argument(\\n        \\\"--model-endpoint\\\",\\n        dest=\\\"model_endpoint\\\",\\n        type=str,\\n        help=\\\"The endpoint to use for the OpenAI Compatible client.\\\",\\n        default=\\\"http://127.0.0.1:8000/v1\\\",\\n    )\\n\\n    parser.add_argument(\\n        \\\"--no-force-validator-permit\\\",\\n        dest=\\\"no_force_validator_permit\\\",\\n        action=\\\"store_true\\\",\\n        help=\\\"If set, we will not force incoming requests to have a permit.\\\",\\n        default=False,\\n    )\\n    parser.add_argument(\\n        \\\"--api-key\\\",\\n        dest=\\\"api_key\\\",\\n        type=str,\\n        help=\\\"API key for openai compatable api\\\",\\n        default=\\\"12345\\\",\\n    )\\n\\n\\ndef add_validator_args(parser):\\n    \\\"\\\"\\\"Add validator specific arguments to the parser.\\\"\\\"\\\"\\n\\n    parser.add_argument(\\n        \\\"--cache-file\\\",\\n        dest=\\\"cache_file\\\",\\n        type=str,\\n        help=\\\"File to save scores, and other misc data that can persist through validator restarts\\\",\\n        default=\\\"cache.json\\\",\\n    )\\n\\n    parser.add_argument(\\n        \\\"--miner-timeout\\\",\\n        dest=\\\"miner_timeout\\\",\\n        type=float,\\n        help=\\\"The timeout for each forward call in seconds.\\\",\\n        default=12,\\n    )\\n\\n    parser.add_argument(\\n        \\\"--vpermit-tao-limit\\\",\\n        dest=\\\"vpermit_tao_limit\\\",\\n        type=int,\\n        help=\\\"The maximum number of TAO allowed to query a validator with a vpermit.\\\",\\n        default=4096,\\n    )\\n\\n    parser.add_argument(\\n        \\\"--database.url\\\",\\n        dest=\\\"database.url\\\",\\n        type=str,\\n        help=\\\"Database URL to score organic queries\\\",\\n        default=None,\\n    )\\n\\n    parser.add_argument(\\n        \\\"--models.mode\\\",\\n        dest=\\\"models.mode\\\",\\n        type=str,\\n        help=\\\"Which method to use when fetching models\\\",\\n        choices=[\\\"endpoint\\\", \\\"config\\\", \\\"default\\\"],\\n        default=\\\"default\\\",\\n    )\\n    parser.add_argument(\\n        \\\"--models.endpoint\\\",\\n        dest=\\\"models.endpoint\\\",\\n        type=str,\\n        help=\\\"Endpoint to query for models\\\",\\n        default=\\\"https://targon.sybil.com/api/models\\\",\\n    )\\n\\n\\ndef get_models_from_endpoint(endpoint: str):\\n    try:\\n        res = requests.get(endpoint)\\n        bt.logging.info(res.text)\\n        res = res.json()\\n        if not isinstance(res, list):\\n            raise Exception(\\n                f\\\"Unexpected type received from endpoint. Must be type list. got {res}\\\"\\n            )\\n        return res\\n    except Exception as e:\\n        bt.logging.error(f\\\"Failed to get models from {endpoint}: {str(e)}\\\")\\n    return None\\n\\n\\ndef get_models_from_config():\\n    filename = \\\"./models.txt\\\"\\n    try:\\n        with open(filename, \\\"r\\\") as file:\\n            models = file.read().strip().split(\\\"\\\\n\\\")\\n            if not len(models):\\n                bt.logging.error(\\\"No models in models file\\\")\\n            else:\\n                bt.logging.info(f\\\"Found models {str(models)}\\\")\\n            return models\\n    except IOError:\\n        bt.logging.info(\\\"No model file found\\\")\\n    except EOFError:\\n        bt.logging.warning(\\\"Curropted models file\\\")\\n    except Exception as e:\\n        bt.logging.error(f\\\"Failed reading model file: {e}\\\")\\n    return None\\n\"\n      },\n      {\n        \"fileName\": \"data\",\n        \"fileContents\": \"\"\n      },\n      {\n        \"fileName\": \"countries.txt\",\n        \"fileContents\": \"Afghanistan\\nAlbania\\nAlgeria\\nAndorra\\nAngola\\nAntigua and Barbuda\\nArgentina\\nArmenia\\nAustralia\\nAustria\\nAzerbaijan\\nThe Bahamas\\nBahrain\\nBangladesh\\nBarbados\\nBelarus\\nBelgium\\nBelize\\nBenin\\nBhutan\\nBolivia\\nBosnia and Herzegovina\\nBotswana\\nBrazil\\nBrunei\\nBulgaria\\nBurkina Faso\\nBurundi\\nCabo Verde\\nCambodia\\nCameroon\\nCanada\\nCentral African Republic\\nChad\\nChile\\nChina\\nColombia\\nComoros\\nCongo, Democratic Republic of the\\nCongo, Republic of the\\nCosta Rica\\nCôte d’Ivoire\\nCroatia\\nCuba\\nCyprus\\nCzech Republic\\nDenmark\\nDjibouti\\nDominica\\nDominican Republic\\nEast Timor (Timor-Leste)\\nEcuador\\nEgypt\\nEl Salvador\\nEquatorial Guinea\\nEritrea\\nEstonia\\nEswatini\\nEthiopia\\nFiji\\nFinland\\nFrance\\nGabon\\nThe Gambia\\nGeorgia\\nGermany\\nGhana\\nGreece\\nGrenada\\nGuatemala\\nGuinea\\nGuinea-Bissau\\nGuyana\\nHaiti\\nHonduras\\nHungary\\nIceland\\nIndia\\nIndonesia\\nIran\\nIraq\\nIreland\\nIsrael\\nItaly\\nJamaica\\nJapan\\nJordan\\nKazakhstan\\nKenya\\nKiribati\\nKorea, North\\nKorea, South\\nKosovo\\nKuwait\\nKyrgyzstan\\nLaos\\nLatvia\\nLebanon\\nLesotho\\nLiberia\\nLibya\\nLiechtenstein\\nLithuania\\nLuxembourg\\nMadagascar\\nMalawi\\nMalaysia\\nMaldives\\nMali\\nMalta\\nMarshall Islands\\nMauritania\\nMauritius\\nMexico\\nMicronesia, Federated States of\\nMoldova\\nMonaco\\nMongolia\\nMontenegro\\nMorocco\\nMozambique\\nMyanmar (Burma)\\nNamibia\\nNauru\\nNepal\\nNetherlands\\nNew Zealand\\nNicaragua\\nNiger\\nNigeria\\nNorth Macedonia\\nNorway\\nOman\\nPakistan\\nPalau\\nPanama\\nPapua New Guinea\\nParaguay\\nPeru\\nPhilippines\\nPoland\\nPortugal\\nQatar\\nRomania\\nRussia\\nRwanda\\nSaint Kitts and Nevis\\nSaint Lucia\\nSaint Vincent and the Grenadines\\nSamoa\\nSan Marino\\nSao Tome and Principe\\nSaudi Arabia\\nSenegal\\nSerbia\\nSeychelles\\nSierra Leone\\nSingapore\\nSlovakia\\nSlovenia\\nSolomon Islands\\nSomalia\\nSouth Africa\\nSpain\\nSri Lanka\\nSudan\\nSudan, South\\nSuriname\\nSweden\\nSwitzerland\\nSyria\\nTaiwan\\nTajikistan\\nTanzania\\nThailand\\nTogo\\nTonga\\nTrinidad and Tobago\\nTunisia\\nTurkey\\nTurkmenistan\\nTuvalu\\nUganda\\nUkraine\\nUnited Arab Emirates\\nUnited Kingdom\\nUnited States\\nUruguay\\nUzbekistan\\nVanuatu\\nVatican City\\nVenezuela\\nVietnam\\nYemen\\nZambia\\nZimbabwe\\n\"\n      },\n      {\n        \"fileName\": \"names.txt\",\n        \"fileContents\": \"Aaren\\nAarika\\nAbagael\\nAbagail\\nAbbe\\nAbbey\\nAbbi\\nAbbie\\nAbby\\nAbbye\\nAbigael\\nAbigail\\nAbigale\\nAbra\\nAda\\nAdah\\nAdaline\\nAdan\\nAdara\\nAdda\\nAddi\\nAddia\\nAddie\\nAddy\\nAdel\\nAdela\\nAdelaida\\nAdelaide\\nAdele\\nAdelheid\\nAdelice\\nAdelina\\nAdelind\\nAdeline\\nAdella\\nAdelle\\nAdena\\nAdey\\nAdi\\nAdiana\\nAdina\\nAdora\\nAdore\\nAdoree\\nAdorne\\nAdrea\\nAdria\\nAdriaens\\nAdrian\\nAdriana\\nAdriane\\nAdrianna\\nAdrianne\\nAdriena\\nAdrienne\\nAeriel\\nAeriela\\nAeriell\\nAfton\\nAg\\nAgace\\nAgata\\nAgatha\\nAgathe\\nAggi\\nAggie\\nAggy\\nAgna\\nAgnella\\nAgnes\\nAgnese\\nAgnesse\\nAgneta\\nAgnola\\nAgretha\\nAida\\nAidan\\nAigneis\\nAila\\nAile\\nAilee\\nAileen\\nAilene\\nAiley\\nAili\\nAilina\\nAilis\\nAilsun\\nAilyn\\nAime\\nAimee\\nAimil\\nAindrea\\nAinslee\\nAinsley\\nAinslie\\nAjay\\nAlaine\\nAlameda\\nAlana\\nAlanah\\nAlane\\nAlanna\\nAlayne\\nAlberta\\nAlbertina\\nAlbertine\\nAlbina\\nAlecia\\nAleda\\nAleece\\nAleen\\nAlejandra\\nAlejandrina\\nAlena\\nAlene\\nAlessandra\\nAleta\\nAlethea\\nAlex\\nAlexa\\nAlexandra\\nAlexandrina\\nAlexi\\nAlexia\\nAlexina\\nAlexine\\nAlexis\\nAlfi\\nAlfie\\nAlfreda\\nAlfy\\nAli\\nAlia\\nAlica\\nAlice\\nAlicea\\nAlicia\\nAlida\\nAlidia\\nAlie\\nAlika\\nAlikee\\nAlina\\nAline\\nAlis\\nAlisa\\nAlisha\\nAlison\\nAlissa\\nAlisun\\nAlix\\nAliza\\nAlla\\nAlleen\\nAllegra\\nAllene\\nAlli\\nAllianora\\nAllie\\nAllina\\nAllis\\nAllison\\nAllissa\\nAllix\\nAllsun\\nAllx\\nAlly\\nAllyce\\nAllyn\\nAllys\\nAllyson\\nAlma\\nAlmeda\\nAlmeria\\nAlmeta\\nAlmira\\nAlmire\\nAloise\\nAloisia\\nAloysia\\nAlta\\nAlthea\\nAlvera\\nAlverta\\nAlvina\\nAlvinia\\nAlvira\\nAlyce\\nAlyda\\nAlys\\nAlysa\\nAlyse\\nAlysia\\nAlyson\\nAlyss\\nAlyssa\\nAmabel\\nAmabelle\\nAmalea\\nAmalee\\nAmaleta\\nAmalia\\nAmalie\\nAmalita\\nAmalle\\nAmanda\\nAmandi\\nAmandie\\nAmandy\\nAmara\\nAmargo\\nAmata\\nAmber\\nAmberly\\nAmbur\\nAme\\nAmelia\\nAmelie\\nAmelina\\nAmeline\\nAmelita\\nAmi\\nAmie\\nAmii\\nAmil\\nAmitie\\nAmity\\nAmmamaria\\nAmy\\nAmye\\nAna\\nAnabal\\nAnabel\\nAnabella\\nAnabelle\\nAnaliese\\nAnalise\\nAnallese\\nAnallise\\nAnastasia\\nAnastasie\\nAnastassia\\nAnatola\\nAndee\\nAndeee\\nAnderea\\nAndi\\nAndie\\nAndra\\nAndrea\\nAndreana\\nAndree\\nAndrei\\nAndria\\nAndriana\\nAndriette\\nAndromache\\nAndy\\nAnestassia\\nAnet\\nAnett\\nAnetta\\nAnette\\nAnge\\nAngel\\nAngela\\nAngele\\nAngelia\\nAngelica\\nAngelika\\nAngelina\\nAngeline\\nAngelique\\nAngelita\\nAngelle\\nAngie\\nAngil\\nAngy\\nAnia\\nAnica\\nAnissa\\nAnita\\nAnitra\\nAnjanette\\nAnjela\\nAnn\\nAnn-Marie\\nAnna\\nAnna-Diana\\nAnna-Diane\\nAnna-Maria\\nAnnabal\\nAnnabel\\nAnnabela\\nAnnabell\\nAnnabella\\nAnnabelle\\nAnnadiana\\nAnnadiane\\nAnnalee\\nAnnaliese\\nAnnalise\\nAnnamaria\\nAnnamarie\\nAnne\\nAnne-Corinne\\nAnne-Marie\\nAnnecorinne\\nAnneliese\\nAnnelise\\nAnnemarie\\nAnnetta\\nAnnette\\nAnni\\nAnnice\\nAnnie\\nAnnis\\nAnnissa\\nAnnmaria\\nAnnmarie\\nAnnnora\\nAnnora\\nAnny\\nAnselma\\nAnsley\\nAnstice\\nAnthe\\nAnthea\\nAnthia\\nAnthiathia\\nAntoinette\\nAntonella\\nAntonetta\\nAntonia\\nAntonie\\nAntonietta\\nAntonina\\nAnya\\nAppolonia\\nApril\\nAprilette\\nAra\\nArabel\\nArabela\\nArabele\\nArabella\\nArabelle\\nArda\\nArdath\\nArdeen\\nArdelia\\nArdelis\\nArdella\\nArdelle\\nArden\\nArdene\\nArdenia\\nArdine\\nArdis\\nArdisj\\nArdith\\nArdra\\nArdyce\\nArdys\\nArdyth\\nAretha\\nAriadne\\nAriana\\nAridatha\\nAriel\\nAriela\\nAriella\\nArielle\\nArlana\\nArlee\\nArleen\\nArlen\\nArlena\\nArlene\\nArleta\\nArlette\\nArleyne\\nArlie\\nArliene\\nArlina\\nArlinda\\nArline\\nArluene\\nArly\\nArlyn\\nArlyne\\nAryn\\nAshely\\nAshia\\nAshien\\nAshil\\nAshla\\nAshlan\\nAshlee\\nAshleigh\\nAshlen\\nAshley\\nAshli\\nAshlie\\nAshly\\nAsia\\nAstra\\nAstrid\\nAstrix\\nAtalanta\\nAthena\\nAthene\\nAtlanta\\nAtlante\\nAuberta\\nAubine\\nAubree\\nAubrette\\nAubrey\\nAubrie\\nAubry\\nAudi\\nAudie\\nAudra\\nAudre\\nAudrey\\nAudrie\\nAudry\\nAudrye\\nAudy\\nAugusta\\nAuguste\\nAugustina\\nAugustine\\nAundrea\\nAura\\nAurea\\nAurel\\nAurelea\\nAurelia\\nAurelie\\nAuria\\nAurie\\nAurilia\\nAurlie\\nAuroora\\nAurora\\nAurore\\nAustin\\nAustina\\nAustine\\nAva\\nAveline\\nAveril\\nAveryl\\nAvie\\nAvis\\nAviva\\nAvivah\\nAvril\\nAvrit\\nAyn\\nBab\\nBabara\\nBabb\\nBabbette\\nBabbie\\nBabette\\nBabita\\nBabs\\nBambi\\nBambie\\nBamby\\nBarb\\nBarbabra\\nBarbara\\nBarbara-Anne\\nBarbaraanne\\nBarbe\\nBarbee\\nBarbette\\nBarbey\\nBarbi\\nBarbie\\nBarbra\\nBarby\\nBari\\nBarrie\\nBarry\\nBasia\\nBathsheba\\nBatsheva\\nBea\\nBeatrice\\nBeatrisa\\nBeatrix\\nBeatriz\\nBebe\\nBecca\\nBecka\\nBecki\\nBeckie\\nBecky\\nBee\\nBeilul\\nBeitris\\nBekki\\nBel\\nBelia\\nBelicia\\nBelinda\\nBelita\\nBell\\nBella\\nBellanca\\nBelle\\nBellina\\nBelva\\nBelvia\\nBendite\\nBenedetta\\nBenedicta\\nBenedikta\\nBenetta\\nBenita\\nBenni\\nBennie\\nBenny\\nBenoite\\nBerenice\\nBeret\\nBerget\\nBerna\\nBernadene\\nBernadette\\nBernadina\\nBernadine\\nBernardina\\nBernardine\\nBernelle\\nBernete\\nBernetta\\nBernette\\nBerni\\nBernice\\nBernie\\nBernita\\nBerny\\nBerri\\nBerrie\\nBerry\\nBert\\nBerta\\nBerte\\nBertha\\nBerthe\\nBerti\\nBertie\\nBertina\\nBertine\\nBerty\\nBeryl\\nBeryle\\nBess\\nBessie\\nBessy\\nBeth\\nBethanne\\nBethany\\nBethena\\nBethina\\nBetsey\\nBetsy\\nBetta\\nBette\\nBette-Ann\\nBetteann\\nBetteanne\\nBetti\\nBettina\\nBettine\\nBetty\\nBettye\\nBeulah\\nBev\\nBeverie\\nBeverlee\\nBeverley\\nBeverlie\\nBeverly\\nBevvy\\nBianca\\nBianka\\nBibbie\\nBibby\\nBibbye\\nBibi\\nBiddie\\nBiddy\\nBidget\\nBili\\nBill\\nBilli\\nBillie\\nBilly\\nBillye\\nBinni\\nBinnie\\nBinny\\nBird\\nBirdie\\nBirgit\\nBirgitta\\nBlair\\nBlaire\\nBlake\\nBlakelee\\nBlakeley\\nBlanca\\nBlanch\\nBlancha\\nBlanche\\nBlinni\\nBlinnie\\nBlinny\\nBliss\\nBlisse\\nBlithe\\nBlondell\\nBlondelle\\nBlondie\\nBlondy\\nBlythe\\nBobbe\\nBobbee\\nBobbette\\nBobbi\\nBobbie\\nBobby\\nBobbye\\nBobette\\nBobina\\nBobine\\nBobinette\\nBonita\\nBonnee\\nBonni\\nBonnibelle\\nBonnie\\nBonny\\nBrana\\nBrandais\\nBrande\\nBrandea\\nBrandi\\nBrandice\\nBrandie\\nBrandise\\nBrandy\\nBreanne\\nBrear\\nBree\\nBreena\\nBren\\nBrena\\nBrenda\\nBrenn\\nBrenna\\nBrett\\nBria\\nBriana\\nBrianna\\nBrianne\\nBride\\nBridget\\nBridgette\\nBridie\\nBrier\\nBrietta\\nBrigid\\nBrigida\\nBrigit\\nBrigitta\\nBrigitte\\nBrina\\nBriney\\nBrinn\\nBrinna\\nBriny\\nBrit\\nBrita\\nBritney\\nBritni\\nBritt\\nBritta\\nBrittan\\nBrittaney\\nBrittani\\nBrittany\\nBritte\\nBritteny\\nBrittne\\nBrittney\\nBrittni\\nBrook\\nBrooke\\nBrooks\\nBrunhilda\\nBrunhilde\\nBryana\\nBryn\\nBryna\\nBrynn\\nBrynna\\nBrynne\\nBuffy\\nBunni\\nBunnie\\nBunny\\nCacilia\\nCacilie\\nCahra\\nCairistiona\\nCaitlin\\nCaitrin\\nCal\\nCalida\\nCalla\\nCalley\\nCalli\\nCallida\\nCallie\\nCally\\nCalypso\\nCam\\nCamala\\nCamel\\nCamella\\nCamellia\\nCami\\nCamila\\nCamile\\nCamilla\\nCamille\\nCammi\\nCammie\\nCammy\\nCandace\\nCandi\\nCandice\\nCandida\\nCandide\\nCandie\\nCandis\\nCandra\\nCandy\\nCaprice\\nCara\\nCaralie\\nCaren\\nCarena\\nCaresa\\nCaressa\\nCaresse\\nCarey\\nCari\\nCaria\\nCarie\\nCaril\\nCarilyn\\nCarin\\nCarina\\nCarine\\nCariotta\\nCarissa\\nCarita\\nCaritta\\nCarla\\nCarlee\\nCarleen\\nCarlen\\nCarlene\\nCarley\\nCarlie\\nCarlin\\nCarlina\\nCarline\\nCarlita\\nCarlota\\nCarlotta\\nCarly\\nCarlye\\nCarlyn\\nCarlynn\\nCarlynne\\nCarma\\nCarmel\\nCarmela\\nCarmelia\\nCarmelina\\nCarmelita\\nCarmella\\nCarmelle\\nCarmen\\nCarmencita\\nCarmina\\nCarmine\\nCarmita\\nCarmon\\nCaro\\nCarol\\nCarol-Jean\\nCarola\\nCarolan\\nCarolann\\nCarole\\nCarolee\\nCarolin\\nCarolina\\nCaroline\\nCaroljean\\nCarolyn\\nCarolyne\\nCarolynn\\nCaron\\nCarree\\nCarri\\nCarrie\\nCarrissa\\nCarroll\\nCarry\\nCary\\nCaryl\\nCaryn\\nCasandra\\nCasey\\nCasi\\nCasie\\nCass\\nCassandra\\nCassandre\\nCassandry\\nCassaundra\\nCassey\\nCassi\\nCassie\\nCassondra\\nCassy\\nCatarina\\nCate\\nCaterina\\nCatha\\nCatharina\\nCatharine\\nCathe\\nCathee\\nCatherin\\nCatherina\\nCatherine\\nCathi\\nCathie\\nCathleen\\nCathlene\\nCathrin\\nCathrine\\nCathryn\\nCathy\\nCathyleen\\nCati\\nCatie\\nCatina\\nCatlaina\\nCatlee\\nCatlin\\nCatrina\\nCatriona\\nCaty\\nCaye\\nCayla\\nCecelia\\nCecil\\nCecile\\nCeciley\\nCecilia\\nCecilla\\nCecily\\nCeil\\nCele\\nCelene\\nCelesta\\nCeleste\\nCelestia\\nCelestina\\nCelestine\\nCelestyn\\nCelestyna\\nCelia\\nCelie\\nCelina\\nCelinda\\nCeline\\nCelinka\\nCelisse\\nCelka\\nCelle\\nCesya\\nChad\\nChanda\\nChandal\\nChandra\\nChanna\\nChantal\\nChantalle\\nCharil\\nCharin\\nCharis\\nCharissa\\nCharisse\\nCharita\\nCharity\\nCharla\\nCharlean\\nCharleen\\nCharlena\\nCharlene\\nCharline\\nCharlot\\nCharlotta\\nCharlotte\\nCharmain\\nCharmaine\\nCharmane\\nCharmian\\nCharmine\\nCharmion\\nCharo\\nCharyl\\nChastity\\nChelsae\\nChelsea\\nChelsey\\nChelsie\\nChelsy\\nCher\\nChere\\nCherey\\nCheri\\nCherianne\\nCherice\\nCherida\\nCherie\\nCherilyn\\nCherilynn\\nCherin\\nCherise\\nCherish\\nCherlyn\\nCherri\\nCherrita\\nCherry\\nChery\\nCherye\\nCheryl\\nCheslie\\nChiarra\\nChickie\\nChicky\\nChiquia\\nChiquita\\nChlo\\nChloe\\nChloette\\nChloris\\nChris\\nChrissie\\nChrissy\\nChrista\\nChristabel\\nChristabella\\nChristal\\nChristalle\\nChristan\\nChristean\\nChristel\\nChristen\\nChristi\\nChristian\\nChristiana\\nChristiane\\nChristie\\nChristin\\nChristina\\nChristine\\nChristy\\nChristye\\nChristyna\\nChrysa\\nChrysler\\nChrystal\\nChryste\\nChrystel\\nCicely\\nCicily\\nCiel\\nCilka\\nCinda\\nCindee\\nCindelyn\\nCinderella\\nCindi\\nCindie\\nCindra\\nCindy\\nCinnamon\\nCissiee\\nCissy\\nClair\\nClaire\\nClara\\nClarabelle\\nClare\\nClaresta\\nClareta\\nClaretta\\nClarette\\nClarey\\nClari\\nClaribel\\nClarice\\nClarie\\nClarinda\\nClarine\\nClarissa\\nClarisse\\nClarita\\nClary\\nClaude\\nClaudelle\\nClaudetta\\nClaudette\\nClaudia\\nClaudie\\nClaudina\\nClaudine\\nClea\\nClem\\nClemence\\nClementia\\nClementina\\nClementine\\nClemmie\\nClemmy\\nCleo\\nCleopatra\\nClerissa\\nClio\\nClo\\nCloe\\nCloris\\nClotilda\\nClovis\\nCodee\\nCodi\\nCodie\\nCody\\nColeen\\nColene\\nColetta\\nColette\\nColleen\\nCollen\\nCollete\\nCollette\\nCollie\\nColline\\nColly\\nCon\\nConcettina\\nConchita\\nConcordia\\nConni\\nConnie\\nConny\\nConsolata\\nConstance\\nConstancia\\nConstancy\\nConstanta\\nConstantia\\nConstantina\\nConstantine\\nConsuela\\nConsuelo\\nCookie\\nCora\\nCorabel\\nCorabella\\nCorabelle\\nCoral\\nCoralie\\nCoraline\\nCoralyn\\nCordelia\\nCordelie\\nCordey\\nCordi\\nCordie\\nCordula\\nCordy\\nCoreen\\nCorella\\nCorenda\\nCorene\\nCoretta\\nCorette\\nCorey\\nCori\\nCorie\\nCorilla\\nCorina\\nCorine\\nCorinna\\nCorinne\\nCoriss\\nCorissa\\nCorliss\\nCorly\\nCornela\\nCornelia\\nCornelle\\nCornie\\nCorny\\nCorrena\\nCorrey\\nCorri\\nCorrianne\\nCorrie\\nCorrina\\nCorrine\\nCorrinne\\nCorry\\nCortney\\nCory\\nCosetta\\nCosette\\nCostanza\\nCourtenay\\nCourtnay\\nCourtney\\nCrin\\nCris\\nCrissie\\nCrissy\\nCrista\\nCristabel\\nCristal\\nCristen\\nCristi\\nCristie\\nCristin\\nCristina\\nCristine\\nCristionna\\nCristy\\nCrysta\\nCrystal\\nCrystie\\nCthrine\\nCyb\\nCybil\\nCybill\\nCymbre\\nCynde\\nCyndi\\nCyndia\\nCyndie\\nCyndy\\nCynthea\\nCynthia\\nCynthie\\nCynthy\\nDacey\\nDacia\\nDacie\\nDacy\\nDael\\nDaffi\\nDaffie\\nDaffy\\nDagmar\\nDahlia\\nDaile\\nDaisey\\nDaisi\\nDaisie\\nDaisy\\nDale\\nDalenna\\nDalia\\nDalila\\nDallas\\nDaloris\\nDamara\\nDamaris\\nDamita\\nDana\\nDanell\\nDanella\\nDanette\\nDani\\nDania\\nDanica\\nDanice\\nDaniela\\nDaniele\\nDaniella\\nDanielle\\nDanika\\nDanila\\nDanit\\nDanita\\nDanna\\nDanni\\nDannie\\nDanny\\nDannye\\nDanya\\nDanyelle\\nDanyette\\nDaphene\\nDaphna\\nDaphne\\nDara\\nDarb\\nDarbie\\nDarby\\nDarcee\\nDarcey\\nDarci\\nDarcie\\nDarcy\\nDarda\\nDareen\\nDarell\\nDarelle\\nDari\\nDaria\\nDarice\\nDarla\\nDarleen\\nDarlene\\nDarline\\nDarlleen\\nDaron\\nDarrelle\\nDarryl\\nDarsey\\nDarsie\\nDarya\\nDaryl\\nDaryn\\nDasha\\nDasi\\nDasie\\nDasya\\nDatha\\nDaune\\nDaveen\\nDaveta\\nDavida\\nDavina\\nDavine\\nDavita\\nDawn\\nDawna\\nDayle\\nDayna\\nDdene\\nDe\\nDeana\\nDeane\\nDeanna\\nDeanne\\nDeb\\nDebbi\\nDebbie\\nDebby\\nDebee\\nDebera\\nDebi\\nDebor\\nDebora\\nDeborah\\nDebra\\nDede\\nDedie\\nDedra\\nDee\\nDee Dee\\nDeeann\\nDeeanne\\nDeedee\\nDeena\\nDeerdre\\nDeeyn\\nDehlia\\nDeidre\\nDeina\\nDeirdre\\nDel\\nDela\\nDelcina\\nDelcine\\nDelia\\nDelila\\nDelilah\\nDelinda\\nDell\\nDella\\nDelly\\nDelora\\nDelores\\nDeloria\\nDeloris\\nDelphine\\nDelphinia\\nDemeter\\nDemetra\\nDemetria\\nDemetris\\nDena\\nDeni\\nDenice\\nDenise\\nDenna\\nDenni\\nDennie\\nDenny\\nDeny\\nDenys\\nDenyse\\nDeonne\\nDesdemona\\nDesirae\\nDesiree\\nDesiri\\nDeva\\nDevan\\nDevi\\nDevin\\nDevina\\nDevinne\\nDevon\\nDevondra\\nDevonna\\nDevonne\\nDevora\\nDi\\nDiahann\\nDian\\nDiana\\nDiandra\\nDiane\\nDiane-Marie\\nDianemarie\\nDiann\\nDianna\\nDianne\\nDiannne\\nDidi\\nDido\\nDiena\\nDierdre\\nDina\\nDinah\\nDinnie\\nDinny\\nDion\\nDione\\nDionis\\nDionne\\nDita\\nDix\\nDixie\\nDniren\\nDode\\nDodi\\nDodie\\nDody\\nDoe\\nDoll\\nDolley\\nDolli\\nDollie\\nDolly\\nDolores\\nDolorita\\nDoloritas\\nDomeniga\\nDominga\\nDomini\\nDominica\\nDominique\\nDona\\nDonella\\nDonelle\\nDonetta\\nDonia\\nDonica\\nDonielle\\nDonna\\nDonnamarie\\nDonni\\nDonnie\\nDonny\\nDora\\nDoralia\\nDoralin\\nDoralyn\\nDoralynn\\nDoralynne\\nDore\\nDoreen\\nDorelia\\nDorella\\nDorelle\\nDorena\\nDorene\\nDoretta\\nDorette\\nDorey\\nDori\\nDoria\\nDorian\\nDorice\\nDorie\\nDorine\\nDoris\\nDorisa\\nDorise\\nDorita\\nDoro\\nDorolice\\nDorolisa\\nDorotea\\nDoroteya\\nDorothea\\nDorothee\\nDorothy\\nDorree\\nDorri\\nDorrie\\nDorris\\nDorry\\nDorthea\\nDorthy\\nDory\\nDosi\\nDot\\nDoti\\nDotti\\nDottie\\nDotty\\nDre\\nDreddy\\nDredi\\nDrona\\nDru\\nDruci\\nDrucie\\nDrucill\\nDrucy\\nDrusi\\nDrusie\\nDrusilla\\nDrusy\\nDulce\\nDulcea\\nDulci\\nDulcia\\nDulciana\\nDulcie\\nDulcine\\nDulcinea\\nDulcy\\nDulsea\\nDusty\\nDyan\\nDyana\\nDyane\\nDyann\\nDyanna\\nDyanne\\nDyna\\nDynah\\nEachelle\\nEada\\nEadie\\nEadith\\nEalasaid\\nEartha\\nEaster\\nEba\\nEbba\\nEbonee\\nEbony\\nEda\\nEddi\\nEddie\\nEddy\\nEde\\nEdee\\nEdeline\\nEden\\nEdi\\nEdie\\nEdin\\nEdita\\nEdith\\nEditha\\nEdithe\\nEdiva\\nEdna\\nEdwina\\nEdy\\nEdyth\\nEdythe\\nEffie\\nEileen\\nEilis\\nEimile\\nEirena\\nEkaterina\\nElaina\\nElaine\\nElana\\nElane\\nElayne\\nElberta\\nElbertina\\nElbertine\\nEleanor\\nEleanora\\nEleanore\\nElectra\\nEleen\\nElena\\nElene\\nEleni\\nElenore\\nEleonora\\nEleonore\\nElfie\\nElfreda\\nElfrida\\nElfrieda\\nElga\\nElianora\\nElianore\\nElicia\\nElie\\nElinor\\nElinore\\nElisa\\nElisabet\\nElisabeth\\nElisabetta\\nElise\\nElisha\\nElissa\\nElita\\nEliza\\nElizabet\\nElizabeth\\nElka\\nElke\\nElla\\nElladine\\nElle\\nEllen\\nEllene\\nEllette\\nElli\\nEllie\\nEllissa\\nElly\\nEllyn\\nEllynn\\nElmira\\nElna\\nElnora\\nElnore\\nEloisa\\nEloise\\nElonore\\nElora\\nElsa\\nElsbeth\\nElse\\nElset\\nElsey\\nElsi\\nElsie\\nElsinore\\nElspeth\\nElsy\\nElva\\nElvera\\nElvina\\nElvira\\nElwira\\nElyn\\nElyse\\nElysee\\nElysha\\nElysia\\nElyssa\\nEm\\nEma\\nEmalee\\nEmalia\\nEmelda\\nEmelia\\nEmelina\\nEmeline\\nEmelita\\nEmelyne\\nEmera\\nEmilee\\nEmili\\nEmilia\\nEmilie\\nEmiline\\nEmily\\nEmlyn\\nEmlynn\\nEmlynne\\nEmma\\nEmmalee\\nEmmaline\\nEmmalyn\\nEmmalynn\\nEmmalynne\\nEmmeline\\nEmmey\\nEmmi\\nEmmie\\nEmmy\\nEmmye\\nEmogene\\nEmyle\\nEmylee\\nEngracia\\nEnid\\nEnrica\\nEnrichetta\\nEnrika\\nEnriqueta\\nEolanda\\nEolande\\nEran\\nErda\\nErena\\nErica\\nEricha\\nEricka\\nErika\\nErin\\nErina\\nErinn\\nErinna\\nErma\\nErmengarde\\nErmentrude\\nErmina\\nErminia\\nErminie\\nErna\\nErnaline\\nErnesta\\nErnestine\\nErtha\\nEryn\\nEsma\\nEsmaria\\nEsme\\nEsmeralda\\nEssa\\nEssie\\nEssy\\nEsta\\nEstel\\nEstele\\nEstell\\nEstella\\nEstelle\\nEster\\nEsther\\nEstrella\\nEstrellita\\nEthel\\nEthelda\\nEthelin\\nEthelind\\nEtheline\\nEthelyn\\nEthyl\\nEtta\\nEtti\\nEttie\\nEtty\\nEudora\\nEugenia\\nEugenie\\nEugine\\nEula\\nEulalie\\nEunice\\nEuphemia\\nEustacia\\nEva\\nEvaleen\\nEvangelia\\nEvangelin\\nEvangelina\\nEvangeline\\nEvania\\nEvanne\\nEve\\nEveleen\\nEvelina\\nEveline\\nEvelyn\\nEvey\\nEvie\\nEvita\\nEvonne\\nEvvie\\nEvvy\\nEvy\\nEyde\\nEydie\\nEzmeralda\\nFae\\nFaina\\nFaith\\nFallon\\nFan\\nFanchette\\nFanchon\\nFancie\\nFancy\\nFanechka\\nFania\\nFanni\\nFannie\\nFanny\\nFanya\\nFara\\nFarah\\nFarand\\nFarica\\nFarra\\nFarrah\\nFarrand\\nFaun\\nFaunie\\nFaustina\\nFaustine\\nFawn\\nFawne\\nFawnia\\nFay\\nFaydra\\nFaye\\nFayette\\nFayina\\nFayre\\nFayth\\nFaythe\\nFederica\\nFedora\\nFelecia\\nFelicdad\\nFelice\\nFelicia\\nFelicity\\nFelicle\\nFelipa\\nFelisha\\nFelita\\nFeliza\\nFenelia\\nFeodora\\nFerdinanda\\nFerdinande\\nFern\\nFernanda\\nFernande\\nFernandina\\nFerne\\nFey\\nFiann\\nFianna\\nFidela\\nFidelia\\nFidelity\\nFifi\\nFifine\\nFilia\\nFilide\\nFilippa\\nFina\\nFiona\\nFionna\\nFionnula\\nFiorenze\\nFleur\\nFleurette\\nFlo\\nFlor\\nFlora\\nFlorance\\nFlore\\nFlorella\\nFlorence\\nFlorencia\\nFlorentia\\nFlorenza\\nFlorette\\nFlori\\nFloria\\nFlorida\\nFlorie\\nFlorina\\nFlorinda\\nFloris\\nFlorri\\nFlorrie\\nFlorry\\nFlory\\nFlossi\\nFlossie\\nFlossy\\nFlss\\nFran\\nFrancene\\nFrances\\nFrancesca\\nFrancine\\nFrancisca\\nFranciska\\nFrancoise\\nFrancyne\\nFrank\\nFrankie\\nFranky\\nFranni\\nFrannie\\nFranny\\nFrayda\\nFred\\nFreda\\nFreddi\\nFreddie\\nFreddy\\nFredelia\\nFrederica\\nFredericka\\nFrederique\\nFredi\\nFredia\\nFredra\\nFredrika\\nFreida\\nFrieda\\nFriederike\\nFulvia\\nGabbey\\nGabbi\\nGabbie\\nGabey\\nGabi\\nGabie\\nGabriel\\nGabriela\\nGabriell\\nGabriella\\nGabrielle\\nGabriellia\\nGabrila\\nGaby\\nGae\\nGael\\nGail\\nGale\\nGalina\\nGarland\\nGarnet\\nGarnette\\nGates\\nGavra\\nGavrielle\\nGay\\nGaye\\nGayel\\nGayla\\nGayle\\nGayleen\\nGaylene\\nGaynor\\nGelya\\nGena\\nGene\\nGeneva\\nGenevieve\\nGenevra\\nGenia\\nGenna\\nGenni\\nGennie\\nGennifer\\nGenny\\nGenovera\\nGenvieve\\nGeorge\\nGeorgeanna\\nGeorgeanne\\nGeorgena\\nGeorgeta\\nGeorgetta\\nGeorgette\\nGeorgia\\nGeorgiana\\nGeorgianna\\nGeorgianne\\nGeorgie\\nGeorgina\\nGeorgine\\nGeralda\\nGeraldine\\nGerda\\nGerhardine\\nGeri\\nGerianna\\nGerianne\\nGerladina\\nGermain\\nGermaine\\nGermana\\nGerri\\nGerrie\\nGerrilee\\nGerry\\nGert\\nGerta\\nGerti\\nGertie\\nGertrud\\nGertruda\\nGertrude\\nGertrudis\\nGerty\\nGiacinta\\nGiana\\nGianina\\nGianna\\nGigi\\nGilberta\\nGilberte\\nGilbertina\\nGilbertine\\nGilda\\nGilemette\\nGill\\nGillan\\nGilli\\nGillian\\nGillie\\nGilligan\\nGilly\\nGina\\nGinelle\\nGinevra\\nGinger\\nGinni\\nGinnie\\nGinnifer\\nGinny\\nGiorgia\\nGiovanna\\nGipsy\\nGiralda\\nGisela\\nGisele\\nGisella\\nGiselle\\nGiuditta\\nGiulia\\nGiulietta\\nGiustina\\nGizela\\nGlad\\nGladi\\nGladys\\nGleda\\nGlen\\nGlenda\\nGlenine\\nGlenn\\nGlenna\\nGlennie\\nGlennis\\nGlori\\nGloria\\nGloriana\\nGloriane\\nGlory\\nGlyn\\nGlynda\\nGlynis\\nGlynnis\\nGnni\\nGodiva\\nGolda\\nGoldarina\\nGoldi\\nGoldia\\nGoldie\\nGoldina\\nGoldy\\nGrace\\nGracia\\nGracie\\nGrata\\nGratia\\nGratiana\\nGray\\nGrayce\\nGrazia\\nGreer\\nGreta\\nGretal\\nGretchen\\nGrete\\nGretel\\nGrethel\\nGretna\\nGretta\\nGrier\\nGriselda\\nGrissel\\nGuendolen\\nGuenevere\\nGuenna\\nGuglielma\\nGui\\nGuillema\\nGuillemette\\nGuinevere\\nGuinna\\nGunilla\\nGus\\nGusella\\nGussi\\nGussie\\nGussy\\nGusta\\nGusti\\nGustie\\nGusty\\nGwen\\nGwendolen\\nGwendolin\\nGwendolyn\\nGweneth\\nGwenette\\nGwenneth\\nGwenni\\nGwennie\\nGwenny\\nGwenora\\nGwenore\\nGwyn\\nGwyneth\\nGwynne\\nGypsy\\nHadria\\nHailee\\nHaily\\nHaleigh\\nHalette\\nHaley\\nHali\\nHalie\\nHalimeda\\nHalley\\nHalli\\nHallie\\nHally\\nHana\\nHanna\\nHannah\\nHanni\\nHannie\\nHannis\\nHanny\\nHappy\\nHarlene\\nHarley\\nHarli\\nHarlie\\nHarmonia\\nHarmonie\\nHarmony\\nHarri\\nHarrie\\nHarriet\\nHarriett\\nHarrietta\\nHarriette\\nHarriot\\nHarriott\\nHatti\\nHattie\\nHatty\\nHayley\\nHazel\\nHeath\\nHeather\\nHeda\\nHedda\\nHeddi\\nHeddie\\nHedi\\nHedvig\\nHedvige\\nHedwig\\nHedwiga\\nHedy\\nHeida\\nHeidi\\nHeidie\\nHelaina\\nHelaine\\nHelen\\nHelen-Elizabeth\\nHelena\\nHelene\\nHelenka\\nHelga\\nHelge\\nHelli\\nHeloise\\nHelsa\\nHelyn\\nHendrika\\nHenka\\nHenrie\\nHenrieta\\nHenrietta\\nHenriette\\nHenryetta\\nHephzibah\\nHermia\\nHermina\\nHermine\\nHerminia\\nHermione\\nHerta\\nHertha\\nHester\\nHesther\\nHestia\\nHetti\\nHettie\\nHetty\\nHilary\\nHilda\\nHildagard\\nHildagarde\\nHilde\\nHildegaard\\nHildegarde\\nHildy\\nHillary\\nHilliary\\nHinda\\nHolli\\nHollie\\nHolly\\nHolly-Anne\\nHollyanne\\nHoney\\nHonor\\nHonoria\\nHope\\nHoratia\\nHortense\\nHortensia\\nHulda\\nHyacinth\\nHyacintha\\nHyacinthe\\nHyacinthia\\nHyacinthie\\nHynda\\nIanthe\\nIbbie\\nIbby\\nIda\\nIdalia\\nIdalina\\nIdaline\\nIdell\\nIdelle\\nIdette\\nIleana\\nIleane\\nIlene\\nIlise\\nIlka\\nIlla\\nIlsa\\nIlse\\nIlysa\\nIlyse\\nIlyssa\\nImelda\\nImogen\\nImogene\\nImojean\\nIna\\nIndira\\nInes\\nInesita\\nInessa\\nInez\\nInga\\nIngaberg\\nIngaborg\\nInge\\nIngeberg\\nIngeborg\\nInger\\nIngrid\\nIngunna\\nInna\\nIolande\\nIolanthe\\nIona\\nIormina\\nIra\\nIrena\\nIrene\\nIrina\\nIris\\nIrita\\nIrma\\nIsa\\nIsabel\\nIsabelita\\nIsabella\\nIsabelle\\nIsadora\\nIsahella\\nIseabal\\nIsidora\\nIsis\\nIsobel\\nIssi\\nIssie\\nIssy\\nIvett\\nIvette\\nIvie\\nIvonne\\nIvory\\nIvy\\nIzabel\\nJacenta\\nJacinda\\nJacinta\\nJacintha\\nJacinthe\\nJackelyn\\nJacki\\nJackie\\nJacklin\\nJacklyn\\nJackquelin\\nJackqueline\\nJacky\\nJaclin\\nJaclyn\\nJacquelin\\nJacqueline\\nJacquelyn\\nJacquelynn\\nJacquenetta\\nJacquenette\\nJacquetta\\nJacquette\\nJacqui\\nJacquie\\nJacynth\\nJada\\nJade\\nJaime\\nJaimie\\nJaine\\nJami\\nJamie\\nJamima\\nJammie\\nJan\\nJana\\nJanaya\\nJanaye\\nJandy\\nJane\\nJanean\\nJaneczka\\nJaneen\\nJanel\\nJanela\\nJanella\\nJanelle\\nJanene\\nJanenna\\nJanessa\\nJanet\\nJaneta\\nJanetta\\nJanette\\nJaneva\\nJaney\\nJania\\nJanice\\nJanie\\nJanifer\\nJanina\\nJanine\\nJanis\\nJanith\\nJanka\\nJanna\\nJannel\\nJannelle\\nJanot\\nJany\\nJaquelin\\nJaquelyn\\nJaquenetta\\nJaquenette\\nJaquith\\nJasmin\\nJasmina\\nJasmine\\nJayme\\nJaymee\\nJayne\\nJaynell\\nJazmin\\nJean\\nJeana\\nJeane\\nJeanelle\\nJeanette\\nJeanie\\nJeanine\\nJeanna\\nJeanne\\nJeannette\\nJeannie\\nJeannine\\nJehanna\\nJelene\\nJemie\\nJemima\\nJemimah\\nJemmie\\nJemmy\\nJen\\nJena\\nJenda\\nJenelle\\nJeni\\nJenica\\nJeniece\\nJenifer\\nJeniffer\\nJenilee\\nJenine\\nJenn\\nJenna\\nJennee\\nJennette\\nJenni\\nJennica\\nJennie\\nJennifer\\nJennilee\\nJennine\\nJenny\\nJeralee\\nJere\\nJeri\\nJermaine\\nJerrie\\nJerrilee\\nJerrilyn\\nJerrine\\nJerry\\nJerrylee\\nJess\\nJessa\\nJessalin\\nJessalyn\\nJessamine\\nJessamyn\\nJesse\\nJesselyn\\nJessi\\nJessica\\nJessie\\nJessika\\nJessy\\nJewel\\nJewell\\nJewelle\\nJill\\nJillana\\nJillane\\nJillayne\\nJilleen\\nJillene\\nJilli\\nJillian\\nJillie\\nJilly\\nJinny\\nJo\\nJo Ann\\nJo-Ann\\nJo-Anne\\nJoan\\nJoana\\nJoane\\nJoanie\\nJoann\\nJoanna\\nJoanne\\nJoannes\\nJobey\\nJobi\\nJobie\\nJobina\\nJoby\\nJobye\\nJobyna\\nJocelin\\nJoceline\\nJocelyn\\nJocelyne\\nJodee\\nJodi\\nJodie\\nJody\\nJoeann\\nJoela\\nJoelie\\nJoell\\nJoella\\nJoelle\\nJoellen\\nJoelly\\nJoellyn\\nJoelynn\\nJoete\\nJoey\\nJohanna\\nJohannah\\nJohna\\nJohnath\\nJohnette\\nJohnna\\nJoice\\nJojo\\nJolee\\nJoleen\\nJolene\\nJoletta\\nJoli\\nJolie\\nJoline\\nJoly\\nJolyn\\nJolynn\\nJonell\\nJoni\\nJonie\\nJonis\\nJordain\\nJordan\\nJordana\\nJordanna\\nJorey\\nJori\\nJorie\\nJorrie\\nJorry\\nJoscelin\\nJosee\\nJosefa\\nJosefina\\nJosepha\\nJosephina\\nJosephine\\nJosey\\nJosi\\nJosie\\nJosselyn\\nJosy\\nJourdan\\nJoy\\nJoya\\nJoyan\\nJoyann\\nJoyce\\nJoycelin\\nJoye\\nJsandye\\nJuana\\nJuanita\\nJudi\\nJudie\\nJudith\\nJuditha\\nJudy\\nJudye\\nJuieta\\nJulee\\nJuli\\nJulia\\nJuliana\\nJuliane\\nJuliann\\nJulianna\\nJulianne\\nJulie\\nJulienne\\nJuliet\\nJulieta\\nJulietta\\nJuliette\\nJulina\\nJuline\\nJulissa\\nJulita\\nJune\\nJunette\\nJunia\\nJunie\\nJunina\\nJustina\\nJustine\\nJustinn\\nJyoti\\nKacey\\nKacie\\nKacy\\nKaela\\nKai\\nKaia\\nKaila\\nKaile\\nKailey\\nKaitlin\\nKaitlyn\\nKaitlynn\\nKaja\\nKakalina\\nKala\\nKaleena\\nKali\\nKalie\\nKalila\\nKalina\\nKalinda\\nKalindi\\nKalli\\nKally\\nKameko\\nKamila\\nKamilah\\nKamillah\\nKandace\\nKandy\\nKania\\nKanya\\nKara\\nKara-Lynn\\nKaralee\\nKaralynn\\nKare\\nKaree\\nKarel\\nKaren\\nKarena\\nKari\\nKaria\\nKarie\\nKaril\\nKarilynn\\nKarin\\nKarina\\nKarine\\nKariotta\\nKarisa\\nKarissa\\nKarita\\nKarla\\nKarlee\\nKarleen\\nKarlen\\nKarlene\\nKarlie\\nKarlotta\\nKarlotte\\nKarly\\nKarlyn\\nKarmen\\nKarna\\nKarol\\nKarola\\nKarole\\nKarolina\\nKaroline\\nKaroly\\nKaron\\nKarrah\\nKarrie\\nKarry\\nKary\\nKaryl\\nKarylin\\nKaryn\\nKasey\\nKass\\nKassandra\\nKassey\\nKassi\\nKassia\\nKassie\\nKat\\nKata\\nKatalin\\nKate\\nKatee\\nKaterina\\nKaterine\\nKatey\\nKath\\nKatha\\nKatharina\\nKatharine\\nKatharyn\\nKathe\\nKatherina\\nKatherine\\nKatheryn\\nKathi\\nKathie\\nKathleen\\nKathlin\\nKathrine\\nKathryn\\nKathryne\\nKathy\\nKathye\\nKati\\nKatie\\nKatina\\nKatine\\nKatinka\\nKatleen\\nKatlin\\nKatrina\\nKatrine\\nKatrinka\\nKatti\\nKattie\\nKatuscha\\nKatusha\\nKaty\\nKatya\\nKay\\nKaycee\\nKaye\\nKayla\\nKayle\\nKaylee\\nKayley\\nKaylil\\nKaylyn\\nKeeley\\nKeelia\\nKeely\\nKelcey\\nKelci\\nKelcie\\nKelcy\\nKelila\\nKellen\\nKelley\\nKelli\\nKellia\\nKellie\\nKellina\\nKellsie\\nKelly\\nKellyann\\nKelsey\\nKelsi\\nKelsy\\nKendra\\nKendre\\nKenna\\nKeri\\nKeriann\\nKerianne\\nKerri\\nKerrie\\nKerrill\\nKerrin\\nKerry\\nKerstin\\nKesley\\nKeslie\\nKessia\\nKessiah\\nKetti\\nKettie\\nKetty\\nKevina\\nKevyn\\nKi\\nKiah\\nKial\\nKiele\\nKiersten\\nKikelia\\nKiley\\nKim\\nKimberlee\\nKimberley\\nKimberli\\nKimberly\\nKimberlyn\\nKimbra\\nKimmi\\nKimmie\\nKimmy\\nKinna\\nKip\\nKipp\\nKippie\\nKippy\\nKira\\nKirbee\\nKirbie\\nKirby\\nKiri\\nKirsten\\nKirsteni\\nKirsti\\nKirstin\\nKirstyn\\nKissee\\nKissiah\\nKissie\\nKit\\nKitti\\nKittie\\nKitty\\nKizzee\\nKizzie\\nKlara\\nKlarika\\nKlarrisa\\nKonstance\\nKonstanze\\nKoo\\nKora\\nKoral\\nKoralle\\nKordula\\nKore\\nKorella\\nKoren\\nKoressa\\nKori\\nKorie\\nKorney\\nKorrie\\nKorry\\nKris\\nKrissie\\nKrissy\\nKrista\\nKristal\\nKristan\\nKriste\\nKristel\\nKristen\\nKristi\\nKristien\\nKristin\\nKristina\\nKristine\\nKristy\\nKristyn\\nKrysta\\nKrystal\\nKrystalle\\nKrystle\\nKrystyna\\nKyla\\nKyle\\nKylen\\nKylie\\nKylila\\nKylynn\\nKym\\nKynthia\\nKyrstin\\nLa Verne\\nLacee\\nLacey\\nLacie\\nLacy\\nLadonna\\nLaetitia\\nLaina\\nLainey\\nLana\\nLanae\\nLane\\nLanette\\nLaney\\nLani\\nLanie\\nLanita\\nLanna\\nLanni\\nLanny\\nLara\\nLaraine\\nLari\\nLarina\\nLarine\\nLarisa\\nLarissa\\nLark\\nLaryssa\\nLatashia\\nLatia\\nLatisha\\nLatrena\\nLatrina\\nLaura\\nLauraine\\nLaural\\nLauralee\\nLaure\\nLauree\\nLaureen\\nLaurel\\nLaurella\\nLauren\\nLaurena\\nLaurene\\nLauretta\\nLaurette\\nLauri\\nLaurianne\\nLaurice\\nLaurie\\nLauryn\\nLavena\\nLaverna\\nLaverne\\nLavina\\nLavinia\\nLavinie\\nLayla\\nLayne\\nLayney\\nLea\\nLeah\\nLeandra\\nLeann\\nLeanna\\nLeanor\\nLeanora\\nLebbie\\nLeda\\nLee\\nLeeann\\nLeeanne\\nLeela\\nLeelah\\nLeena\\nLeesa\\nLeese\\nLegra\\nLeia\\nLeigh\\nLeigha\\nLeila\\nLeilah\\nLeisha\\nLela\\nLelah\\nLeland\\nLelia\\nLena\\nLenee\\nLenette\\nLenka\\nLenna\\nLenora\\nLenore\\nLeodora\\nLeoine\\nLeola\\nLeoline\\nLeona\\nLeonanie\\nLeone\\nLeonelle\\nLeonie\\nLeonora\\nLeonore\\nLeontine\\nLeontyne\\nLeora\\nLeshia\\nLesley\\nLesli\\nLeslie\\nLesly\\nLesya\\nLeta\\nLethia\\nLeticia\\nLetisha\\nLetitia\\nLetizia\\nLetta\\nLetti\\nLettie\\nLetty\\nLexi\\nLexie\\nLexine\\nLexis\\nLexy\\nLeyla\\nLezlie\\nLia\\nLian\\nLiana\\nLiane\\nLianna\\nLianne\\nLib\\nLibbey\\nLibbi\\nLibbie\\nLibby\\nLicha\\nLida\\nLidia\\nLiesa\\nLil\\nLila\\nLilah\\nLilas\\nLilia\\nLilian\\nLiliane\\nLilias\\nLilith\\nLilla\\nLilli\\nLillian\\nLillis\\nLilllie\\nLilly\\nLily\\nLilyan\\nLin\\nLina\\nLind\\nLinda\\nLindi\\nLindie\\nLindsay\\nLindsey\\nLindsy\\nLindy\\nLinea\\nLinell\\nLinet\\nLinette\\nLinn\\nLinnea\\nLinnell\\nLinnet\\nLinnie\\nLinzy\\nLira\\nLisa\\nLisabeth\\nLisbeth\\nLise\\nLisetta\\nLisette\\nLisha\\nLishe\\nLissa\\nLissi\\nLissie\\nLissy\\nLita\\nLiuka\\nLiv\\nLiva\\nLivia\\nLivvie\\nLivvy\\nLivvyy\\nLivy\\nLiz\\nLiza\\nLizabeth\\nLizbeth\\nLizette\\nLizzie\\nLizzy\\nLoella\\nLois\\nLoise\\nLola\\nLoleta\\nLolita\\nLolly\\nLona\\nLonee\\nLoni\\nLonna\\nLonni\\nLonnie\\nLora\\nLorain\\nLoraine\\nLoralee\\nLoralie\\nLoralyn\\nLoree\\nLoreen\\nLorelei\\nLorelle\\nLoren\\nLorena\\nLorene\\nLorenza\\nLoretta\\nLorette\\nLori\\nLoria\\nLorianna\\nLorianne\\nLorie\\nLorilee\\nLorilyn\\nLorinda\\nLorine\\nLorita\\nLorna\\nLorne\\nLorraine\\nLorrayne\\nLorri\\nLorrie\\nLorrin\\nLorry\\nLory\\nLotta\\nLotte\\nLotti\\nLottie\\nLotty\\nLou\\nLouella\\nLouisa\\nLouise\\nLouisette\\nLoutitia\\nLu\\nLuce\\nLuci\\nLucia\\nLuciana\\nLucie\\nLucienne\\nLucila\\nLucilia\\nLucille\\nLucina\\nLucinda\\nLucine\\nLucita\\nLucky\\nLucretia\\nLucy\\nLudovika\\nLuella\\nLuelle\\nLuisa\\nLuise\\nLula\\nLulita\\nLulu\\nLura\\nLurette\\nLurleen\\nLurlene\\nLurline\\nLusa\\nLuz\\nLyda\\nLydia\\nLydie\\nLyn\\nLynda\\nLynde\\nLyndel\\nLyndell\\nLyndsay\\nLyndsey\\nLyndsie\\nLyndy\\nLynea\\nLynelle\\nLynett\\nLynette\\nLynn\\nLynna\\nLynne\\nLynnea\\nLynnell\\nLynnelle\\nLynnet\\nLynnett\\nLynnette\\nLynsey\\nLyssa\\nMab\\nMabel\\nMabelle\\nMable\\nMada\\nMadalena\\nMadalyn\\nMaddalena\\nMaddi\\nMaddie\\nMaddy\\nMadel\\nMadelaine\\nMadeleine\\nMadelena\\nMadelene\\nMadelin\\nMadelina\\nMadeline\\nMadella\\nMadelle\\nMadelon\\nMadelyn\\nMadge\\nMadlen\\nMadlin\\nMadonna\\nMady\\nMae\\nMaegan\\nMag\\nMagda\\nMagdaia\\nMagdalen\\nMagdalena\\nMagdalene\\nMaggee\\nMaggi\\nMaggie\\nMaggy\\nMahala\\nMahalia\\nMaia\\nMaible\\nMaiga\\nMaighdiln\\nMair\\nMaire\\nMaisey\\nMaisie\\nMaitilde\\nMala\\nMalanie\\nMalena\\nMalia\\nMalina\\nMalinda\\nMalinde\\nMalissa\\nMalissia\\nMallissa\\nMallorie\\nMallory\\nMalorie\\nMalory\\nMalva\\nMalvina\\nMalynda\\nMame\\nMamie\\nManda\\nMandi\\nMandie\\nMandy\\nManon\\nManya\\nMara\\nMarabel\\nMarcela\\nMarcelia\\nMarcella\\nMarcelle\\nMarcellina\\nMarcelline\\nMarchelle\\nMarci\\nMarcia\\nMarcie\\nMarcile\\nMarcille\\nMarcy\\nMareah\\nMaren\\nMarena\\nMaressa\\nMarga\\nMargalit\\nMargalo\\nMargaret\\nMargareta\\nMargarete\\nMargaretha\\nMargarethe\\nMargaretta\\nMargarette\\nMargarita\\nMargaux\\nMarge\\nMargeaux\\nMargery\\nMarget\\nMargette\\nMargi\\nMargie\\nMargit\\nMargo\\nMargot\\nMargret\\nMarguerite\\nMargy\\nMari\\nMaria\\nMariam\\nMarian\\nMariana\\nMariann\\nMarianna\\nMarianne\\nMaribel\\nMaribelle\\nMaribeth\\nMarice\\nMaridel\\nMarie\\nMarie-Ann\\nMarie-Jeanne\\nMarieann\\nMariejeanne\\nMariel\\nMariele\\nMarielle\\nMariellen\\nMarietta\\nMariette\\nMarigold\\nMarijo\\nMarika\\nMarilee\\nMarilin\\nMarillin\\nMarilyn\\nMarin\\nMarina\\nMarinna\\nMarion\\nMariquilla\\nMaris\\nMarisa\\nMariska\\nMarissa\\nMarita\\nMaritsa\\nMariya\\nMarj\\nMarja\\nMarje\\nMarji\\nMarjie\\nMarjorie\\nMarjory\\nMarjy\\nMarketa\\nMarla\\nMarlane\\nMarleah\\nMarlee\\nMarleen\\nMarlena\\nMarlene\\nMarley\\nMarlie\\nMarline\\nMarlo\\nMarlyn\\nMarna\\nMarne\\nMarney\\nMarni\\nMarnia\\nMarnie\\nMarquita\\nMarrilee\\nMarris\\nMarrissa\\nMarsha\\nMarsiella\\nMarta\\nMartelle\\nMartguerita\\nMartha\\nMarthe\\nMarthena\\nMarti\\nMartica\\nMartie\\nMartina\\nMartita\\nMarty\\nMartynne\\nMary\\nMarya\\nMaryann\\nMaryanna\\nMaryanne\\nMarybelle\\nMarybeth\\nMaryellen\\nMaryjane\\nMaryjo\\nMaryl\\nMarylee\\nMarylin\\nMarylinda\\nMarylou\\nMarylynne\\nMaryrose\\nMarys\\nMarysa\\nMasha\\nMatelda\\nMathilda\\nMathilde\\nMatilda\\nMatilde\\nMatti\\nMattie\\nMatty\\nMaud\\nMaude\\nMaudie\\nMaura\\nMaure\\nMaureen\\nMaureene\\nMaurene\\nMaurine\\nMaurise\\nMaurita\\nMaurizia\\nMavis\\nMavra\\nMax\\nMaxi\\nMaxie\\nMaxine\\nMaxy\\nMay\\nMaybelle\\nMaye\\nMead\\nMeade\\nMeagan\\nMeaghan\\nMeara\\nMechelle\\nMeg\\nMegan\\nMegen\\nMeggi\\nMeggie\\nMeggy\\nMeghan\\nMeghann\\nMehetabel\\nMei\\nMel\\nMela\\nMelamie\\nMelania\\nMelanie\\nMelantha\\nMelany\\nMelba\\nMelesa\\nMelessa\\nMelicent\\nMelina\\nMelinda\\nMelinde\\nMelisa\\nMelisande\\nMelisandra\\nMelisenda\\nMelisent\\nMelissa\\nMelisse\\nMelita\\nMelitta\\nMella\\nMelli\\nMellicent\\nMellie\\nMellisa\\nMellisent\\nMelloney\\nMelly\\nMelodee\\nMelodie\\nMelody\\nMelonie\\nMelony\\nMelosa\\nMelva\\nMercedes\\nMerci\\nMercie\\nMercy\\nMeredith\\nMeredithe\\nMeridel\\nMeridith\\nMeriel\\nMerilee\\nMerilyn\\nMeris\\nMerissa\\nMerl\\nMerla\\nMerle\\nMerlina\\nMerline\\nMerna\\nMerola\\nMerralee\\nMerridie\\nMerrie\\nMerrielle\\nMerrile\\nMerrilee\\nMerrili\\nMerrill\\nMerrily\\nMerry\\nMersey\\nMeryl\\nMeta\\nMia\\nMicaela\\nMichaela\\nMichaelina\\nMichaeline\\nMichaella\\nMichal\\nMichel\\nMichele\\nMichelina\\nMicheline\\nMichell\\nMichelle\\nMicki\\nMickie\\nMicky\\nMidge\\nMignon\\nMignonne\\nMiguela\\nMiguelita\\nMikaela\\nMil\\nMildred\\nMildrid\\nMilena\\nMilicent\\nMilissent\\nMilka\\nMilli\\nMillicent\\nMillie\\nMillisent\\nMilly\\nMilzie\\nMimi\\nMin\\nMina\\nMinda\\nMindy\\nMinerva\\nMinetta\\nMinette\\nMinna\\nMinnaminnie\\nMinne\\nMinni\\nMinnie\\nMinnnie\\nMinny\\nMinta\\nMiof Mela\\nMiquela\\nMira\\nMirabel\\nMirabella\\nMirabelle\\nMiran\\nMiranda\\nMireielle\\nMireille\\nMirella\\nMirelle\\nMiriam\\nMirilla\\nMirna\\nMisha\\nMissie\\nMissy\\nMisti\\nMisty\\nMitzi\\nModesta\\nModestia\\nModestine\\nModesty\\nMoina\\nMoira\\nMoll\\nMollee\\nMolli\\nMollie\\nMolly\\nMommy\\nMona\\nMonah\\nMonica\\nMonika\\nMonique\\nMora\\nMoreen\\nMorena\\nMorgan\\nMorgana\\nMorganica\\nMorganne\\nMorgen\\nMoria\\nMorissa\\nMorna\\nMoselle\\nMoyna\\nMoyra\\nMozelle\\nMuffin\\nMufi\\nMufinella\\nMuire\\nMureil\\nMurial\\nMuriel\\nMurielle\\nMyra\\nMyrah\\nMyranda\\nMyriam\\nMyrilla\\nMyrle\\nMyrlene\\nMyrna\\nMyrta\\nMyrtia\\nMyrtice\\nMyrtie\\nMyrtle\\nNada\\nNadean\\nNadeen\\nNadia\\nNadine\\nNadiya\\nNady\\nNadya\\nNalani\\nNan\\nNana\\nNananne\\nNance\\nNancee\\nNancey\\nNanci\\nNancie\\nNancy\\nNanete\\nNanette\\nNani\\nNanice\\nNanine\\nNannette\\nNanni\\nNannie\\nNanny\\nNanon\\nNaoma\\nNaomi\\nNara\\nNari\\nNariko\\nNat\\nNata\\nNatala\\nNatalee\\nNatalie\\nNatalina\\nNataline\\nNatalya\\nNatasha\\nNatassia\\nNathalia\\nNathalie\\nNatividad\\nNatka\\nNatty\\nNeala\\nNeda\\nNedda\\nNedi\\nNeely\\nNeila\\nNeile\\nNeilla\\nNeille\\nNelia\\nNelie\\nNell\\nNelle\\nNelli\\nNellie\\nNelly\\nNerissa\\nNerita\\nNert\\nNerta\\nNerte\\nNerti\\nNertie\\nNerty\\nNessa\\nNessi\\nNessie\\nNessy\\nNesta\\nNetta\\nNetti\\nNettie\\nNettle\\nNetty\\nNevsa\\nNeysa\\nNichol\\nNichole\\nNicholle\\nNicki\\nNickie\\nNicky\\nNicol\\nNicola\\nNicole\\nNicolea\\nNicolette\\nNicoli\\nNicolina\\nNicoline\\nNicolle\\nNikaniki\\nNike\\nNiki\\nNikki\\nNikkie\\nNikoletta\\nNikolia\\nNina\\nNinetta\\nNinette\\nNinnetta\\nNinnette\\nNinon\\nNissa\\nNisse\\nNissie\\nNissy\\nNita\\nNixie\\nNoami\\nNoel\\nNoelani\\nNoell\\nNoella\\nNoelle\\nNoellyn\\nNoelyn\\nNoemi\\nNola\\nNolana\\nNolie\\nNollie\\nNomi\\nNona\\nNonah\\nNoni\\nNonie\\nNonna\\nNonnah\\nNora\\nNorah\\nNorean\\nNoreen\\nNorene\\nNorina\\nNorine\\nNorma\\nNorri\\nNorrie\\nNorry\\nNovelia\\nNydia\\nNyssa\\nOctavia\\nOdele\\nOdelia\\nOdelinda\\nOdella\\nOdelle\\nOdessa\\nOdetta\\nOdette\\nOdilia\\nOdille\\nOfelia\\nOfella\\nOfilia\\nOla\\nOlenka\\nOlga\\nOlia\\nOlimpia\\nOlive\\nOlivette\\nOlivia\\nOlivie\\nOliy\\nOllie\\nOlly\\nOlva\\nOlwen\\nOlympe\\nOlympia\\nOlympie\\nOndrea\\nOneida\\nOnida\\nOona\\nOpal\\nOpalina\\nOpaline\\nOphelia\\nOphelie\\nOra\\nOralee\\nOralia\\nOralie\\nOralla\\nOralle\\nOrel\\nOrelee\\nOrelia\\nOrelie\\nOrella\\nOrelle\\nOriana\\nOrly\\nOrsa\\nOrsola\\nOrtensia\\nOtha\\nOthelia\\nOthella\\nOthilia\\nOthilie\\nOttilie\\nPage\\nPaige\\nPaloma\\nPam\\nPamela\\nPamelina\\nPamella\\nPammi\\nPammie\\nPammy\\nPandora\\nPansie\\nPansy\\nPaola\\nPaolina\\nPapagena\\nPat\\nPatience\\nPatrica\\nPatrice\\nPatricia\\nPatrizia\\nPatsy\\nPatti\\nPattie\\nPatty\\nPaula\\nPaule\\nPauletta\\nPaulette\\nPauli\\nPaulie\\nPaulina\\nPauline\\nPaulita\\nPauly\\nPavia\\nPavla\\nPearl\\nPearla\\nPearle\\nPearline\\nPeg\\nPegeen\\nPeggi\\nPeggie\\nPeggy\\nPen\\nPenelopa\\nPenelope\\nPenni\\nPennie\\nPenny\\nPepi\\nPepita\\nPeri\\nPeria\\nPerl\\nPerla\\nPerle\\nPerri\\nPerrine\\nPerry\\nPersis\\nPet\\nPeta\\nPetra\\nPetrina\\nPetronella\\nPetronia\\nPetronilla\\nPetronille\\nPetunia\\nPhaedra\\nPhaidra\\nPhebe\\nPhedra\\nPhelia\\nPhil\\nPhilipa\\nPhilippa\\nPhilippe\\nPhilippine\\nPhilis\\nPhillida\\nPhillie\\nPhillis\\nPhilly\\nPhilomena\\nPhoebe\\nPhylis\\nPhyllida\\nPhyllis\\nPhyllys\\nPhylys\\nPia\\nPier\\nPierette\\nPierrette\\nPietra\\nPiper\\nPippa\\nPippy\\nPolly\\nPollyanna\\nPooh\\nPoppy\\nPortia\\nPris\\nPrisca\\nPriscella\\nPriscilla\\nPrissie\\nPru\\nPrudence\\nPrudi\\nPrudy\\nPrue\\nQueenie\\nQuentin\\nQuerida\\nQuinn\\nQuinta\\nQuintana\\nQuintilla\\nQuintina\\nRachael\\nRachel\\nRachele\\nRachelle\\nRae\\nRaeann\\nRaf\\nRafa\\nRafaela\\nRafaelia\\nRafaelita\\nRahal\\nRahel\\nRaina\\nRaine\\nRakel\\nRalina\\nRamona\\nRamonda\\nRana\\nRanda\\nRandee\\nRandene\\nRandi\\nRandie\\nRandy\\nRanee\\nRani\\nRania\\nRanice\\nRanique\\nRanna\\nRaphaela\\nRaquel\\nRaquela\\nRasia\\nRasla\\nRaven\\nRay\\nRaychel\\nRaye\\nRayna\\nRaynell\\nRayshell\\nRea\\nReba\\nRebbecca\\nRebe\\nRebeca\\nRebecca\\nRebecka\\nRebeka\\nRebekah\\nRebekkah\\nRee\\nReeba\\nReena\\nReeta\\nReeva\\nRegan\\nReggi\\nReggie\\nRegina\\nRegine\\nReiko\\nReina\\nReine\\nRemy\\nRena\\nRenae\\nRenata\\nRenate\\nRene\\nRenee\\nRenell\\nRenelle\\nRenie\\nRennie\\nReta\\nRetha\\nRevkah\\nRey\\nReyna\\nRhea\\nRheba\\nRheta\\nRhetta\\nRhiamon\\nRhianna\\nRhianon\\nRhoda\\nRhodia\\nRhodie\\nRhody\\nRhona\\nRhonda\\nRiane\\nRiannon\\nRianon\\nRica\\nRicca\\nRici\\nRicki\\nRickie\\nRicky\\nRiki\\nRikki\\nRina\\nRisa\\nRita\\nRiva\\nRivalee\\nRivi\\nRivkah\\nRivy\\nRoana\\nRoanna\\nRoanne\\nRobbi\\nRobbie\\nRobbin\\nRobby\\nRobbyn\\nRobena\\nRobenia\\nRoberta\\nRobin\\nRobina\\nRobinet\\nRobinett\\nRobinetta\\nRobinette\\nRobinia\\nRoby\\nRobyn\\nRoch\\nRochell\\nRochella\\nRochelle\\nRochette\\nRoda\\nRodi\\nRodie\\nRodina\\nRois\\nRomola\\nRomona\\nRomonda\\nRomy\\nRona\\nRonalda\\nRonda\\nRonica\\nRonna\\nRonni\\nRonnica\\nRonnie\\nRonny\\nRoobbie\\nRora\\nRori\\nRorie\\nRory\\nRos\\nRosa\\nRosabel\\nRosabella\\nRosabelle\\nRosaleen\\nRosalia\\nRosalie\\nRosalind\\nRosalinda\\nRosalinde\\nRosaline\\nRosalyn\\nRosalynd\\nRosamond\\nRosamund\\nRosana\\nRosanna\\nRosanne\\nRose\\nRoseann\\nRoseanna\\nRoseanne\\nRoselia\\nRoselin\\nRoseline\\nRosella\\nRoselle\\nRosemaria\\nRosemarie\\nRosemary\\nRosemonde\\nRosene\\nRosetta\\nRosette\\nRoshelle\\nRosie\\nRosina\\nRosita\\nRoslyn\\nRosmunda\\nRosy\\nRow\\nRowe\\nRowena\\nRoxana\\nRoxane\\nRoxanna\\nRoxanne\\nRoxi\\nRoxie\\nRoxine\\nRoxy\\nRoz\\nRozalie\\nRozalin\\nRozamond\\nRozanna\\nRozanne\\nRoze\\nRozele\\nRozella\\nRozelle\\nRozina\\nRubetta\\nRubi\\nRubia\\nRubie\\nRubina\\nRuby\\nRuperta\\nRuth\\nRuthann\\nRuthanne\\nRuthe\\nRuthi\\nRuthie\\nRuthy\\nRyann\\nRycca\\nSaba\\nSabina\\nSabine\\nSabra\\nSabrina\\nSacha\\nSada\\nSadella\\nSadie\\nSadye\\nSaidee\\nSal\\nSalaidh\\nSallee\\nSalli\\nSallie\\nSally\\nSallyann\\nSallyanne\\nSaloma\\nSalome\\nSalomi\\nSam\\nSamantha\\nSamara\\nSamaria\\nSammy\\nSande\\nSandi\\nSandie\\nSandra\\nSandy\\nSandye\\nSapphira\\nSapphire\\nSara\\nSara-Ann\\nSaraann\\nSarah\\nSarajane\\nSaree\\nSarena\\nSarene\\nSarette\\nSari\\nSarina\\nSarine\\nSarita\\nSascha\\nSasha\\nSashenka\\nSaudra\\nSaundra\\nSavina\\nSayre\\nScarlet\\nScarlett\\nSean\\nSeana\\nSeka\\nSela\\nSelena\\nSelene\\nSelestina\\nSelia\\nSelie\\nSelina\\nSelinda\\nSeline\\nSella\\nSelle\\nSelma\\nSena\\nSephira\\nSerena\\nSerene\\nShae\\nShaina\\nShaine\\nShalna\\nShalne\\nShana\\nShanda\\nShandee\\nShandeigh\\nShandie\\nShandra\\nShandy\\nShane\\nShani\\nShanie\\nShanna\\nShannah\\nShannen\\nShannon\\nShanon\\nShanta\\nShantee\\nShara\\nSharai\\nShari\\nSharia\\nSharity\\nSharl\\nSharla\\nSharleen\\nSharlene\\nSharline\\nSharon\\nSharona\\nSharron\\nSharyl\\nShaun\\nShauna\\nShawn\\nShawna\\nShawnee\\nShay\\nShayla\\nShaylah\\nShaylyn\\nShaylynn\\nShayna\\nShayne\\nShea\\nSheba\\nSheela\\nSheelagh\\nSheelah\\nSheena\\nSheeree\\nSheila\\nSheila-Kathryn\\nSheilah\\nShel\\nShela\\nShelagh\\nShelba\\nShelbi\\nShelby\\nShelia\\nShell\\nShelley\\nShelli\\nShellie\\nShelly\\nShena\\nSher\\nSheree\\nSheri\\nSherie\\nSherill\\nSherilyn\\nSherline\\nSherri\\nSherrie\\nSherry\\nSherye\\nSheryl\\nShina\\nShir\\nShirl\\nShirlee\\nShirleen\\nShirlene\\nShirley\\nShirline\\nShoshana\\nShoshanna\\nSiana\\nSianna\\nSib\\nSibbie\\nSibby\\nSibeal\\nSibel\\nSibella\\nSibelle\\nSibilla\\nSibley\\nSibyl\\nSibylla\\nSibylle\\nSidoney\\nSidonia\\nSidonnie\\nSigrid\\nSile\\nSileas\\nSilva\\nSilvana\\nSilvia\\nSilvie\\nSimona\\nSimone\\nSimonette\\nSimonne\\nSindee\\nSiobhan\\nSioux\\nSiouxie\\nSisely\\nSisile\\nSissie\\nSissy\\nSiusan\\nSofia\\nSofie\\nSondra\\nSonia\\nSonja\\nSonni\\nSonnie\\nSonnnie\\nSonny\\nSonya\\nSophey\\nSophi\\nSophia\\nSophie\\nSophronia\\nSorcha\\nSosanna\\nStace\\nStacee\\nStacey\\nStaci\\nStacia\\nStacie\\nStacy\\nStafani\\nStar\\nStarla\\nStarlene\\nStarlin\\nStarr\\nStefa\\nStefania\\nStefanie\\nSteffane\\nSteffi\\nSteffie\\nStella\\nStepha\\nStephana\\nStephani\\nStephanie\\nStephannie\\nStephenie\\nStephi\\nStephie\\nStephine\\nStesha\\nStevana\\nStevena\\nStoddard\\nStorm\\nStormi\\nStormie\\nStormy\\nSue\\nSuellen\\nSukey\\nSuki\\nSula\\nSunny\\nSunshine\\nSusan\\nSusana\\nSusanetta\\nSusann\\nSusanna\\nSusannah\\nSusanne\\nSusette\\nSusi\\nSusie\\nSusy\\nSuzann\\nSuzanna\\nSuzanne\\nSuzette\\nSuzi\\nSuzie\\nSuzy\\nSybil\\nSybila\\nSybilla\\nSybille\\nSybyl\\nSydel\\nSydelle\\nSydney\\nSylvia\\nTabatha\\nTabbatha\\nTabbi\\nTabbie\\nTabbitha\\nTabby\\nTabina\\nTabitha\\nTaffy\\nTalia\\nTallia\\nTallie\\nTallou\\nTallulah\\nTally\\nTalya\\nTalyah\\nTamar\\nTamara\\nTamarah\\nTamarra\\nTamera\\nTami\\nTamiko\\nTamma\\nTammara\\nTammi\\nTammie\\nTammy\\nTamqrah\\nTamra\\nTana\\nTandi\\nTandie\\nTandy\\nTanhya\\nTani\\nTania\\nTanitansy\\nTansy\\nTanya\\nTara\\nTarah\\nTarra\\nTarrah\\nTaryn\\nTasha\\nTasia\\nTate\\nTatiana\\nTatiania\\nTatum\\nTawnya\\nTawsha\\nTed\\nTedda\\nTeddi\\nTeddie\\nTeddy\\nTedi\\nTedra\\nTeena\\nTEirtza\\nTeodora\\nTera\\nTeresa\\nTerese\\nTeresina\\nTeresita\\nTeressa\\nTeri\\nTeriann\\nTerra\\nTerri\\nTerrie\\nTerrijo\\nTerry\\nTerrye\\nTersina\\nTerza\\nTess\\nTessa\\nTessi\\nTessie\\nTessy\\nThalia\\nThea\\nTheadora\\nTheda\\nThekla\\nThelma\\nTheo\\nTheodora\\nTheodosia\\nTheresa\\nTherese\\nTheresina\\nTheresita\\nTheressa\\nTherine\\nThia\\nThomasa\\nThomasin\\nThomasina\\nThomasine\\nTiena\\nTierney\\nTiertza\\nTiff\\nTiffani\\nTiffanie\\nTiffany\\nTiffi\\nTiffie\\nTiffy\\nTilda\\nTildi\\nTildie\\nTildy\\nTillie\\nTilly\\nTim\\nTimi\\nTimmi\\nTimmie\\nTimmy\\nTimothea\\nTina\\nTine\\nTiphani\\nTiphanie\\nTiphany\\nTish\\nTisha\\nTobe\\nTobey\\nTobi\\nToby\\nTobye\\nToinette\\nToma\\nTomasina\\nTomasine\\nTomi\\nTommi\\nTommie\\nTommy\\nToni\\nTonia\\nTonie\\nTony\\nTonya\\nTonye\\nTootsie\\nTorey\\nTori\\nTorie\\nTorrie\\nTory\\nTova\\nTove\\nTracee\\nTracey\\nTraci\\nTracie\\nTracy\\nTrenna\\nTresa\\nTrescha\\nTressa\\nTricia\\nTrina\\nTrish\\nTrisha\\nTrista\\nTrix\\nTrixi\\nTrixie\\nTrixy\\nTruda\\nTrude\\nTrudey\\nTrudi\\nTrudie\\nTrudy\\nTrula\\nTuesday\\nTwila\\nTwyla\\nTybi\\nTybie\\nTyne\\nUla\\nUlla\\nUlrica\\nUlrika\\nUlrikaumeko\\nUlrike\\nUmeko\\nUna\\nUrsa\\nUrsala\\nUrsola\\nUrsula\\nUrsulina\\nUrsuline\\nUta\\nVal\\nValaree\\nValaria\\nVale\\nValeda\\nValencia\\nValene\\nValenka\\nValentia\\nValentina\\nValentine\\nValera\\nValeria\\nValerie\\nValery\\nValerye\\nValida\\nValina\\nValli\\nVallie\\nVally\\nValma\\nValry\\nVan\\nVanda\\nVanessa\\nVania\\nVanna\\nVanni\\nVannie\\nVanny\\nVanya\\nVeda\\nVelma\\nVelvet\\nVenita\\nVenus\\nVera\\nVeradis\\nVere\\nVerena\\nVerene\\nVeriee\\nVerile\\nVerina\\nVerine\\nVerla\\nVerna\\nVernice\\nVeronica\\nVeronika\\nVeronike\\nVeronique\\nVevay\\nVi\\nVicki\\nVickie\\nVicky\\nVictoria\\nVida\\nViki\\nVikki\\nVikky\\nVilhelmina\\nVilma\\nVin\\nVina\\nVinita\\nVinni\\nVinnie\\nVinny\\nViola\\nViolante\\nViole\\nViolet\\nVioletta\\nViolette\\nVirgie\\nVirgina\\nVirginia\\nVirginie\\nVita\\nVitia\\nVitoria\\nVittoria\\nViv\\nViva\\nVivi\\nVivia\\nVivian\\nViviana\\nVivianna\\nVivianne\\nVivie\\nVivien\\nViviene\\nVivienne\\nViviyan\\nVivyan\\nVivyanne\\nVonni\\nVonnie\\nVonny\\nVyky\\nWallie\\nWallis\\nWalliw\\nWally\\nWaly\\nWanda\\nWandie\\nWandis\\nWaneta\\nWanids\\nWenda\\nWendeline\\nWendi\\nWendie\\nWendy\\nWendye\\nWenona\\nWenonah\\nWhitney\\nWileen\\nWilhelmina\\nWilhelmine\\nWilie\\nWilla\\nWillabella\\nWillamina\\nWilletta\\nWillette\\nWilli\\nWillie\\nWillow\\nWilly\\nWillyt\\nWilma\\nWilmette\\nWilona\\nWilone\\nWilow\\nWindy\\nWini\\nWinifred\\nWinna\\nWinnah\\nWinne\\nWinni\\nWinnie\\nWinnifred\\nWinny\\nWinona\\nWinonah\\nWren\\nWrennie\\nWylma\\nWynn\\nWynne\\nWynnie\\nWynny\\nXaviera\\nXena\\nXenia\\nXylia\\nXylina\\nYalonda\\nYasmeen\\nYasmin\\nYelena\\nYetta\\nYettie\\nYetty\\nYevette\\nYnes\\nYnez\\nYoko\\nYolanda\\nYolande\\nYolane\\nYolanthe\\nYoshi\\nYoshiko\\nYovonnda\\nYsabel\\nYvette\\nYvonne\\nZabrina\\nZahara\\nZandra\\nZaneta\\nZara\\nZarah\\nZaria\\nZarla\\nZea\\nZelda\\nZelma\\nZena\\nZenia\\nZia\\nZilvia\\nZita\\nZitella\\nZoe\\nZola\\nZonda\\nZondra\\nZonnya\\nZora\\nZorah\\nZorana\\nZorina\\nZorine\\nZsa Zsa\\nZsazsa\\nZulema\\nZuzana\\n\"\n      },\n      {\n        \"fileName\": \"dataset.py\",\n        \"fileContents\": \"import os\\nfrom datasets import load_dataset\\nimport logging\\nimport random\\nfrom datetime import datetime\\nfrom typing import Dict, Iterable, Union\\nfrom openai.types.chat import ChatCompletionMessageParam\\n\\nfrom targon.types import Endpoints\\n\\nBASE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"data\\\")\\nNAMES = [line.strip() for line in open(os.path.join(BASE_DIR, \\\"names.txt\\\")).readlines()]\\nCOUNTRIES = [\\n    line.strip() for line in open(os.path.join(BASE_DIR, \\\"countries.txt\\\")).readlines()\\n]\\n\\n\\ndef create_search_prompt(\\n    query: str, endpoint: Endpoints\\n) -> Dict[str, Union[str, Iterable[ChatCompletionMessageParam]]]:\\n    # Format the current date for inclusion in the prompt\\n    date = datetime.now().strftime(\\\"%Y-%m-%d\\\")\\n    system_message = f\\\"\\\"\\\"\\n### Current Date: {date}\\n### Instruction:\\nYou are to take on the role of {random.choice(NAMES)}, an expert language model\\ndeveloped in {random.choice(COUNTRIES)}, tasked with generating responses to user queries.\\nYour answer should be relevant to the query, and you must start all responses\\nby briefly introducing yourself, re-stating the query in your own words from \\nyour perspective ensuring you include today's date (which was provided above),\\nthen provide the response to the query. You should always respond in English.\\n\\\"\\\"\\\"\\n    # Compile the chat components into a structured format\\n    match endpoint:\\n        case Endpoints.CHAT:\\n            messages: Iterable[ChatCompletionMessageParam] = [\\n                {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_message},\\n                {\\\"role\\\": \\\"user\\\", \\\"content\\\": query},\\n            ]\\n            return {\\\"messages\\\": messages}\\n        case Endpoints.COMPLETION:\\n            prompt: str = f\\\"\\\"\\\"{system_message}\\\\n\\\\n{query}\\\"\\\"\\\"\\n            return {\\\"prompt\\\": prompt}\\n        case _:\\n            raise Exception(\\\"Unknown Endpoint\\\")\\n\\n\\ndef create_query_prompt(query: str) -> Iterable[ChatCompletionMessageParam]:\\n    # Format the current date for inclusion in the prompt\\n    date = datetime.now().strftime(\\\"%Y-%m-%d\\\")\\n\\n    # Construct the system message with dynamic content\\n    system_message = f\\\"\\\"\\\"\\n### Current Date: {date}\\n### Instruction:\\nYou are to take the query information that is passed from you and create a search query for the query data. \\nDo not answer the information, just create a search query. The search query should not be longer than a sentence.\\nAssistant should always start the response with \\\"Search query: \\\"\\n\\\"\\\"\\\"\\n\\n    # Compile the chat components into a structured format\\n    chats: Iterable[ChatCompletionMessageParam] = [\\n        {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_message},\\n        {\\\"role\\\": \\\"user\\\", \\\"content\\\": query},\\n    ]\\n\\n    # Apply the chat template without tokenization\\n    return chats\\n\\n\\ndef download_dataset():\\n    logger = logging.getLogger(\\\"huggingface_hub.utils._http\\\")\\n    logger.setLevel(logging.CRITICAL + 1)\\n    ds = load_dataset(\\\"manifoldlabs/Infinity-Instruct\\\", \\\"7M\\\")\\n    return ds\\n\"\n      },\n      {\n        \"fileName\": \"docker.py\",\n        \"fileContents\": \"from time import sleep\\nimport random\\nfrom typing import Any, Dict, List, Tuple\\nimport re\\nimport math\\nimport docker\\nimport bittensor as bt\\nimport subprocess\\nfrom accelerate.commands import estimate\\n\\nfrom docker.models.containers import Container\\nfrom docker.types import DeviceRequest\\nimport requests\\n\\nfrom targon.config import IMAGE_TAG\\nfrom targon.types import Endpoints\\n\\n\\ndef get_gpu_with_space(gpus: List[Tuple[int, int, int]], required: int):\\n    \\\"[GPU_ID, free, total] in MB\\\"\\n    bt.logging.info(f\\\"Need: {required}, have: {gpus}\\\")\\n    \\n    # find unsused GPUS\\n    unused = [gpu for gpu in gpus if gpu[1] / gpu[2] > 0.9]\\n\\n    # find first gpu with enough space\\n    for gpu in unused:\\n        if gpu[1] >= required * 1.2:\\n            return [gpu]\\n    \\n    # if we need multiple gpu, only used unused\\n    total_free = 0\\n    next_gpus = []\\n    for gpu in unused:\\n        total_free += gpu[1]\\n        next_gpus.append(gpu)\\n        if total_free > required * 1.2:\\n            return next_gpus\\n    return None\\n\\n\\ndef bytes_to_mib(bytes_value):\\n    mib_value = bytes_value / (1024**2)  # 1024^2 = 1,048,576\\n    return math.ceil(mib_value)\\n\\n\\ndef estimate_max_size(model_name):\\n    \\\"Returns size in MiB, what nvidia smi prints\\\"\\n    try:\\n        model = estimate.create_empty_model(\\n            model_name, library_name=\\\"transformers\\\", trust_remote_code=False\\n        )\\n    except (RuntimeError, OSError) as e:\\n        library = estimate.check_has_model(e)\\n        if library != \\\"unknown\\\":\\n            raise RuntimeError(\\n                f\\\"Tried to load `{model_name}` with `{library}` but a possible model to load was not found inside the repo.\\\"\\n            )\\n        return None\\n\\n    total_size, _ = estimate.calculate_maximum_sizes(model)\\n    return bytes_to_mib(total_size)\\n\\n\\nMANIFOLD_VERIFIER = \\\"manifoldlabs/sn4-verifier\\\"\\n\\n\\ndef load_docker():\\n    client = docker.from_env()\\n    return client\\n\\n\\ndef get_free_gpus() -> List[Tuple[int, int, int]]:\\n    \\\"[GPU_ID, free, total] in MB\\\"\\n    res = subprocess.run(\\n        [\\n            \\\"nvidia-smi\\\",\\n            \\\"--query-gpu=memory.free,memory.total\\\",\\n            \\\"--format=csv,noheader\\\",\\n        ],\\n        stdout=subprocess.PIPE,\\n        stderr=subprocess.STDOUT,\\n    )\\n    if res.returncode != 0:\\n        bt.logging.error(res.stdout.decode(\\\"utf-8\\\"))\\n        raise Exception(\\\"Failed to detect nvida gpus\\\")\\n\\n    lines = [line.split(\\\" \\\") for line in res.stdout.decode(\\\"utf-8\\\").strip().split(\\\"\\\\n\\\")]\\n    gpus = [(i, int(line[0]), int(line[2])) for i, line in enumerate(lines)]\\n    return gpus\\n\\n\\ndef remove_containers(client):\\n    containers: List[Container] = client.containers.list(  # type: ignore\\n        filters={\\\"label\\\": \\\"model\\\"}\\n    )\\n    for container in containers:\\n        model = container.labels.get(\\\"model\\\")\\n        bt.logging.info(f\\\"Removing {container.name}: {model}\\\")\\n        container.remove(force=True)\\n\\ndef sync_output_checkers(\\n    client: docker.DockerClient, models: List[str]\\n) -> Dict[str, Dict[str, Any]]:\\n\\n    # Get new image hash (if any)\\n    image_name = f\\\"{MANIFOLD_VERIFIER}:{IMAGE_TAG}\\\"\\n    try:\\n        client.images.pull(image_name)  # type: ignore\\n    except Exception as e:\\n        bt.logging.error(str(e))\\n    bt.logging.info(f\\\"Syncing {models}\\\")\\n\\n    # Remove all containers\\n    remove_containers(client)\\n    verification_ports = {}\\n    used_ports = []\\n    random.shuffle(models)\\n    min_port = 5555\\n\\n    # Clear containers that arent running\\n    client.containers.prune()\\n\\n    # Load all models\\n    bt.logging.info(f\\\"Starting subset of {list(models)}\\\")\\n    for model in models:\\n        container_name = re.sub(r\\\"[\\\\W_]\\\", \\\"-\\\", model).lower()\\n\\n        # Delete if existing and out of date\\n        existing_containers: List[Container] = client.containers.list(filters={\\\"name\\\": container_name})  # type: ignore\\n        if len(existing_containers):\\n            existing_containers[0].remove(force=True)\\n\\n        # Determine GPU free\\n        free_gpus = get_free_gpus()\\n        required_vram = estimate_max_size(model)\\n        if required_vram is None:\\n            bt.logging.error(f\\\"Failed to find model {model}\\\")\\n            continue\\n        gpus = get_gpu_with_space(free_gpus, required_vram)\\n        if gpus is None:\\n            bt.logging.info(f\\\"Not enough space to run {model}\\\")\\n            continue\\n\\n        # Find Port\\n        while min_port in used_ports:\\n            min_port += 1\\n        used_ports.append(min_port)\\n\\n        # Init new container\\n        bt.logging.info(\\n            f\\\"Loading {model} on gpu(s) {[gpu[0] for gpu in gpus]}\\\"\\n        )\\n        config: Dict[str, Any] = {\\n            \\\"image\\\": image_name,\\n            \\\"ports\\\": {f\\\"80/tcp\\\": min_port},\\n            \\\"environment\\\": [\\n                f\\\"MODEL={model}\\\",\\n                f\\\"TENSOR_PARALLEL={len(gpus)}\\\",\\n            ],\\n            \\\"volumes\\\": [\\\"/var/targon/huggingface/cache:/root/.cache/huggingface\\\"],\\n            \\\"runtime\\\": \\\"nvidia\\\",\\n            \\\"detach\\\": True,\\n            \\\"ipc_mode\\\": \\\"host\\\",\\n            \\\"name\\\": container_name,\\n            \\\"extra_hosts\\\": {\\\"host.docker.internal\\\": \\\"host-gateway\\\"},\\n            \\\"labels\\\": {\\\"model\\\": str(model), \\\"port\\\": str(min_port)},\\n            \\\"device_requests\\\": [\\n                DeviceRequest(\\n                    device_ids=[str(gpu[0]) for gpu in gpus], capabilities=[[\\\"gpu\\\"]]\\n                )\\n            ],\\n        }\\n        client.containers.run(**config)  # type: ignore\\n        while True:\\n            ready = True\\n            std_model = re.sub(r\\\"[\\\\W_]\\\", \\\"-\\\", model).lower()\\n            containers: List[Container] = client.containers.list(filters={\\\"name\\\": std_model}, all=True)  # type: ignore\\n            if not len(containers):\\n                bt.logging.info(\\n                    f\\\"Failed starting container {std_model}: Removing from verifiers\\\"\\n                )\\n                break\\n            (container,) = containers\\n            if container.health == \\\"unhealthy\\\":\\n                container_logs = container.logs()\\n                bt.logging.error(\\n                    f\\\"Failed starting container {std_model}: Removing from verifiers\\\"\\n                )\\n                bt.logging.error(\\\"---- Verifier Logs ----\\\")\\n                bt.logging.error(container_logs)\\n                bt.logging.error(\\\"-----------------------\\\")\\n                break\\n            if container.health != \\\"healthy\\\":\\n                bt.logging.info(f\\\"{container.name}: {container.health}\\\")\\n                ready = False\\n            if ready:\\n                verification_ports[model] = {\\\"port\\\": min_port}\\n                endpoints = requests.get(\\n                    f\\\"http://localhost:{min_port}/endpoints\\\"\\n                ).json()\\n                endpoints = [Endpoints(e.upper()) for e in endpoints]\\n                verification_ports[model][\\\"endpoints\\\"] = endpoints\\n                break\\n            bt.logging.info(\\\"Checking again in 5 seconds\\\")\\n            sleep(5)\\n\\n    bt.logging.info(\\\"Successfully started verifiers\\\")\\n    bt.logging.info(str(verification_ports))\\n    if len(list(verification_ports.keys())) == 0:\\n        bt.logging.error(\\\"No verification ports\\\")\\n        exit()\\n    return verification_ports\\n\"\n      },\n      {\n        \"fileName\": \"epistula.py\",\n        \"fileContents\": \"import json\\nfrom hashlib import sha256\\nfrom uuid import uuid4\\nfrom math import ceil\\nfrom typing import Annotated, Any, Dict, Optional\\n\\nimport time\\nimport httpx\\nfrom substrateinterface import Keypair\\n\\n\\ndef generate_header(\\n    hotkey: Keypair,\\n    body: Any,\\n    signed_for: Optional[str] = None,\\n) -> Dict[str, Any]:\\n    timestamp = round(time.time() * 1000)\\n    timestampInterval = ceil(timestamp / 1e4) * 1e4\\n    uuid = str(uuid4())\\n    req_hash = None\\n    if isinstance(body, bytes):\\n        req_hash = sha256(body).hexdigest()\\n    else:\\n        req_hash = sha256(json.dumps(body).encode(\\\"utf-8\\\")).hexdigest()\\n\\n    headers = {\\n        \\\"Epistula-Version\\\": str(2),\\n        \\\"Epistula-Timestamp\\\": str(timestamp),\\n        \\\"Epistula-Uuid\\\": uuid,\\n        \\\"Epistula-Signed-By\\\": hotkey.ss58_address,\\n        \\\"Epistula-Request-Signature\\\": \\\"0x\\\"\\n        + hotkey.sign(f\\\"{req_hash}.{uuid}.{timestamp}.{signed_for or ''}\\\").hex(),\\n    }\\n    if signed_for:\\n        headers[\\\"Epistula-Signed-For\\\"] = signed_for\\n        headers[\\\"Epistula-Secret-Signature-0\\\"] = (\\n            \\\"0x\\\" + hotkey.sign(str(timestampInterval - 1) + \\\".\\\" + signed_for).hex()\\n        )\\n        headers[\\\"Epistula-Secret-Signature-1\\\"] = (\\n            \\\"0x\\\" + hotkey.sign(str(timestampInterval) + \\\".\\\" + signed_for).hex()\\n        )\\n        headers[\\\"Epistula-Secret-Signature-2\\\"] = (\\n            \\\"0x\\\" + hotkey.sign(str(timestampInterval + 1) + \\\".\\\" + signed_for).hex()\\n        )\\n    return headers\\n\\n\\ndef verify_signature(\\n    signature, body: bytes, timestamp, uuid, signed_for, signed_by, now\\n) -> Optional[Annotated[str, \\\"Error Message\\\"]]:\\n    if not isinstance(signature, str):\\n        return \\\"Invalid Signature\\\"\\n    timestamp = int(timestamp)\\n    if not isinstance(timestamp, int):\\n        return \\\"Invalid Timestamp\\\"\\n    if not isinstance(signed_by, str):\\n        return \\\"Invalid Sender key\\\"\\n    if not isinstance(signed_for, str):\\n        return \\\"Invalid receiver key\\\"\\n    if not isinstance(uuid, str):\\n        return \\\"Invalid uuid\\\"\\n    if not isinstance(body, bytes):\\n        return \\\"Body is not of type bytes\\\"\\n    ALLOWED_DELTA_MS = 8000\\n    keypair = Keypair(ss58_address=signed_by)\\n    if timestamp + ALLOWED_DELTA_MS < now:\\n        return \\\"Request is too stale\\\"\\n    message = f\\\"{sha256(body).hexdigest()}.{uuid}.{timestamp}.{signed_for}\\\"\\n    verified = keypair.verify(message, signature)\\n    if not verified:\\n        return \\\"Signature Mismatch\\\"\\n    return None\\n\\n\\ndef create_header_hook(hotkey, axon_hotkey, model):\\n    async def add_headers(request: httpx.Request):\\n        for key, header in generate_header(hotkey, request.read(), axon_hotkey).items():\\n            request.headers[key] = header\\n        request.headers[\\\"X-Targon-Model\\\"] = model\\n\\n    return add_headers\\n\"\n      },\n      {\n        \"fileName\": \"jugo.py\",\n        \"fileContents\": \"from typing import Any, Dict, List, Optional, Tuple\\n\\nimport aiohttp\\nimport traceback\\nfrom nanoid import generate\\n\\nfrom targon.epistula import generate_header\\nfrom targon.request import check_tokens\\nfrom targon.types import Endpoints, InferenceStats, OrganicStats\\nimport bittensor as bt\\n\\nJUGO_URL = \\\"https://jugo.targon.com\\\"\\n\\n\\nasync def send_organics_to_jugo(\\n    wallet: \\\"bt.wallet\\\",\\n    organics: List[OrganicStats],\\n):\\n    try:\\n        body = {\\\"organics\\\": [organic.model_dump() for organic in organics]}\\n        headers = generate_header(wallet.hotkey, body)\\n        # Send request to the FastAPI server\\n        async with aiohttp.ClientSession() as session:\\n            async with session.post(\\n                f\\\"{JUGO_URL}/organics/scores\\\",\\n                headers=headers,\\n                json=body,\\n                timeout=aiohttp.ClientTimeout(60),\\n            ) as response:\\n                if response.status == 200:\\n                    bt.logging.info(\\\"Records sent successfully.\\\")\\n                else:\\n                    error_detail = await response.text()\\n                    bt.logging.error(\\n                        f\\\"Error sending records: {response.status} - {error_detail}\\\"\\n                    )\\n\\n    except aiohttp.ClientConnectionError:\\n        bt.logging.error(\\\"Error conecting to jugo, offline.\\\")\\n    except Exception as e:\\n        bt.logging.error(f\\\"Error in send_stats_to_jugo: {e}\\\")\\n        bt.logging.error(traceback.format_exc())\\n\\n\\nasync def send_stats_to_jugo(\\n    metagraph: \\\"bt.metagraph\\\",\\n    subtensor: \\\"bt.subtensor\\\",\\n    wallet: \\\"bt.wallet\\\",\\n    stats: List[Tuple[int, Optional[InferenceStats]]],\\n    req: Dict[str, Any],\\n    endpoint: Endpoints,\\n    version: int,\\n    models: List[str],\\n    miner_tps: Dict[int, Dict[str, List[Optional[float]]]],\\n):\\n    try:\\n        r_nanoid = generate(size=48)\\n        responses = [\\n            {\\n                \\\"r_nanoid\\\": r_nanoid,\\n                \\\"hotkey\\\": metagraph.axons[uid].hotkey,\\n                \\\"coldkey\\\": metagraph.axons[uid].coldkey,\\n                \\\"uid\\\": int(uid),\\n                \\\"stats\\\": stat and stat.model_dump(),\\n            }\\n            for uid, stat in stats\\n        ]\\n        request = {\\n            \\\"r_nanoid\\\": r_nanoid,\\n            \\\"block\\\": subtensor.block,\\n            \\\"request\\\": req,\\n            \\\"request_endpoint\\\": str(endpoint),\\n            \\\"version\\\": version,\\n            \\\"hotkey\\\": wallet.hotkey.ss58_address,\\n        }\\n        # Prepare the data\\n        body = {\\n            \\\"request\\\": request,\\n            \\\"responses\\\": responses,\\n            \\\"models\\\": models,\\n            \\\"scores\\\": miner_tps,\\n        }\\n        headers = generate_header(wallet.hotkey, body)\\n        # Send request to the FastAPI server\\n        async with aiohttp.ClientSession() as session:\\n            async with session.post(\\n                f\\\"{JUGO_URL}/\\\",\\n                headers=headers,\\n                json=body,\\n                timeout=aiohttp.ClientTimeout(60),\\n            ) as response:\\n                if response.status == 200:\\n                    bt.logging.info(\\\"Records sent successfully.\\\")\\n                else:\\n                    error_detail = await response.text()\\n                    bt.logging.error(\\n                        f\\\"Error sending records: {response.status} - {error_detail}\\\"\\n                    )\\n\\n    except aiohttp.ClientConnectionError:\\n        bt.logging.error(\\\"Error conecting to jugo, offline.\\\")\\n    except Exception as e:\\n        bt.logging.error(f\\\"Error in send_stats_to_jugo: {e}\\\")\\n        bt.logging.error(traceback.format_exc())\\n\\n\\nasync def score_organics(last_bucket_id, ports, wallet):\\n    try:\\n        async with aiohttp.ClientSession() as session:\\n            body = list(ports.keys())\\n            headers = generate_header(wallet.hotkey, body)\\n            async with session.post(\\n                JUGO_URL + \\\"/organics\\\",\\n                headers=headers,\\n                json=body,\\n                timeout=aiohttp.ClientTimeout(60),\\n            ) as res:\\n                if res.status != 200:\\n                    bt.logging.info(f\\\"Error pinging jugo {res.text}\\\")\\n                    return last_bucket_id, None, None\\n                res_body = await res.json()\\n        bucket_id = res_body.get(\\\"bucket_id\\\")\\n        organics = res_body.get(\\\"organics\\\")\\n        if last_bucket_id == bucket_id:\\n            bt.logging.info(f\\\"Already seen this bucket id\\\")\\n            return last_bucket_id, None, None\\n        scores = {}\\n        organic_stats = []\\n        bt.logging.info(f\\\"Found {len(organics)} organics\\\")\\n        for model, records in organics.items():\\n            for record in records:\\n                uid = record[\\\"uid\\\"]\\n                if scores.get(uid) is None:\\n                    scores[uid] = []\\n                if not record[\\\"success\\\"]:\\n                    scores[uid].append(-500)\\n                    continue\\n                tokens = []\\n                for token in record[\\\"response\\\"]:\\n                    choice = token.get(\\\"choices\\\", [{}])[0]\\n                    text = \\\"\\\"\\n                    logprob = -100\\n                    match record[\\\"endpoint\\\"]:\\n                        case \\\"CHAT\\\":\\n                            text = choice.get(\\\"delta\\\", {}).get(\\\"content\\\")\\n                            logprobs = choice.get(\\\"logprobs\\\")\\n                            if logprobs is None:\\n                                continue\\n                            logprob = logprobs.get(\\\"content\\\", [{}])[0].get(\\n                                \\\"logprob\\\", -100\\n                            )\\n                            token = logprobs.get(\\\"content\\\", [{}])[0].get(\\\"token\\\", None)\\n                            if text is None or (text == \\\"\\\" and len(tokens) == 0):\\n                                continue\\n                        case \\\"COMPLETION\\\":\\n                            text = choice.get(\\\"text\\\")\\n                            logprobs = choice.get(\\\"logprobs\\\")\\n                            if logprobs is None:\\n                                continue\\n                            logprob = logprobs.get(\\\"token_logprobs\\\", [-100])[0]\\n                            token = logprobs.get(\\\"tokens\\\", [\\\"\\\"])[0]\\n                            if text is None or (text == \\\"\\\" and len(tokens) == 0):\\n                                continue\\n\\n                    token_id = -1\\n                    if not token.startswith(\\\"token_id:\\\"):\\n                        continue\\n                    token_parts = token.split(\\\":\\\")\\n                    if len(token_parts) > 1:\\n                        token_id = int(token_parts[1])\\n\\n                    tokens.append(\\n                        {\\n                            \\\"text\\\": text,\\n                            \\\"logprob\\\": logprob,\\n                            \\\"token_id\\\": token_id,\\n                        }\\n                    )\\n\\n                # No response tokens\\n                if len(tokens) == 0:\\n                    scores[uid].append(-100)\\n                    continue\\n\\n                port = ports.get(model, {}).get(\\\"port\\\")\\n                if not port:\\n                    continue\\n                res = await check_tokens(\\n                    record[\\\"request\\\"],\\n                    tokens,\\n                    record[\\\"uid\\\"],\\n                    Endpoints(record[\\\"endpoint\\\"]),\\n                    port,\\n                )\\n                bt.logging.info(str(res))\\n                if res is None:\\n                    continue\\n                verified = res.get(\\\"verified\\\", False)\\n                tps = 0\\n                if verified:\\n                    try:\\n                        response_tokens_count = int(record.get(\\\"response_tokens\\\", 0))\\n\\n                        # This shouldnt happen\\n                        if response_tokens_count == 0:\\n                            continue\\n\\n                        tps = min(\\n                            response_tokens_count, record[\\\"request\\\"][\\\"max_tokens\\\"]\\n                        ) / (int(record.get(\\\"total_time\\\")) / 1000)\\n                        scores[uid].append(tps)\\n                    except Exception as e:\\n                        bt.logging.error(\\\"Error scoring record: \\\" + str(e))\\n                        continue\\n                organic_stats.append(\\n                    OrganicStats(\\n                        time_to_first_token=int(record.get(\\\"time_to_first_token\\\")),\\n                        time_for_all_tokens=int(record.get(\\\"total_time\\\"))\\n                        - int(record.get(\\\"time_to_first_token\\\")),\\n                        total_time=int(record.get(\\\"total_time\\\")),\\n                        tps=tps,\\n                        tokens=[],\\n                        verified=verified,\\n                        error=res.get(\\\"error\\\"),\\n                        cause=res.get(\\\"cause\\\"),\\n                        model=model,\\n                        max_tokens=record.get(\\\"request\\\").get(\\\"max_tokens\\\"),\\n                        seed=record.get(\\\"request\\\").get(\\\"seed\\\"),\\n                        temperature=record.get(\\\"request\\\").get(\\\"temperature\\\"),\\n                        uid=uid,\\n                        hotkey=record.get(\\\"hotkey\\\"),\\n                        coldkey=record.get(\\\"coldkey\\\"),\\n                        endpoint=record.get(\\\"endpoint\\\"),\\n                        total_tokens=record.get(\\\"response_tokens\\\"),\\n                    )\\n                )\\n        bt.logging.info(f\\\"{bucket_id}: {scores}\\\")\\n        return bucket_id, scores, organic_stats\\n    except Exception as e:\\n        bt.logging.error(str(e))\\n        return None\\n\"\n      },\n      {\n        \"fileName\": \"math.py\",\n        \"fileContents\": \"from math import exp\\nimport bittensor as bt\\nimport numpy as np\\nfrom typing import Dict, List, Optional, Tuple\\n\\nfrom targon.config import SLIDING_WINDOW\\nfrom targon.utils import fail_with_none\\n\\n\\ndef normalize(arr: List[float], t_min=0, t_max=1) -> List[float]:\\n    norm_arr = []\\n    diff = t_max - t_min\\n    diff_arr = max(arr) - min(arr)\\n    for i in arr:\\n        temp = (((i - min(arr)) * diff) / diff_arr) + t_min\\n        norm_arr.append(temp)\\n    return norm_arr\\n\\n\\ndef sigmoid(num):\\n    return 1 / (1 + exp(-((num - 0.5) / 0.1)))\\n\\n\\ndef safe_mean_score(data):\\n    clean_data = [x for x in data if x is not None]\\n    if len(clean_data) == 0:\\n        return 0.0\\n    mean_value = np.mean(clean_data)\\n    if np.isnan(mean_value) or np.isinf(mean_value):\\n        return 0.0\\n    return float(mean_value) * sigmoid(len(clean_data) / len(data))\\n\\n\\n@fail_with_none(\\\"Failed getting Weights\\\")\\ndef get_weights(\\n    miner_models: Dict[int, List[str]],\\n    miner_tps: Dict[int, Dict[str, List[Optional[float]]]],\\n    organics: Dict[int, list[int]],\\n    models: List[str],\\n) -> Tuple[List[int], List[float]]:\\n    # Mean and sigmoid of tps scores from each model. Since all miners are queried with\\n    # All models, more models served = higher score. *then* it becomes a speed game.\\n    tps = {}\\n    for uid in miner_tps:\\n        tps[uid] = 0\\n        if (organic := organics.get(uid)) is not None:\\n            tps[uid] = safe_mean_score(organic)\\n        for model in miner_models.get(uid, []):\\n            if model not in models:\\n                continue\\n            if miner_tps.get(uid) is None:\\n                continue\\n            if miner_tps[uid].get(model) is None:\\n                continue\\n\\n            tps[uid] += safe_mean_score(miner_tps[uid][model][-SLIDING_WINDOW:])\\n\\n    tps_list = list(tps.values())\\n    if len(tps_list) == 0:\\n        bt.logging.warning(\\\"Not setting weights, no responses from miners\\\")\\n        return [], []\\n    uids: List[int] = sorted(tps.keys())\\n    rewards = [tps[uid] for uid in uids]\\n\\n    bt.logging.info(f\\\"All wps: {tps}\\\")\\n    if sum(rewards) < 1 / 1e9:\\n        bt.logging.warning(\\\"No one gave responses worth scoring\\\")\\n        return [], []\\n    raw_weights = normalize(rewards)\\n    bt.logging.info(f\\\"Raw Weights: {raw_weights}\\\")\\n    return uids, raw_weights\\n\"\n      },\n      {\n        \"fileName\": \"metagraph.py\",\n        \"fileContents\": \"import time\\nfrom typing import Callable, Dict, List, Tuple\\nimport numpy as np\\nimport bittensor as bt\\nfrom bittensor.utils.weight_utils import process_weights_for_netuid\\n\\nfrom targon.utils import fail_with_none\\n\\nimport threading\\n\\n\\ndef get_miner_uids(\\n    metagraph: \\\"bt.metagraph\\\", self_uid: int, vpermit_tao_limit: int\\n) -> List[int]:\\n    available_uids = []\\n    for uid in range(int(metagraph.n.item())):\\n        if uid == self_uid:\\n            continue\\n\\n        # Filter non serving axons.\\n        if not metagraph.axons[uid].is_serving:\\n            continue\\n        # Filter validator permit > 1024 stake.\\n        if metagraph.validator_permit[uid]:\\n            if metagraph.S[uid] > vpermit_tao_limit:\\n                continue\\n        available_uids.append(uid)\\n        continue\\n    return available_uids\\n\\n\\n@fail_with_none(\\\"Failed resyncing hotkeys\\\")\\ndef resync_hotkeys(metagraph: \\\"bt.metagraph\\\", miner_tps: Dict):\\n    bt.logging.info(\\\"re-syncing hotkeys\\\")\\n    # Zero out all hotkeys that have been replaced.\\n    for uid, hotkey in enumerate(metagraph.hotkeys):\\n        if miner_tps.get(uid) is None:\\n            miner_tps[uid] = {}\\n        if hotkey != metagraph.hotkeys[uid]:\\n            miner_tps[uid] = {}\\n\\n\\ndef create_set_weights(version: int, netuid):\\n    @fail_with_none(\\\"Failed setting weights\\\")\\n    def set_weights(\\n        wallet: \\\"bt.wallet\\\",\\n        metagraph: \\\"bt.metagraph\\\",\\n        subtensor: \\\"bt.subtensor\\\",\\n        weights: Tuple[List[int], List[float]],\\n    ):\\n        if weights is None:\\n            return None\\n        uids, raw_weights = weights\\n        if not len(uids):\\n            bt.logging.info(\\\"No UIDS to score\\\")\\n            return\\n\\n        # Set the weights on chain via our subtensor connection.\\n        (\\n            processed_weight_uids,\\n            processed_weights,\\n        ) = process_weights_for_netuid(\\n            uids=np.asarray(uids),\\n            weights=np.asarray(raw_weights),\\n            netuid=netuid,\\n            subtensor=subtensor,\\n            metagraph=metagraph,\\n        )\\n\\n        bt.logging.info(\\\"Setting Weights: \\\" + str(processed_weights))\\n        bt.logging.info(\\\"Weight Uids: \\\" + str(processed_weight_uids))\\n        for _ in range(3):\\n            result, message = subtensor.set_weights(\\n                wallet=wallet,\\n                netuid=netuid,\\n                uids=processed_weight_uids,  # type: ignore\\n                weights=processed_weights,\\n                wait_for_finalization=False,\\n                wait_for_inclusion=False,\\n                version_key=version,\\n                max_retries=1,\\n            )\\n            if result is True:\\n                bt.logging.info(\\\"set_weights on chain successfully!\\\")\\n                break\\n            else:\\n                bt.logging.error(f\\\"set_weights failed {message}\\\")\\n            time.sleep(15)\\n\\n    return set_weights\\n\\n\\ndef create_subscription_handler(substrate, callback: Callable):\\n    def inner(obj, update_nr, _):\\n        substrate.get_block(block_number=obj[\\\"header\\\"][\\\"number\\\"])\\n\\n        if update_nr >= 1:\\n            return callback(obj[\\\"header\\\"][\\\"number\\\"])  # type: int\\n\\n    return inner\\n\\n\\ndef start_subscription(substrate, callback: Callable):\\n    return substrate.subscribe_block_headers(\\n        create_subscription_handler(substrate, callback)\\n    )\\n\\n\\ndef run_block_callback_thread(substrate, callback: Callable):\\n    subscription_thread = threading.Thread(\\n        target=start_subscription, args=[substrate, callback], daemon=True\\n    )\\n    subscription_thread.start()\\n    bt.logging.info(\\\"Block subscription started in background thread.\\\")\\n    return subscription_thread\\n\"\n      },\n      {\n        \"fileName\": \"request.py\",\n        \"fileContents\": \"import math\\nfrom os import urandom\\nimport time\\nimport traceback\\nfrom typing import Dict, List, Optional, Tuple\\n\\nfrom httpx import Timeout\\nimport openai\\nimport requests\\nfrom targon.dataset import create_query_prompt, create_search_prompt\\nfrom targon.epistula import create_header_hook\\nfrom targon.types import Endpoints, InferenceStats\\nfrom targon.utils import fail_with_none\\nimport random\\nimport bittensor as bt\\n\\n\\n@fail_with_none(\\\"Error generating dataset\\\")\\ndef generate_request(dataset, model_name, endpoint: Endpoints, port: int):\\n    # Generate a random seed for reproducibility in sampling and text generation\\n    random.seed(urandom(100))\\n    seed = random.randint(10000, 10000000)\\n    temperature = random.random()\\n    max_tokens = random.randint(512, 1920)\\n\\n    total_rows = len(dataset[\\\"train\\\"])\\n    random_row_text = dataset[\\\"train\\\"][random.randint(0, total_rows - 1)][\\n        \\\"conversations\\\"\\n    ][0][\\\"value\\\"]\\n    # Generate a query from the sampled text and perform text generation\\n    messages = create_query_prompt(random_row_text)\\n    res: Optional[str] = None\\n    response = None\\n    for _ in range(3):\\n        try:\\n            response = requests.post(\\n                f\\\"http://localhost:{port}/generate\\\",\\n                headers={\\\"Content-Type\\\": \\\"application/json\\\"},\\n                json={\\n                    \\\"messages\\\": messages,\\n                    \\\"sampling_params\\\": {\\n                        \\\"temperature\\\": 0.5,\\n                        \\\"seed\\\": seed,\\n                        \\\"max_tokens\\\": random.randint(16, 64),\\n                    },\\n                },\\n            )\\n            if response.status_code != 200:\\n                bt.logging.error(f\\\"Failed to generate request for {model_name}\\\")\\n                return None\\n            res = response.json().get(\\\"text\\\")\\n        except Exception:\\n            bt.logging.error(f\\\"Failed to generate request for {model_name}\\\")\\n        break\\n    if res is None:\\n        bt.logging.error(\\n            f\\\"Failed to generate prompt for {model_name}: {endpoint}: {response}\\\"\\n        )\\n        return None\\n\\n    # Create sampling parameters using the generated seed and token limit\\n    return {\\n        \\\"seed\\\": seed,\\n        \\\"max_tokens\\\": max_tokens,\\n        \\\"temperature\\\": temperature,\\n        \\\"model\\\": model_name,\\n        \\\"stream\\\": True,\\n        \\\"logprobs\\\": True,\\n        **create_search_prompt(res, endpoint),\\n    }\\n\\n\\nasync def handle_inference(\\n    metagraph: \\\"bt.metagraph\\\",\\n    wallet: \\\"bt.wallet\\\",\\n    request,\\n    uid: int,\\n    endpoint: Endpoints,\\n) -> Tuple[int, InferenceStats]:\\n    stats = InferenceStats(\\n        time_to_first_token=0,\\n        time_for_all_tokens=0,\\n        tps=0,\\n        total_time=0,\\n        tokens=[],\\n        verified=False,\\n    )\\n    try:\\n        axon_info = metagraph.axons[uid]\\n        miner = openai.AsyncOpenAI(\\n            base_url=f\\\"http://{axon_info.ip}:{axon_info.port}/v1\\\",\\n            api_key=\\\"sn4\\\",\\n            max_retries=0,\\n            timeout=Timeout(12, connect=5, read=5),\\n            http_client=openai.DefaultAsyncHttpxClient(\\n                event_hooks={\\n                    \\\"request\\\": [\\n                        create_header_hook(\\n                            wallet.hotkey, axon_info.hotkey, request[\\\"model\\\"]\\n                        )\\n                    ]\\n                }\\n            ),\\n        )\\n        start_token_time = 0\\n        start_send_message_time = time.time()\\n        token_times = []\\n        try:\\n            match endpoint:\\n                case Endpoints.CHAT:\\n                    chat = await miner.chat.completions.create(**request)\\n                    async for chunk in chat:\\n                        if chunk.choices[0].delta is None:\\n                            continue\\n                        if (\\n                            chunk.choices[0].delta.content == \\\"\\\"\\n                            or chunk.choices[0].delta.content is None\\n                        ) and len(stats.tokens) == 0:\\n                            continue\\n                        if start_token_time == 0:\\n                            start_token_time = time.time()\\n                        choice = chunk.choices[0]\\n                        logprob = -100\\n                        token_id = -1\\n                        choiceprobs = choice.logprobs\\n                        if choiceprobs is not None:\\n                            if choiceprobs.content:\\n                                logprob = choiceprobs.content[0].logprob\\n                                token = choiceprobs.content[0].token\\n                                if token is None:\\n                                    continue\\n                                if not token.startswith(\\\"token_id:\\\"):\\n                                    continue\\n                                token_parts = token.split(\\\":\\\")\\n                                if len(token_parts) > 1:\\n                                    token_id = int(token_parts[1])\\n                        stats.tokens.append(\\n                            {\\n                                \\\"text\\\": choice.delta.content or \\\"\\\",\\n                                \\\"token_id\\\": token_id,\\n                                \\\"logprob\\\": logprob,\\n                            }\\n                        )\\n                        token_times.append(time.time())\\n                case Endpoints.COMPLETION:\\n                    comp = await miner.completions.create(**request)\\n                    async for chunk in comp:\\n                        if (\\n                            chunk.choices[0].text == \\\"\\\" or chunk.choices[0].text is None\\n                        ) and len(stats.tokens) == 0:\\n                            continue\\n                        if start_token_time == 0:\\n                            start_token_time = time.time()\\n                        choice = chunk.choices[0]\\n                        if choice.logprobs is None:\\n                            continue\\n                        token_id = -1\\n                        logprob = -100\\n                        if choice.logprobs.token_logprobs:\\n                            logprob = choice.logprobs.token_logprobs[0]\\n                        if (\\n                            choice.logprobs.tokens is not None\\n                            and len(choice.logprobs.tokens) > 0\\n                        ):\\n                            token = choice.logprobs.tokens[0]\\n                            if token is None:\\n                                continue\\n                            if not token.startswith(\\\"token_id:\\\"):\\n                                continue\\n                            token_parts = token.split(\\\":\\\")\\n                            if len(token_parts) > 1:\\n                                token_id = int(token_parts[1])\\n                        stats.tokens.append(\\n                            {\\n                                \\\"text\\\": choice.text or \\\"\\\",\\n                                \\\"token_id\\\": token_id,\\n                                \\\"logprob\\\": logprob,\\n                            }\\n                        )\\n                        token_times.append(time.time())\\n        except openai.APIConnectionError as e:\\n            bt.logging.trace(f\\\"Miner {uid} failed request: {e}\\\")\\n            stats.error = str(e)\\n            stats.cause = \\\"BAD_STREAM\\\"\\n        except Exception as e:\\n            bt.logging.trace(f\\\"Unknown Error when sending to miner {uid}: {e}\\\")\\n            stats.error = str(e)\\n            stats.cause = \\\"BAD_STREAM\\\"\\n\\n        if start_token_time == 0:\\n            start_token_time = time.time()\\n        end_token_time = time.time()\\n        time_to_first_token = start_token_time - start_send_message_time\\n        time_for_all_tokens = end_token_time - start_token_time\\n        if stats.error:\\n            return uid, stats\\n        stats.time_to_first_token = time_to_first_token\\n        stats.time_for_all_tokens = time_for_all_tokens\\n        stats.total_time = end_token_time - start_send_message_time\\n        stats.tps = min(len(stats.tokens), request[\\\"max_tokens\\\"]) / stats.total_time\\n\\n        # Detect when response was fully generated, then streamed, which leads to\\n        # poor user experience (slow time to N tokens vs total time).\\n        token_count = len(stats.tokens)\\n        if token_count > 60:\\n            time_to_5th_percent = (\\n                token_times[math.ceil(token_count * 0.05)] - start_send_message_time\\n            )\\n            if time_to_5th_percent / stats.total_time >= 0.85:\\n                stats.verified = False\\n                stats.error = \\\"Likely non-streamed response\\\"\\n                stats.cause = \\\"BAD_STREAM\\\"\\n        return uid, stats\\n    except Exception as e:\\n        bt.logging.error(f\\\"{uid}: Error in forward for: {e}\\\")\\n        bt.logging.error(traceback.format_exc())\\n        return uid, stats\\n\\n\\n@fail_with_none(\\\"Failed to check tokens\\\")\\nasync def check_tokens(\\n    request,\\n    responses: List[Dict],\\n    uid,\\n    endpoint: Endpoints,\\n    port: int,\\n    url=\\\"http://localhost\\\",\\n) -> Optional[Dict]:\\n    try:\\n        result = requests.post(\\n            f\\\"{url}:{port}/verify\\\",\\n            headers={\\\"Content-Type\\\": \\\"application/json\\\"},\\n            json={\\n                \\\"model\\\": request.get(\\\"model\\\"),\\n                \\\"request_type\\\": endpoint.value,\\n                \\\"request_params\\\": request,\\n                \\\"output_sequence\\\": responses,\\n            },\\n        ).json()\\n        if result.get(\\\"verified\\\") is None:\\n            bt.logging.error(str(result))\\n            return None\\n        return result\\n    except Exception as e:\\n        bt.logging.error(f\\\"{uid}: \\\" + str(e))\\n        return None\\n\"\n      },\n      {\n        \"fileName\": \"types.py\",\n        \"fileContents\": \"# The MIT License (MIT)\\n# Copyright © 2023 Yuma Rao\\n# Copyright © 2024 Manifold Labs\\n\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the “Software”), to deal in the Software without restriction, including without limitation\\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\\n# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\\n# the Software.\\n\\n# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\\n# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\\n# DEALINGS IN THE SOFTWARE.\\n\\n\\nfrom enum import Enum\\nfrom pydantic import BaseModel\\nfrom typing import Any, List, Optional\\n\\n\\nclass Endpoints(Enum):\\n    CHAT = \\\"CHAT\\\"\\n    COMPLETION = \\\"COMPLETION\\\"\\n\\nclass InferenceStats(BaseModel):\\n    time_to_first_token: float\\n    time_for_all_tokens: float\\n    total_time: float\\n    tps: float\\n    tokens: List[Any]\\n    verified: bool\\n    error: Optional[str] = None\\n    cause: Optional[str] = None\\n\\nclass OrganicStats(InferenceStats):\\n    model: str\\n    max_tokens: int\\n    seed: int\\n    temperature: float\\n    uid: int\\n    hotkey: str\\n    coldkey: str\\n    endpoint: str\\n    total_tokens: int\\n\\n\"\n      },\n      {\n        \"fileName\": \"updater.py\",\n        \"fileContents\": \"# The MIT License (MIT)\\n# Copyright © 2023 Yuma Rao\\n# Copyright © 2024 Manifold Labs\\n\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the “Software”), to deal in the Software without restriction, including without limitation\\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\\n# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\\n# the Software.\\n\\n# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\\n# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\\n# DEALINGS IN THE SOFTWARE.\\n\\nimport time\\nimport os\\nimport sys\\nimport requests\\nimport bittensor as bt\\nimport targon\\n\\n\\ndef autoupdate(branch: str = \\\"main\\\", force=False):\\n    \\\"\\\"\\\"\\n    Automatically updates the Targon codebase to the latest version available on the specified branch.\\n\\n    This function checks the remote repository for the latest version of Targon by fetching the VERSION file from the specified branch.\\n    If the local version is older than the remote version, it performs a git pull to update the local codebase to the latest version.\\n    After successfully updating, it restarts the application with the updated code.\\n\\n    Args:\\n    - branch (str): The name of the branch to check for updates. Defaults to \\\"main\\\".\\n\\n    Note:\\n    - The function assumes that the local codebase is a git repository and has the same structure as the remote repository.\\n    - It requires git to be installed and accessible from the command line.\\n    - The function will restart the application using the same command-line arguments it was originally started with.\\n    - If the update fails, manual intervention is required to resolve the issue and restart the application.\\n    \\\"\\\"\\\"\\n    bt.logging.info(\\\"Checking for updates...\\\")\\n    try:\\n        response = requests.get(\\n            f\\\"https://raw.githubusercontent.com/manifold-inc/targon/{branch}/VERSION?token={time.time()}\\\",\\n            headers={\\\"Cache-Control\\\": \\\"no-cache\\\"},\\n        )\\n        response.raise_for_status()\\n        repo_version = response.content.decode()\\n        latest_version = [int(v) for v in repo_version.split(\\\".\\\")]\\n        local_version = [int(v) for v in targon.__version__.split(\\\".\\\")]\\n\\n        bt.logging.info(f\\\"Local version: {targon.__version__}\\\")\\n        bt.logging.info(f\\\"Latest version: {repo_version}\\\")\\n\\n        if latest_version > local_version or force:\\n            bt.logging.info(\\\"A newer version of Targon is available. Updating...\\\")\\n            base_path = os.path.abspath(__file__)\\n            while os.path.basename(base_path) != \\\"targon\\\":\\n                base_path = os.path.dirname(base_path)\\n            base_path = os.path.dirname(base_path)\\n\\n            os.system(f\\\"cd {base_path} && git pull && pip install -r requirements.txt\\\")\\n\\n            with open(os.path.join(base_path, \\\"VERSION\\\")) as f:\\n                new_version = f.read().strip()\\n                new_version = [int(v) for v in new_version.split(\\\".\\\")]\\n\\n                if new_version == latest_version:\\n                    bt.logging.info(\\\"Targon updated successfully. Restarting...\\\")\\n                    sys.exit(0)\\n                else:\\n                    bt.logging.error(\\\"Update failed. Manual update required.\\\")\\n    except Exception as e:\\n        bt.logging.error(f\\\"Update check failed: {e}\\\")\\n\"\n      },\n      {\n        \"fileName\": \"utils.py\",\n        \"fileContents\": \"import traceback\\nimport bittensor as bt\\n\\n\\ndef print_info(metagraph, hotkey, block, isMiner=True):\\n    uid = metagraph.hotkeys.index(hotkey)\\n    log = f\\\"UID:{uid} | Block:{block} | Consensus:{metagraph.C[uid]} | \\\"\\n    if isMiner:\\n        bt.logging.info(\\n            log\\n            + f\\\"Stake:{metagraph.S[uid]} | Trust:{metagraph.T[uid]} | Incentive:{metagraph.I[uid]} | Emission:{metagraph.E[uid]}\\\"\\n        )\\n        return\\n    bt.logging.info(log + f\\\"VTrust:{metagraph.Tv[uid]} | \\\")\\n\\n\\ndef fail_with_none(message: str = \\\"\\\"):\\n    def outer(func):\\n        def inner(*args, **kwargs):\\n            try:\\n                return func(*args, **kwargs)\\n            except Exception as e:\\n                bt.logging.error(message)\\n                bt.logging.error(str(e))\\n                bt.logging.error(traceback.format_exc())\\n                return None\\n\\n        return inner\\n\\n    return outer\\n\\n\\nclass ExitContext:\\n    \\\"\\\"\\\"\\n    Using this as a class lets us pass this to other threads\\n    \\\"\\\"\\\"\\n    isExiting: bool = False\\n\\n    def startExit(self, *_):\\n        if self.isExiting:\\n            exit()\\n        self.isExiting = True\\n\\n    def __bool__(self):\\n        return self.isExiting\\n\"\n      },\n      {\n        \"fileName\": \"verifier\",\n        \"fileContents\": \"\"\n      },\n      {\n        \"fileName\": \"Dockerfile\",\n        \"fileContents\": \"FROM python:3.9\\nWORKDIR /app\\n\\nCOPY ./requirements.txt requirements.txt\\nRUN pip install --no-cache-dir -r requirements.txt\\nCOPY ./verifier.py .\\n\\nHEALTHCHECK --interval=15s --timeout=5s --start-period=30s --start-interval=30s --retries=15 CMD curl --silent --fail http://localhost/ > /dev/null || exit 1\\n\\nENTRYPOINT [\\\"uvicorn\\\", \\\"verifier:app\\\", \\\"--port\\\", \\\"80\\\", \\\"--host\\\", \\\"0.0.0.0\\\"]\\n\"\n      },\n      {\n        \"fileName\": \"requirements.txt\",\n        \"fileContents\": \"vllm==0.6.2\\nfastapi==0.115.0\\nopenai==1.44.1\\nuvicorn==0.30.6\\n\"\n      },\n      {\n        \"fileName\": \"verifier.py\",\n        \"fileContents\": \"import random\\nimport math\\nimport os\\nimport asyncio\\nimport traceback\\nfrom fastapi import FastAPI\\nfrom pydantic import BaseModel\\nfrom enum import Enum\\nfrom typing import Dict, List, Optional, Tuple\\nfrom vllm import LLM, SamplingParams\\n\\n# Load the model.\\nMODEL_NAME = os.getenv(\\\"MODEL\\\", None)\\nif MODEL_NAME is None:\\n    print(\\\"No model name provided, exiting.\\\")\\n    exit()\\n# Constants.\\nLOGPROB_LOG_THRESHOLD = 0.65\\nLOGPROB_FAILURE_THRESHOLD = 0.75\\nTENSOR_PARALLEL = int(os.getenv(\\\"TENSOR_PARALLEL\\\", 1))\\nMODEL_WRAPPER = LLM(\\n    model=MODEL_NAME,\\n    enforce_eager=True,\\n    gpu_memory_utilization=.9,\\n    tensor_parallel_size=TENSOR_PARALLEL,\\n)\\nTOKENIZER = MODEL_WRAPPER.get_tokenizer()\\nMODEL = MODEL_WRAPPER.llm_engine.model_executor.driver_worker.model_runner.model  # type: ignore\\nMODEL_NUM_PARAMS = sum(1 for _ in MODEL.parameters())\\n\\n# Lock to ensure atomicity.\\nLOCK = asyncio.Lock()\\nLOCK_GENERATE = asyncio.Lock()\\n\\nENDPOINTS = [\\\"completion\\\"]\\nif TOKENIZER.chat_template is not None:\\n    ENDPOINTS.append(\\\"chat\\\")\\n\\n\\nclass RequestParams(BaseModel):\\n    messages: Optional[List[Dict[str, str]]] = None\\n    prompt: Optional[str] = None\\n    temperature: float = 0.0\\n    seed: int = 42\\n    max_tokens: int\\n\\n\\nclass OutputItem(BaseModel):\\n    text: str\\n    logprob: float\\n    token_id: int\\n\\n\\nclass RequestType(Enum):\\n    CHAT = \\\"CHAT\\\"\\n    COMPLETION = \\\"COMPLETION\\\"\\n\\n\\nclass VerificationRequest(BaseModel):\\n    request_type: str\\n    model: str = MODEL_NAME\\n    request_params: RequestParams\\n    output_sequence: List[OutputItem]\\n\\n\\nclass RequestSamplingParams(BaseModel):\\n    temperature: float = 0.0\\n    seed: int = 42\\n    max_tokens: int\\n\\n\\nclass GenerateRequest(BaseModel):\\n    messages: List[Dict[str, str]]\\n    sampling_params: RequestSamplingParams\\n\\n\\napp = FastAPI()\\n\\n\\n@app.post(\\\"/generate\\\")\\nasync def generate_question(req: GenerateRequest):\\n    async with LOCK_GENERATE:\\n        try:\\n            if \\\"chat\\\" in ENDPOINTS:\\n                output = (\\n                    MODEL_WRAPPER.chat(\\n                        messages=req.messages, sampling_params=SamplingParams(**req.sampling_params.model_dump()), use_tqdm=False  # type: ignore\\n                    )[0]\\n                    .outputs[0]\\n                    .text\\n                )\\n            else:\\n                prompt = \\\"\\\"\\n                for message in req.messages:\\n                    prompt += (\\n                        message.get(\\\"role\\\", \\\"\\\")\\n                        + \\\": \\\"\\n                        + message.get(\\\"content\\\", \\\"\\\")\\n                        + \\\"\\\\n\\\"\\n                    )\\n                prompt += \\\"\\\\nResponse: \\\"\\n                output = (\\n                    MODEL_WRAPPER.generate(\\n                        prompts=prompt,\\n                        sampling_params=SamplingParams(\\n                            **req.sampling_params.model_dump()\\n                        ),\\n                        use_tqdm=False,\\n                    )[0]\\n                    .outputs[0]\\n                    .text\\n                )\\n            return {\\\"text\\\": output}\\n        except Exception as e:\\n            print(\\\"Failed generate request\\\", str(e), traceback.format_exc())\\n        return {\\\"text\\\": None}\\n\\n\\ndef verify_logprobs_random(\\n    request: VerificationRequest, input_text: str\\n) -> Tuple[bool, str]:\\n    \\\"\\\"\\\"\\n    Generate a handful of random outputs to ensure the logprobs weren't generated after the fact.\\n    \\\"\\\"\\\"\\n    indices = list(range(1, len(request.output_sequence) - 1))\\n    indices_to_check = list(\\n        sorted(\\n            [\\n                0,  # always check first token\\n                len(request.output_sequence) - 1,  # always check last token\\n            ]\\n            + random.sample(indices, min(len(indices), 3))\\n        )\\n    )\\n\\n    # Generate a single token at each index, comparing logprobs.\\n    top_logprobs = int(request.request_params.temperature * 10) + 3\\n    sampling_params = SamplingParams(\\n        temperature=request.request_params.temperature,\\n        seed=request.request_params.seed,\\n        max_tokens=1,\\n        logprobs=top_logprobs,\\n    )\\n    for idx in indices_to_check:\\n        full_text = input_text + \\\"\\\".join(\\n            [item.text for item in request.output_sequence[0:idx]]\\n        )\\n        output = MODEL_WRAPPER.generate([full_text], sampling_params, use_tqdm=False)[\\n            0\\n        ].outputs[0]\\n\\n        # The miner's output token should be in the logprobs...\\n        top_tokens = []\\n        if output.logprobs is None:\\n            print(\\\"No log probs to check\\\")\\n            continue\\n        for lp in output.logprobs:\\n            top_tokens += list(lp.keys())\\n        if request.output_sequence[idx].token_id not in top_tokens:\\n            message = f\\\"Token output at index {idx} [{TOKENIZER.decode([request.output_sequence[idx].token_id])}] not found in top {top_logprobs} logprobs: {[TOKENIZER.decode([token]) for token in top_tokens]}\\\"\\n            return False, message\\n    return (\\n        True,\\n        f\\\"Successfully verified {len(indices_to_check)} random logprobs: {indices_to_check}\\\",\\n    )\\n\\n\\ndef verify_logprobs(\\n    request: VerificationRequest, input_text: str, input_tokens: List[int]\\n) -> Optional[Tuple[bool, str, str]]:\\n    \\\"\\\"\\\"\\n    Compare the produced logprob values against the ground truth, or at least\\n    the ground truth according to this particular GPU/software pairing.\\n    \\\"\\\"\\\"\\n\\n    # Set up sampling parameters for the \\\"fast\\\" check, which just compares input logprobs against output logprobs.\\n    top_logprobs = int(request.request_params.temperature * 10) + 6\\n    sampling_params = SamplingParams(\\n        temperature=request.request_params.temperature,\\n        seed=request.request_params.seed,\\n        max_tokens=1,\\n        logprobs=top_logprobs,\\n        prompt_logprobs=top_logprobs,\\n    )\\n\\n    # Generate output for a single token, which will return input logprobs based on prompt_logprobs=1\\n    output = None\\n    for _ in range(5):\\n        full_text = input_text + \\\"\\\".join(\\n            [item.text for item in request.output_sequence]\\n        )\\n        output = MODEL_WRAPPER.generate([full_text], sampling_params, use_tqdm=False)[0]\\n        if output.prompt_logprobs is not None:\\n            break\\n\\n    if not output or output.prompt_logprobs is None:\\n        return None\\n\\n    # The actual logprobs should be *very* close, but typically not 100% because of GPU/driver/etc. differences.\\n    total_score = 0.0\\n    idxs = min(\\n        len(output.prompt_logprobs) - len(input_tokens) - 3,\\n        len(request.output_sequence) - 1,\\n    )\\n    perfect_tokens = 0\\n    eos_token_id = getattr(TOKENIZER, \\\"eos_token_id\\\", -1)\\n    eot_token_id = TOKENIZER.get_vocab().get(\\\"<|eot_id|>\\\", -1)  # type: ignore\\n    output_tokens = [item.token_id for item in request.output_sequence]\\n    really_low_prob = 0\\n    not_first = 0\\n    for idx in range(idxs):\\n        item = request.output_sequence[idx]\\n        expected_logprob = output.prompt_logprobs[idx + len(input_tokens)]\\n        assert expected_logprob is not None\\n        eos_logprob = expected_logprob.get(eos_token_id)\\n        eot_logprob = expected_logprob.get(eot_token_id)\\n        if (\\n            not eos_logprob\\n            and eot_logprob\\n            or (\\n                eos_logprob\\n                and eot_logprob\\n                and eot_logprob.rank != None\\n                and eos_logprob.rank != None\\n                and eot_logprob.rank < eos_logprob.rank\\n            )\\n        ):\\n            eos_logprob = eot_logprob\\n        expected_logprob = expected_logprob.get(item.token_id)\\n        if eos_logprob and (\\n            not expected_logprob\\n            or (\\n                eos_logprob\\n                and expected_logprob.rank != None\\n                and eos_logprob.rank != None\\n                and eos_logprob.rank < expected_logprob.rank\\n                and expected_logprob.rank > 10\\n            )\\n        ):\\n            return False, f\\\"Expected EOS/EOT token at index {idx}\\\", \\\"SKIPPED_EOS_EOT\\\"\\n        if expected_logprob is None:\\n            continue\\n        rank = expected_logprob.rank\\n        assert rank != None\\n        if rank >= 75:\\n            return (\\n                False,\\n                f\\\"Found extraordinarily improbable token '{TOKENIZER.decode([item.token_id])}' at index {idx}: {rank=}\\\",\\n                \\\"UNLIKELY_TOKEN\\\",\\n            )\\n        elif rank >= 25:\\n            really_low_prob += 1\\n        elif rank > top_logprobs:\\n            continue\\n        if rank != 1:\\n            not_first += 1\\n        expected_logprob = expected_logprob.logprob\\n        produced_logprob = item.logprob\\n        score = 1.0 - min(\\n            1.0, abs(math.exp(expected_logprob) - math.exp(produced_logprob))\\n        )\\n\\n        # Prevents over fitting smaller models\\n        if produced_logprob == 0:\\n            perfect_tokens += 1\\n\\n        # To accomodate architectural difference and such, we'll give a perfect score if >= 0.9\\n        if score >= 0.9:\\n            score = 1.0\\n\\n        # Logprobs rarely match well for high temps so we can use rank instead.\\n        if (\\n            rank == 1\\n            and request.request_params.temperature >= 0.9\\n            and produced_logprob != 0\\n        ):\\n            score = 1.0\\n\\n        total_score += score\\n\\n    # Check if miner produced non-top ranking tokens more than top-ranking tokens.\\n    ratio = not_first / len(output_tokens)\\n    if ratio >= 0.5:\\n        return (\\n            False,\\n            f\\\"{not_first} of {len(output_tokens)} [{ratio=}] tokens were not rank 1.\\\",\\n            \\\"UNLIKELY_TOKENS\\\",\\n        )\\n\\n    # Check if miner prematurely stopped generating, meaning the single output token generated\\n    # from the \\\"throwaway\\\" above was NOT an EOS/EOT token.\\n    if eos_token_id > 0 or eot_token_id > 0:\\n        if len(output_tokens) < request.request_params.max_tokens:\\n            last_token_probs = []\\n            if output:\\n                last_token_probs = output.outputs[0]\\n                last_token_probs = (\\n                    last_token_probs.logprobs[0]\\n                    if last_token_probs and last_token_probs.logprobs\\n                    else []\\n                )\\n            if (\\n                eos_token_id not in last_token_probs\\n                and eot_token_id not in last_token_probs\\n                and len(last_token_probs) != 0\\n            ):\\n                return (\\n                    False,\\n                    \\\"Premature end of generation, EOS/EOT unlikely after last token.\\\",\\n                    \\\"EARLY_END\\\",\\n                )\\n\\n    # Calculate average score.\\n    average_score = round(total_score / idxs, 5)\\n    passes = average_score >= LOGPROB_FAILURE_THRESHOLD\\n    perfect_avg = round(perfect_tokens / idxs, 5)\\n    if passes and perfect_avg >= (\\n        1 - min(request.request_params.temperature * 0.5, 0.6)\\n    ):\\n        return False, f\\\"Overfitted response tokens. {perfect_avg}% perfect\\\", \\\"OVERFIT\\\"\\n    if really_low_prob >= 5:\\n        return (\\n            False,\\n            f\\\"Found {really_low_prob} highly improbable tokens.\\\",\\n            \\\"UNLIKELY_TOKEN\\\",\\n        )\\n\\n    return True, \\\"\\\", \\\"\\\"\\n\\n\\n@app.post(\\\"/verify\\\")\\nasync def verify(request: VerificationRequest) -> Dict:\\n    \\\"\\\"\\\"Verify a miner's output.\\\"\\\"\\\"\\n\\n    # If the miner didn't return any outputs, fail.\\n    if len(request.output_sequence) < 3:\\n        return {\\n            \\\"verified\\\": False,\\n            \\\"error\\\": \\\"Output sequence too short!\\\",\\n            \\\"cause\\\": \\\"TOO_SHORT\\\",\\n        }\\n    if (\\n        request.request_params.max_tokens\\n        and len(request.output_sequence) > request.request_params.max_tokens\\n    ):\\n        return {\\n            \\\"verified\\\": False,\\n            \\\"error\\\": f\\\"Too many tokens produced: {request.request_params.max_tokens} < {len(request.output_sequence)}\\\",\\n            \\\"cause\\\": \\\"TOO_LONG\\\",\\n        }\\n    if request.model != MODEL_NAME:\\n        return {\\n            \\\"error\\\": f\\\"Unable to verify model={request.model}, since we are using {MODEL_NAME}\\\",\\n            \\\"cause\\\": \\\"INTERNAL_ERROR\\\",\\n        }\\n\\n    # Tokenize the input sequence.\\n    input_text = (\\n        request.request_params.prompt\\n        if request.request_type == RequestType.COMPLETION.value\\n        else TOKENIZER.apply_chat_template(\\n            request.request_params.messages,  # type: ignore\\n            tokenize=False,\\n            add_special_tokens=False,\\n            add_generation_prompt=True,\\n        )\\n    )\\n    assert isinstance(input_text, str)\\n    if hasattr(TOKENIZER, \\\"bos_token\\\"):\\n        if input_text.startswith(TOKENIZER.bos_token):  # type: ignore\\n            input_text = input_text[len(TOKENIZER.bos_token) :]  # type: ignore\\n    input_tokens = TOKENIZER(input_text).input_ids\\n\\n    # Verify!\\n    async with LOCK:\\n        return_value = {\\n            \\\"verified\\\": False,\\n            \\\"error\\\": None,\\n        }\\n\\n        # Logprob checks.\\n        res = verify_logprobs(request, str(input_text), input_tokens)\\n        if res is None:\\n            return {\\\"error\\\": \\\"Failed to check log probs\\\", \\\"cause\\\": \\\"INTERNAL_ERROR\\\"}\\n        result, message, cause = res\\n        return_value.update(\\n            {\\n                \\\"verified\\\": result,\\n                \\\"cause\\\": cause,\\n                \\\"error\\\": message,\\n            }\\n        )\\n        if not result:\\n            return return_value\\n\\n        # Random logprob check.\\n        if request.request_params.temperature > 0.75:\\n            return {\\\"verified\\\": True}\\n\\n        res = verify_logprobs_random(request, str(input_text))\\n        if res is None:\\n            return {\\n                \\\"error\\\": \\\"Failed to check log probs\\\",\\n                \\\"cause\\\": \\\"INTERNAL_ERROR\\\",\\n            }\\n        result, message = res\\n        return_value.update(\\n            {\\n                \\\"verified\\\": result,\\n                \\\"cause\\\": \\\"LOGPROB_RANDOM\\\",\\n                \\\"error\\\": message,\\n            }\\n        )\\n        if not result:\\n            return return_value\\n\\n        return {\\\"verified\\\": True}\\n\\n\\n@app.get(\\\"/endpoints\\\")\\ndef endpoints():\\n    return ENDPOINTS\\n\\n\\n@app.get(\\\"/\\\")\\ndef ping():\\n    return \\\"\\\", 200\\n\"\n      }\n    ],\n    \"repo_data\": {\n      \"json\": {\n        \"id\": 697482210,\n        \"node_id\": \"R_kgDOKZK74g\",\n        \"name\": \"targon\",\n        \"full_name\": \"manifold-inc/targon\",\n        \"private\": false,\n        \"owner\": {\n          \"login\": \"manifold-inc\",\n          \"id\": 146253941,\n          \"node_id\": \"O_kgDOCLeodQ\",\n          \"avatar_url\": \"https://avatars.githubusercontent.com/u/146253941?v=4\",\n          \"gravatar_id\": \"\",\n          \"url\": \"https://api.github.com/users/manifold-inc\",\n          \"html_url\": \"https://github.com/manifold-inc\",\n          \"followers_url\": \"https://api.github.com/users/manifold-inc/followers\",\n          \"following_url\": \"https://api.github.com/users/manifold-inc/following{/other_user}\",\n          \"gists_url\": \"https://api.github.com/users/manifold-inc/gists{/gist_id}\",\n          \"starred_url\": \"https://api.github.com/users/manifold-inc/starred{/owner}{/repo}\",\n          \"subscriptions_url\": \"https://api.github.com/users/manifold-inc/subscriptions\",\n          \"organizations_url\": \"https://api.github.com/users/manifold-inc/orgs\",\n          \"repos_url\": \"https://api.github.com/users/manifold-inc/repos\",\n          \"events_url\": \"https://api.github.com/users/manifold-inc/events{/privacy}\",\n          \"received_events_url\": \"https://api.github.com/users/manifold-inc/received_events\",\n          \"type\": \"Organization\",\n          \"user_view_type\": \"public\",\n          \"site_admin\": false\n        },\n        \"html_url\": \"https://github.com/manifold-inc/targon\",\n        \"description\": \"A library for building subnets with the manifold reward stack\",\n        \"fork\": false,\n        \"url\": \"https://api.github.com/repos/manifold-inc/targon\",\n        \"forks_url\": \"https://api.github.com/repos/manifold-inc/targon/forks\",\n        \"keys_url\": \"https://api.github.com/repos/manifold-inc/targon/keys{/key_id}\",\n        \"collaborators_url\": \"https://api.github.com/repos/manifold-inc/targon/collaborators{/collaborator}\",\n        \"teams_url\": \"https://api.github.com/repos/manifold-inc/targon/teams\",\n        \"hooks_url\": \"https://api.github.com/repos/manifold-inc/targon/hooks\",\n        \"issue_events_url\": \"https://api.github.com/repos/manifold-inc/targon/issues/events{/number}\",\n        \"events_url\": \"https://api.github.com/repos/manifold-inc/targon/events\",\n        \"assignees_url\": \"https://api.github.com/repos/manifold-inc/targon/assignees{/user}\",\n        \"branches_url\": \"https://api.github.com/repos/manifold-inc/targon/branches{/branch}\",\n        \"tags_url\": \"https://api.github.com/repos/manifold-inc/targon/tags\",\n        \"blobs_url\": \"https://api.github.com/repos/manifold-inc/targon/git/blobs{/sha}\",\n        \"git_tags_url\": \"https://api.github.com/repos/manifold-inc/targon/git/tags{/sha}\",\n        \"git_refs_url\": \"https://api.github.com/repos/manifold-inc/targon/git/refs{/sha}\",\n        \"trees_url\": \"https://api.github.com/repos/manifold-inc/targon/git/trees{/sha}\",\n        \"statuses_url\": \"https://api.github.com/repos/manifold-inc/targon/statuses/{sha}\",\n        \"languages_url\": \"https://api.github.com/repos/manifold-inc/targon/languages\",\n        \"stargazers_url\": \"https://api.github.com/repos/manifold-inc/targon/stargazers\",\n        \"contributors_url\": \"https://api.github.com/repos/manifold-inc/targon/contributors\",\n        \"subscribers_url\": \"https://api.github.com/repos/manifold-inc/targon/subscribers\",\n        \"subscription_url\": \"https://api.github.com/repos/manifold-inc/targon/subscription\",\n        \"commits_url\": \"https://api.github.com/repos/manifold-inc/targon/commits{/sha}\",\n        \"git_commits_url\": \"https://api.github.com/repos/manifold-inc/targon/git/commits{/sha}\",\n        \"comments_url\": \"https://api.github.com/repos/manifold-inc/targon/comments{/number}\",\n        \"issue_comment_url\": \"https://api.github.com/repos/manifold-inc/targon/issues/comments{/number}\",\n        \"contents_url\": \"https://api.github.com/repos/manifold-inc/targon/contents/{+path}\",\n        \"compare_url\": \"https://api.github.com/repos/manifold-inc/targon/compare/{base}...{head}\",\n        \"merges_url\": \"https://api.github.com/repos/manifold-inc/targon/merges\",\n        \"archive_url\": \"https://api.github.com/repos/manifold-inc/targon/{archive_format}{/ref}\",\n        \"downloads_url\": \"https://api.github.com/repos/manifold-inc/targon/downloads\",\n        \"issues_url\": \"https://api.github.com/repos/manifold-inc/targon/issues{/number}\",\n        \"pulls_url\": \"https://api.github.com/repos/manifold-inc/targon/pulls{/number}\",\n        \"milestones_url\": \"https://api.github.com/repos/manifold-inc/targon/milestones{/number}\",\n        \"notifications_url\": \"https://api.github.com/repos/manifold-inc/targon/notifications{?since,all,participating}\",\n        \"labels_url\": \"https://api.github.com/repos/manifold-inc/targon/labels{/name}\",\n        \"releases_url\": \"https://api.github.com/repos/manifold-inc/targon/releases{/id}\",\n        \"deployments_url\": \"https://api.github.com/repos/manifold-inc/targon/deployments\",\n        \"created_at\": \"2023-09-27T20:23:15Z\",\n        \"updated_at\": \"2025-01-22T17:41:40Z\",\n        \"pushed_at\": \"2025-01-30T03:07:41Z\",\n        \"git_url\": \"git://github.com/manifold-inc/targon.git\",\n        \"ssh_url\": \"git@github.com:manifold-inc/targon.git\",\n        \"clone_url\": \"https://github.com/manifold-inc/targon.git\",\n        \"svn_url\": \"https://github.com/manifold-inc/targon\",\n        \"homepage\": null,\n        \"size\": 11356,\n        \"stargazers_count\": 36,\n        \"watchers_count\": 36,\n        \"language\": \"Python\",\n        \"has_issues\": true,\n        \"has_projects\": true,\n        \"has_downloads\": true,\n        \"has_wiki\": true,\n        \"has_pages\": false,\n        \"has_discussions\": false,\n        \"forks_count\": 35,\n        \"mirror_url\": null,\n        \"archived\": false,\n        \"disabled\": false,\n        \"open_issues_count\": 3,\n        \"license\": {\n          \"key\": \"mit\",\n          \"name\": \"MIT License\",\n          \"spdx_id\": \"MIT\",\n          \"url\": \"https://api.github.com/licenses/mit\",\n          \"node_id\": \"MDc6TGljZW5zZTEz\"\n        },\n        \"allow_forking\": true,\n        \"is_template\": false,\n        \"web_commit_signoff_required\": false,\n        \"topics\": [],\n        \"visibility\": \"public\",\n        \"forks\": 35,\n        \"open_issues\": 3,\n        \"watchers\": 36,\n        \"default_branch\": \"main\",\n        \"permissions\": {\n          \"admin\": false,\n          \"maintain\": false,\n          \"push\": false,\n          \"triage\": false,\n          \"pull\": true\n        },\n        \"temp_clone_token\": \"\",\n        \"custom_properties\": {},\n        \"organization\": {\n          \"login\": \"manifold-inc\",\n          \"id\": 146253941,\n          \"node_id\": \"O_kgDOCLeodQ\",\n          \"avatar_url\": \"https://avatars.githubusercontent.com/u/146253941?v=4\",\n          \"gravatar_id\": \"\",\n          \"url\": \"https://api.github.com/users/manifold-inc\",\n          \"html_url\": \"https://github.com/manifold-inc\",\n          \"followers_url\": \"https://api.github.com/users/manifold-inc/followers\",\n          \"following_url\": \"https://api.github.com/users/manifold-inc/following{/other_user}\",\n          \"gists_url\": \"https://api.github.com/users/manifold-inc/gists{/gist_id}\",\n          \"starred_url\": \"https://api.github.com/users/manifold-inc/starred{/owner}{/repo}\",\n          \"subscriptions_url\": \"https://api.github.com/users/manifold-inc/subscriptions\",\n          \"organizations_url\": \"https://api.github.com/users/manifold-inc/orgs\",\n          \"repos_url\": \"https://api.github.com/users/manifold-inc/repos\",\n          \"events_url\": \"https://api.github.com/users/manifold-inc/events{/privacy}\",\n          \"received_events_url\": \"https://api.github.com/users/manifold-inc/received_events\",\n          \"type\": \"Organization\",\n          \"user_view_type\": \"public\",\n          \"site_admin\": false\n        },\n        \"network_count\": 35,\n        \"subscribers_count\": 9\n      },\n      \"pairedItem\": {\n        \"item\": 0\n      }\n    },\n    \"config\": {\n      \"limit\": 50,\n      \"batch_size\": 10\n    }\n  }"
      },
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1.1,
      "position": [
        540,
        -80
      ],
      "id": "d5421c7f-e7e2-4152-ae31-931d0def3dd7",
      "name": "trigger_input"
    },
    {
      "parameters": {
        "fieldToSplitOut": "files",
        "options": {}
      },
      "type": "n8n-nodes-base.splitOut",
      "typeVersion": 1,
      "position": [
        760,
        -80
      ],
      "id": "2b47f4b3-891b-4db6-a5cd-8aa6c441add2",
      "name": "Split Out"
    }
  ],
  "connections": {
    "Anthropic Chat Model1": {
      "ai_languageModel": [
        [
          {
            "node": "summarize_repo",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Structured Output Parser": {
      "ai_outputParser": [
        [
          {
            "node": "summarize_repo",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    },
    "Loop Over Items": {
      "main": [
        [
          {
            "node": "aggregate_summaries",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "aggregate_files",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Anthropic Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "summarize_batch",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "filter_files": {
      "main": [
        [
          {
            "node": "limit_files",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "limit_files": {
      "main": [
        [
          {
            "node": "optimize_tokens",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "aggregate_summaries": {
      "main": [
        [
          {
            "node": "summarize_repo",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "aggregate_files": {
      "main": [
        [
          {
            "node": "set_params",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "set_params": {
      "main": [
        [
          {
            "node": "summarize_batch",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "summarize_batch": {
      "main": [
        [
          {
            "node": "Loop Over Items",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "summarize_repo": {
      "main": [
        [
          {
            "node": "set_repo_info_variables",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "optimize_tokens": {
      "main": [
        [
          {
            "node": "Loop Over Items",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "trigger_input": {
      "main": [
        [
          {
            "node": "Split Out",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Out": {
      "main": [
        [
          {
            "node": "filter_files",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "pinData": {
    "summarize_batch": [
      {
        "text": "{\n  \"primary_technologies\": [\n    \"Python\",\n    \"Docker\",\n    \"NVIDIA\",\n    \"Bittensor\",\n    \"OpenAI API\"\n  ],\n  \"goals\": \"Build a subnet system for the Manifold reward stack that enables distributed model serving and validation with a focus on large language models and AI model deployment.\",\n  \"features\": [\n    \"model rotation system\",\n    \"validator monitoring\",\n    \"request verification\",\n    \"GPU support\",\n    \"distributed computing\",\n    \"token verification\",\n    \"heartbeat monitoring\",\n    \"docker containerization\",\n    \"API endpoints for chat and completion\",\n    \"huggingface integration\",\n    \"cryptocurrency/blockchain integration\",\n    \"request signature verification\",\n    \"metagraph synchronization\"\n  ]\n}\n\nThis repository appears to be a sophisticated system for managing AI model deployment in a distributed network. It leverages Bittensor's blockchain technology for coordinating nodes and managing rewards, while providing infrastructure for serving large language models like Meta-Llama. The system includes robust security features, monitoring capabilities, and integration with popular AI frameworks and tools.\n\nThe codebase shows a clear focus on building reliable infrastructure for AI model serving with features for validation, monitoring, and secure communication between nodes. The Docker configuration and GPU support indicate this is designed for production-grade AI model deployment."
      }
    ],
    "summarize_repo": [
      {
        "output": {
          "name": "Manifold Subnet",
          "description": "A distributed system for managing and serving large language models on the Bittensor network, with focus on model validation, reward distribution, and secure API endpoints.",
          "primaryTechnologies": [
            "Python",
            "Docker",
            "NVIDIA",
            "Bittensor",
            "OpenAI API",
            "FastAPI",
            "VLLM",
            "Hugging Face"
          ],
          "features": [
            "Model rotation and serving system",
            "Validator monitoring and performance tracking",
            "Request signature verification and security",
            "GPU support and distributed computing",
            "Blockchain integration for rewards and consensus",
            "OpenAI-compatible API endpoints",
            "Caching and response optimization",
            "Metagraph synchronization and weight management",
            "Dataset management and model validation",
            "Comprehensive logging and error handling"
          ],
          "useCases": [
            "Production deployment of AI language models",
            "Distributed model serving infrastructure",
            "Decentralized AI validation networks",
            "Secure API service providers",
            "AI model performance evaluation systems"
          ]
        }
      }
    ],
    "trigger_input": [
      {
        "files": [
          {
            "fileName": "manifold-inc-targon-a854502",
            "fileContents": ""
          },
          {
            "fileName": ".gitignore",
            "fileContents": "*.egg-info\n__pycache__/\ntest.py\ntest_stream.py\n.venv\n.env\n\n!hub-proxy/docker-compose.yml\n.DS_Store\n\noutput.txt\nplay.py\npackages\n*.pickle\n*.json\n*.npy\nmodels.txt\n"
          },
          {
            "fileName": ".python-version",
            "fileContents": "3.10.12\n"
          },
          {
            "fileName": "CHANGELOG.md",
            "fileContents": "# 4.4.0\n\n- Bittensor 8.5.1\n  - Enables CR3\n- Organics speed based scoring\n  - Organics are now scored based on tps.\n  - Organics are now sent back to jugo once scored\n\n# 4.3.0\n\n- Model Rotation\n  - Validators can now host any number of models. These are rotated out each\n    interval randomly. Valis are garunteed to send atleast 1 verifiable request\n    every 3 ticks. Validators are now only limited to the largest model they can\n    run, not the number of models.\n- Dask -> Datasets\n  - Datasets provides huggingface caching for the synthetic dataset. Vali\n    startup time after warmup reduced from ~5 min to \\< 5 seconds. (not\n    including model rotation time)\n- Heartbeat\n  - Validators now self-monitor for system hangs. If no requests are\n    successfully sent within 5 mintues, validators will auto-restart. This is\n    opt-in, and can be enabled with an env file. See readme for details, under\n    the `Validator .env` header.\n- Bittensor 8.4.5\n  - Bumped bittensor version to help with stability when connecting to the chain\n    and getting metagraph / posting weights\n"
          },
          {
            "fileName": "LICENSE",
            "fileContents": "# The MIT License (MIT)\n# Copyright © 2021 Yuma Rao\n# Copyright © 2023 Manifold Labs\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the “Software”), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\n# the Software.\n\n# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\n# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE."
          },
          {
            "fileName": "README.md",
            "fileContents": "# Targon: A Deterministic Verification of Large Language Models\n\nTargon (Bittensor Subnet 4) is a deterministic verification mechanism that is\nused to incentivize miners to run openai compliant endpoints and serve synthetic\nand organic queries.\n\nNOTICE: Using this software, you must agree to the Terms and Agreements provided\nin the terms and conditions document. By downloading and running this software,\nyou implicitly agree to these terms and conditions.\n\n# Table of Contents\n\n1. [Compute Requirements](#recommended-compute-requirements)\n1. [Installation](#installation)\n   - [Install PM2](#install-pm2)\n   - [Install Targon](#install-targon-on-your-machine)\n1. [How to Run Targon](#how-to-run-targon)\n   - [Running VLLM](#vllm)\n   - [Running a Miner](#running-a-miner)\n   - [Running a Validator](#running-a-validator)\n1. [What is a Deterministic Verification Network?](#what-is-a-deterministic-verification-network)\n   - [Role of a Miner](#role-of-a-miner)\n   - [Role of a Validator](#role-of-a-validator)\n1. [Features of Targon](#features-of-targon)\n   - [Full OpenAI Compliance](#full-openai-compliance)\n   - [Targon-Hub](#targon-hub)\n1. [How to Contribute](#how-to-contribute)\n\n# Recommended Compute Requirements\n\nFor validators we recommend a 8xA100, although a 1xA100 could also be used. We\nplan on focusing on bringing these costs down in the coming updates.\n\nFor miners, A100 or H100s are common choices. Benchmarking is up to the miner to\ndetermine what GPU works best for their optimizations.\n\n#### Minimum Viable Compute Recommendations\n\n- **VRAM:** 80 GB\n- **Storage:** 200 GB\n- **RAM:** 16 GB\n- **CPU**: 4 core\n\n# Installation\n\n## Overview\n\nIn order to run Targon, you will need to install PM2 and the Targon package. The\nfollowing instructions apply only to Ubuntu OSes. For your specific OS, please\nrefer to the official documentation.\n\n### Install PM2 on your machine\n\n#### Download NVM\n\nTo install or update nvm, you should run the install script. To do that, you may\neither download and run the script manually, or use the following cURL or Wget\ncommand:\n\n```bash\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash\n```\n\n#### Add NVM to bash profile\n\nRunning either of the above commands downloads a script and runs it. The script\nclones the nvm repository to ~/.nvm, and attempts to add the source lines from\nthe snippet below to the correct profile file (~/.bash_profile, ~/.zshrc,\n~/.profile, or ~/.bashrc).\n\n```bash\nexport NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] && printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\" # This loads nvm\n```\n\n#### Install Node\n\n```bash\nnvm install node\n```\n\n#### Install PM2\n\n```bash\nnpm install pm2@latest -g\n```\n\nYou have now installed PM2.\n\n### Install Targon on your machine\n\n#### Clone the repository\n\n```bash\ngit clone https://github.com/manifold-inc/targon.git\ncd targon\n```\n\n#### Install dependencies\n\n```bash\npython3 -m pip install -e .\n```\n\nYou have now installed Targon. You can now run a validator or a miner.\n\n# How to Run Targon\n\n## Running a Miner\n\nBefore starting or registering your miner in Targon, first you will want to run\nVLLM serving different images validators are requesting. You can find a list at\nhttps://stats.sybil.com/stats/validator under the live tab. The more models you\nrun, the higher your incentive.\n\nVLLM is the recommended engine, however it is not required. If you are using\nVLLM, make sure yo include the `--return-tokens-as-token-ids` flag, or else your\nresponses will fail.\n\nOnce you have one (or multiple) models running, modify the default miner code to\nproxy to the proper VLLM instance on each request. Verifiers will include the\n`X-Targon-Model` header so that the miner node does not need to parse the actual\nbody.\n\nIn the `miner.py` script you will find a function called `list_models`. To serve\nmultiple models you must:\n\n1. Fill this out to respond to validators with any model you currently have\n   available (below is an example):\n\n```\n    async def list_models(self):\n        return [\n            \"ExampleName/Meta-Llama-3.1-8B-Instruct\",\n            \"ExampleName/mythomax-l2-13b\",\n            \"ExampleName/Hermes-3-Llama-3.1-8B\",\n            \"ExampleName/Nxcode-CQ-7B-orpo\",\n            \"ExampleName/deepseek-coder-33b-instruct\",\n            \"ExampleName/Llama-3.1-Nemotron-70B-Instruct-HF\",\n        ]\n```\n\n2. Update the `create_chat_completion` and `create_completion` methods in\n   neurons/miner.py to route to the appropriate vllm upstream server based on\n   the model (which is either in the headers or from the request payload's model\n   param)\n\nHere is a hint / incomplete code snippet to get you started:\n\n```\nmodel_port_map = {\n    'ExampleName/mythomax-l2-13b': 1001,\n    'ExampleName/Hermes-3-Llama-3.1-8B': 1002,\n    'ExampleName/Nxcode-CQ-7B-orpo': 1003,\n    'ExampleName/deepseek-coder-33b-instruct': 1004,\n    'ExampleName/Llama-3.1-Nemotron-70B-Instruct-HF': 1005\n}\nfull_url = f\"http://127.0.0.1:{model_port_map.get(body.get('model'), 1000)}{path}\"\n```\n\nOnce this is complete, you are ready to continue starting your miner node.\n\n### PM2\n\nRunning a miner through PM2 will require the vLLM instance to be running.\n\n```bash\npm2 start neurons/miner.py --name miner --interpreter  python3 -- --wallet.name [WALLET_NAME] --netuid 4 --wallet.hotkey [WALLET_HOTKEY] --subtensor.network finney --model-endpoint [MODEL_ENDPOINT] --api_key [API_KEY] --axon.port [AXON PORT] --logging.trace\n```\n\n> Please replace the following with your specific configuration:\n>\n> - \\[WALLET_NAME\\]\n> - \\[WALLET_HOTKEY\\]\n> - \\[MODEL_ENDPOINT\\]\n> - \\[API_KEY\\]\n> - \\[AXON_PORT\\]\n\nNOTE: Trace logging is very verbose. You can use `--logging.info` instead for\nless log bloat.\n\nAdditionally:\n\n```bash\n--no-force-validator-permit [TRUE/FALSE]\n\n```\n\nis defaulted to false to force incoming requests to have a permit. Set this to\ntrue if you are having trouble getting requests from validators on the 'test'\nnetwork.\n\n## Running a Validator\n\n### PM2\n\nValidators are simply run through pm2, enabling auto restarts and auto updates.\nA validator should be run on atleast an A100, but the larger the better, as\nlarger clusters can handle more models. The machine should have\n[nvidia-smi / cuda](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu)\ninstalled along with [docker](https://docs.docker.com/engine/install/ubuntu/).\n\n**No vllm instance needed**\n\nValidator Instance:\n\n```bash\npm2 start neurons/validator.py --name validator --interperter python3 -- --wallet.name [WALLET_NAME]\n\n```\n\n> Please replace the following with your specific configuration:\n>\n> - \\[WALLET_NAME\\]\n\n## Explanation of Args\n\n### Shared Args\n\n1. **--netuid** ==> Subnet Netuid. *Defaults to 4*\n1. **--epoch-length** ==> Default epoch length (how often we set weights,\n   measured in 12 second blocks). *Defaults to 360*\n1. **--mock** ==> Mock neuron and all network components. *Defaults to False*\n\n### Miner Args\n\n1. **--neuron.name** ==> Trials for this neuron go in neuron.root/ (wallet-cold\n   \\- wallet-hot) / neuron.name. *Defaults to miner*\n1. **--force_validator.permit** ==> If set, forces incoming requests to have a\n   permit. *Defaults to True*\n1. **--model-endpoint** ==> Endpoint to use for the OpenAi CompatibleClient.\n   *Defaults to \"http://127.0.0.1:8000/v1\"*\n1. **--api-key** ==> API key for OpenAi Compatible API. *Defaults to \"12345\"*\n\n### Validator Args\n\n1. **--neuron.name** ==> Trials for this neuron go in neuron.root/ (wallet-cold\n   \\- wallet-hot) / neuron.name. *Defaults to validator*\n1. **--timeout** ==> The timeout for each forward call in seconds. *Defaults to\n   8*\n1. **--vpermit-tao-limit** ==> The maximum number of TAO allowed to query a\n   validator with a permit. *Defaults to 4096*\n1. **--cache-file** ==> Pickle file to save score cache to. *Defaults to\n   cache.pickle*\n1. **--database.url** ==> Database URL to save Miner Data to Targon Hub.\n1. **--autoupdate-off** ==> Disable automatic updates to Targon on latest\n   version on Main if set. *Defaults to True*\n1. **--models.mode** ==> Mode to use for determining what models to run. Can be\n   one of:`default`, or `config`.\n   - `endpoint`: defaults to `https://targon.sybil.com/api/models`. This will\n     mimic the manifold validator\n   - `default`: only run NousResearch/Meta-Llama-3.1-8B-Instruct\n   - `config`: parse a text file named `models.txt` with a list of models\n     separated by newlines\n1. **--models.endpoint** ==> Only used when models.mode is `endpoint`. Sets the\n   api endpoint to ping for list of models. Defaults to targon hub.\n\n> Example model config file `models.txt`\n>\n> ```\n> NousResearch/Meta-Llama-3.1-8B-Instruct\n> NousResearch/Meta-Llama-3.1-70B-Instruct\n> NousResearch/Meta-Llama-3.1-405B-Instruct\n> ```\n\n## Validator .env\n\nSome more robust settings can be applied via a .env file. Eventually, targon\naims to move all settings to a .env file instead of cli arguments. Currently,\nthe following .env variables are supported with their defaults.\n\n```\nAUTO_UPDATE=False # Turn off autoupdate. Overrides cli flag.\nIMAGE_TAG=latest # Verifier image tag. Useful for testing new updates.\nHEARTBEAT=False # Enable heartbeat. Requires pm2. Set to True to enable heartbeat monitoring.\nIS_TESTNET=False # If validator should run in testnet.\n```\n\n## Autoupdate\n\nAutoupdate is implemented in targon/utils.py. This is to ensure that your\ncodebase matches the latest version on Main of the Targon Github Repository.\n\n### Validator Autoupdate\n\nValidator Autoupdate is implemented and defaulted to run once weights have been\nset. To **disable**, please add the flag to your command line build:\n\n```bash\npm2 start neurons/validator.py --name validator --interperter python3 -- --wallet.name [WALLET_NAME] --autoupdate-off\n```\n\n### Miner Autoupdate\n\nMiner Autoupdate is **not** implemented. Miners will need to check the Targon\nrepository and update themselves as new versions are released. If interested in\nutilizing the autoupdate feature that Validators use, please follow the steps\nbelow:\n\n*NOTE*: This will not be maintained by the Manifold Labs Team.\n\n1. Import the autoupdate function into your miner script (neurons/miner.py) at\n   the top of the file.\n\n```python\nfrom targon.updater import autoupdate\n```\n\n3. Call the function at a place of your choosing.\n\n```python\n    if self.config.autoupdate:\n        autoupdate(branch=\"main\")\n\n```\n\n4. Relaunch your miner with the changes.\n\n# What is A Deterministic Verification Network\n\nValidators send queries to miners that are then scored for speed, and verified\nby comparing the logprobs of the responses to a validators own model.\n\n## Role of a Miner\n\nA miner is a node that is responsible for generating a output from a query, both\norganic and synthetic.\n\n## Role of a Validator\n\nA validator is a node that is responsible for verifying a miner's output. The\nvalidator will send an openai compliant request to a miner with. The miner will\nthen send back a response with the output. The validator will then use the log\nprob values of the response to verify that each miners response is accurate.\nValidators will keep score of each miners response time and use their averages\nto assign scores each epoch. Specifically, miner scores are the sum of the\naverage TPS per model.\n\n# Features of Targon\n\n## Full OpenAI Compliance\n\nValidators can query miners directly using any openai package, and Epistula\nheaders. Below is boilerplate for querying a miner in python.\n\n```py\nminer = openai.AsyncOpenAI(\n    base_url=f\"http://{axon.ip}:{axon.port}/v1\",\n    api_key=\"sn4\",\n    max_retries=0,\n    timeout=Timeout(12, connect=5, read=5),\n    http_client=openai.DefaultAsyncHttpxClient(\n        event_hooks={\n            \"request\": [\n                # This injects Epistula headers right before the request is sent.\n                # wallet.hotkey is the public / private keypair\n                #\n                # You can find this function in the `epistula.py` file in \n                # the targon repo\n                create_header_hook(wallet.hotkey, axon.hotkey_ss58)\n            ]\n        }\n    ),\n)\n```\n\n# How to Contribute\n\n## Code Review\n\nProject maintainers reserve the right to weigh the opinions of peer reviewers\nusing common sense judgement and may also weigh based on merit. Reviewers that\nhave demonstrated a deeper commitment and understanding of the project over time\nor who have clear domain expertise may naturally have more weight, as one would\nexpect in all walks of life. Where a patch set affects consensus-critical code,\nthe bar will be much higher in terms of discussion and peer review requirements,\nkeeping in mind that mistakes could be very costly to the wider community. This\nincludes refactoring of consensus-critical code. Where a patch set proposes to\nchange the Targon subnet, it must have been discussed extensively on the discord\nserver and other channels, be accompanied by a widely discussed BIP and have a\ngenerally widely perceived technical consensus of being a worthwhile change\nbased on the judgement of the maintainers. That being said, Manifold welcomes\nall PR's for the betterment of the subnet and Bittensor as a whole. We are\nstriving for improvement at every interval and believe through open\ncommunication and sharing of ideas will success be attainable.\n"
          },
          {
            "fileName": "TERMS.md",
            "fileContents": "### Terms of Service for TARGON search\n\n**Last Updated: \\[01/29/2024\\]**\n\n#### Introduction\n\nWelcome to Targon subnet service (“Service”). These Terms of Service (“Terms”)\ngovern your access to and use of our services, including any applications,\nwebsites, software, and content provided on or through targon. By\nusing our Service, you agree to be bound by these Terms. If you do not agree to\nthese Terms, do not use our Service.\n\n#### 1. Acceptance of Terms\n\nBy creating an account or accessing or using our Service, you agree to be bound\nby these Terms and all applicable laws and regulations.\n\n#### 2. Changes to Terms\n\nWe reserve the right to modify or replace these Terms at any time at our sole\ndiscretion. We will provide notice of any changes by updating the date at the\ntop of these Terms.\n\n#### 3. User Responsibilities\n\n- **3.1 Content Ownership**: You retain all rights and ownership of your data.\n  We do not claim any ownership rights to the content you upload to the Service.\n- **3.2 Acceptable Use**: You agree not to use the Service to store, transmit,\n  or share any data that violates any applicable law, including data that is\n  illegal, infringing, or defamatory. You also agree not to use the Service to\n  store, transmit, or share any “sensitive” personal information, including\n  protected health information covered under HIPAA, financial information\n  covered under PCI, and any other information that would fall under the\n  definition of “sensitive” or “special categories” of data under applicable\n  law.\n\n#### 4. Prohibited Conduct\n\nYou agree not to engage in any of the following activities: (a) Violating any\nlaws or regulations; (b) Infringing the intellectual property or other rights of\nthird parties; (c) Transmitting viruses or harmful code; (d) Attempting to\nbreach or compromise any security measures of the Service.\n\n#### 5. Termination\n\nWe may terminate or suspend your access to the Service immediately, without\nprior notice or liability, if you breach the Terms.\n\n#### 6. Disclaimers\n\nThe Service is provided on an “AS IS” and “AS AVAILABLE” basis. We disclaim all\nwarranties and representations, express or implied, including the implied\nwarranties of merchantability and fitness for a particular purpose.\n\n#### 7. Limitation of Liability\n\nWe shall not be liable for any indirect, incidental, special, consequential, or\npunitive damages, including loss of profits, data, or use, arising out of or in\nconnection with these Terms or the Service, whether in an action of contract,\ntort, or otherwise.\n\n#### 8. Indemnification\n\nYou agree to indemnify, defend, and hold harmless Manifold, its officers,\ndirectors, employees, agents, licensors, suppliers, and any third-party\ninformation providers from and against all losses, expenses, damages, and costs,\nincluding reasonable attorneys' fees, resulting from any violation of these\nTerms or any activity related to your account (including negligent or wrongful\nconduct).\n\n#### 9. Governing Law\n\nThese Terms shall be governed and construed in accordance with the laws of your\nlocal jurisdiction, without regard to its conflict of law provisions.\n\n#### 10. Contact Us\n\nIf you have any questions about these Terms, please contact us at\n\\[operations@manifold.inc\\].\n\n"
          },
          {
            "fileName": "VERSION",
            "fileContents": "4.4.2\n"
          },
          {
            "fileName": "curl.txt",
            "fileContents": "     time_namelookup:  %{time_namelookup}s\\n\n        time_connect:  %{time_connect}s\\n\n     time_appconnect:  %{time_appconnect}s\\n\n    time_pretransfer:  %{time_pretransfer}s\\n\n       time_redirect:  %{time_redirect}s\\n\n  time_starttransfer:  %{time_starttransfer}s\\n\n                     ----------\\n\n          time_total:  %{time_total}s\\n\n"
          },
          {
            "fileName": "docker-compose.testnet.yml",
            "fileContents": "services:\n  miner-vllm:\n    image: manifoldlabs/vllm-openai:latest\n    runtime: nvidia\n    command: --model NousResearch/Meta-Llama-3.1-8B-Instruct\n    ports:\n      - 9000:8000\n    ipc: host\n    volumes:\n      - hf_cache:/root/.cache/huggingface\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids:\n                - \"1\"\n              capabilities: [gpu]\nvolumes:\n  hf_cache:\n"
          },
          {
            "fileName": "extra",
            "fileContents": ""
          },
          {
            "fileName": "requirements.txt",
            "fileContents": "\nbittensor==7.1.2\nopenai==1.44.1\nnanoid==2.0.0\n"
          },
          {
            "fileName": "send_request_to_miner.py",
            "fileContents": "from time import time\nimport bittensor as bt\nfrom typing import Any, List, Optional\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom substrateinterface import Keypair\nfrom bittensor.subtensor import Dict, Union\nfrom httpx import Timeout\nfrom openai import DefaultHttpxClient, OpenAI\nfrom math import ceil\nimport httpx\nfrom hashlib import sha256\nfrom uuid import uuid4\nimport json\n\n\ndef create_header_hook(hotkey, axon_hotkey, model):\n    def add_headers(request: httpx.Request):\n        for key, header in generate_header(hotkey, request.read(), axon_hotkey).items():\n            request.headers[key] = header\n        request.headers[\"X-Targon-Model\"] = model\n\n    return add_headers\n\n\ndef generate_header(\n    hotkey: Keypair,\n    body: Union[Dict[Any, Any], List[Any], bytes],\n    signed_for: Optional[str] = None,\n) -> Dict[str, Any]:\n    timestamp = round(time() * 1000)\n    timestampInterval = ceil(timestamp / 1e4) * 1e4\n    uuid = str(uuid4())\n    req_hash = None\n    if isinstance(body, bytes):\n        req_hash = sha256(body).hexdigest()\n    else:\n        req_hash = sha256(json.dumps(body).encode(\"utf-8\")).hexdigest()\n\n    headers = {\n        \"Epistula-Version\": str(2),\n        \"Epistula-Timestamp\": str(timestamp),\n        \"Epistula-Uuid\": uuid,\n        \"Epistula-Signed-By\": hotkey.ss58_address,\n        \"Epistula-Request-Signature\": \"0x\"\n        + hotkey.sign(f\"{req_hash}.{uuid}.{timestamp}.{signed_for or ''}\").hex(),\n    }\n    if signed_for:\n        headers[\"Epistula-Signed-For\"] = signed_for\n        headers[\"Epistula-Secret-Signature-0\"] = (\n            \"0x\" + hotkey.sign(str(timestampInterval - 1) + \".\" + signed_for).hex()\n        )\n        headers[\"Epistula-Secret-Signature-1\"] = (\n            \"0x\" + hotkey.sign(str(timestampInterval) + \".\" + signed_for).hex()\n        )\n        headers[\"Epistula-Secret-Signature-2\"] = (\n            \"0x\" + hotkey.sign(str(timestampInterval + 1) + \".\" + signed_for).hex()\n        )\n    return headers\n\n\ndef make_client(miner_uid):\n    wallet = bt.wallet()  # Set your wallet config\n    subtensor = bt.subtensor()\n    metagraph = subtensor.metagraph(4)\n    axon_info = metagraph.axons[miner_uid]\n    client = OpenAI(\n        base_url=f\"http://{axon_info.ip}:{axon_info.port}/v1\",\n        api_key=\"sn4\",\n        max_retries=0,\n        timeout=Timeout(12, connect=5, read=5),\n        http_client=DefaultHttpxClient(\n            event_hooks={\n                \"request\": [\n                    create_header_hook(\n                        wallet.hotkey,\n                        axon_info.hotkey,\n                        \"NousResearch/Meta-Llama-3.1-8B-Instruct\",\n                    )\n                ]\n            }\n        ),\n    )\n    return client\n\n\nif __name__ == \"__main__\":\n    messages: List[ChatCompletionMessageParam] = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": f\"What is the deffinition of the x y problem \",\n        },\n    ]\n    model = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n    MINER_UID = -1\n    client = make_client(MINER_UID)\n    response = client.chat.completions.create(\n        model=model,\n        stream=True,\n        logprobs=True,\n        max_tokens=100,\n        messages=messages,\n    )\n    for chunk in response:\n        content = chunk.choices[0].delta.content\n        if content:\n            print(content, end=\"\")\n"
          },
          {
            "fileName": "justfile",
            "fileContents": "set dotenv-load\n# This file is mostly for testing, but can be used as reference for what commands should look like\n\ndefault:\n  @just --list\n\nvalidator:\n  python3 neurons/validator.py --wallet.name validator --netuid 40 --subtensor.network test --epoch-length 101 --logging.trace --autoupdate-off --mock --models.mode endpoint --models.endpoint https://targon.sybil.com/api/models\n\nminer num=\"0\":\n  python neurons/miner.py --wallet.name miner --netuid 40 --wallet.hotkey new-miner{{num}} --subtensor.network test --model-endpoint http://localhost:9000/v1 --axon.port 700{{num}} --api_key abc123 --mock --no-force-validator-permit\n\nscript script_name opts=\"\":\n  python3 scripts/{{script_name}}.py --wallet.name validator --netuid 40 --subtensor.network test --neuron.port 8080 --epoch-length 101 --logging.trace {{opts}}\n\nup:\n  docker compose -f docker-compose.testnet.yml build\n  docker compose -f docker-compose.testnet.yml up -d\n\nbuild_verifier tag='latest':\n  cd verifier && docker build -t manifoldlabs/sn4-verifier:{{tag}} .\n\nrun_verifier model port gpu tag:\n  docker run -p {{port}}:80 -e MODEL={{model}} -e GPU_MEMORY_UTIL=.9 --runtime=nvidia --ipc=host --gpus='\"device={{gpu}}\"' -v ~/.cache/huggingface:/root/.cache/huggingface -d --name dev_image manifoldlabs/sn4-verifier:{{tag}}\n\nrun_verifier_prod model port gpu gpus name memory_util='.9' tag='latest':\n  docker run -p {{port}}:80 -e MODEL={{model}} -e TENSOR_PARALLEL={{gpus}} -e GPU_MEMORY_UTIL={{memory_util}} -l model={{model}} -l port={{port}} --runtime=nvidia --ipc=host --gpus='\"device={{gpu}}\"' -d --name {{name}} manifoldlabs/sn4-verifier:{{tag}}\n\npush_verifier: build_verifier\n  docker push manifoldlabs/sn4-verifier:latest\n"
          },
          {
            "fileName": "neurons",
            "fileContents": ""
          },
          {
            "fileName": "base.py",
            "fileContents": "import argparse\nfrom threading import Thread\nfrom typing import Callable, List\nimport bittensor as bt\nimport copy\n\nfrom nest_asyncio import asyncio\nfrom substrateinterface import SubstrateInterface\nfrom targon import (\n    add_args,\n    add_validator_args,\n    validate_config_and_neuron_path,\n)\nfrom targon.config import add_miner_args\nfrom enum import Enum\nimport signal\n\nfrom targon import (\n    __spec_version__ as spec_version,\n)\nfrom targon.metagraph import run_block_callback_thread\nfrom targon.utils import ExitContext\nfrom bittensor.core.settings import SS58_FORMAT, TYPE_REGISTRY\n\n\nclass NeuronType(Enum):\n    Validator = \"VALIDATOR\"\n    Miner = \"MINER\"\n\n\nclass BaseNeuron:\n    config: \"bt.config\"\n    neuron_type: NeuronType\n    exit_context = ExitContext()\n    next_sync_block = None\n    block_callbacks: List[Callable] = []\n    substrate_thread: Thread\n\n    def check_registered(self):\n        if not self.subtensor.is_hotkey_registered(\n            netuid=self.config.netuid,\n            hotkey_ss58=self.wallet.hotkey.ss58_address,\n        ):\n            bt.logging.error(\n                f\"Wallet: {self.wallet} is not registered on netuid {self.config.netuid}.\"\n                f\" Please register the hotkey using `btcli subnets register` before trying again\"\n            )\n            exit()\n\n    def maybe_sync_metagraph(self, block):\n        assert self.config.neuron\n        if block % self.config.epoch_length:\n            return False\n\n        # Ensure miner or validator hotkey is still registered on the network.\n        self.check_registered()\n        bt.logging.info(\"Resyncing Metagraph\")\n        self.metagraph.sync(subtensor=self.subtensor)\n        return True\n\n    def run_callbacks(self, block):\n        for callback in self.block_callbacks:\n            callback(block)\n\n    def __init__(self, config=None):\n        # Add parser args\n        bt.logging.info(f\"Targon version {spec_version}\")\n        parser = argparse.ArgumentParser()\n        bt.wallet.add_args(parser)\n        bt.subtensor.add_args(parser)\n        bt.logging.add_args(parser)\n        bt.axon.add_args(parser)\n        add_args(parser)\n        if self.neuron_type == NeuronType.Validator:\n            add_validator_args(parser)\n        if self.neuron_type == NeuronType.Miner:\n            add_miner_args(parser)\n        self.config = bt.config(parser)\n        if config:\n            base_config = copy.deepcopy(config)\n            self.config.merge(base_config)\n        validate_config_and_neuron_path(self.config)\n\n        ## Add kill signals\n        signal.signal(signal.SIGINT, self.exit_context.startExit)\n        signal.signal(signal.SIGTERM, self.exit_context.startExit)\n\n        ## Typesafety\n        assert self.config.logging\n        assert self.config.neuron\n        assert self.config.netuid\n        assert self.config.axon\n        assert self.config.subtensor\n\n        ## LOGGING\n        bt.logging(config=self.config, logging_dir=self.config.neuron.full_path)\n        bt.logging.set_info()\n        if self.config.logging.debug:\n            bt.logging.set_debug(True)\n        if self.config.logging.trace:\n            bt.logging.set_trace(True)\n\n        ## BITTENSOR INITIALIZATION\n        self.wallet = bt.wallet(config=self.config)\n        self.subtensor = bt.subtensor(config=self.config)\n        self.metagraph = self.subtensor.metagraph(self.config.netuid)\n\n        self.loop = asyncio.get_event_loop()\n        bt.logging.debug(f\"Wallet: {self.wallet}\")\n        bt.logging.debug(f\"Subtensor: {self.subtensor}\")\n        bt.logging.debug(f\"Metagraph: {self.metagraph}\")\n\n        ## CHECK IF REGG'D\n        self.check_registered()\n        self.uid = self.metagraph.hotkeys.index(self.wallet.hotkey.ss58_address)\n\n        ## Substrate, Subtensor and Metagraph\n        self.substrate = SubstrateInterface(\n            ss58_format=SS58_FORMAT,\n            use_remote_preset=True,\n            url=self.config.subtensor.chain_endpoint,\n            type_registry=TYPE_REGISTRY,\n        )\n        self.block_callbacks.append(self.maybe_sync_metagraph)\n        self.substrate_thread = run_block_callback_thread(self.substrate, self.run_callbacks)\n"
          },
          {
            "fileName": "miner.py",
            "fileContents": "import traceback\nimport time\nfrom bittensor.core.axon import FastAPIThreadedServer\nfrom bittensor.core.extrinsics.serving import serve_extrinsic\nfrom fastapi import APIRouter, Depends, FastAPI, HTTPException, Request\nimport httpx\nimport netaddr\nimport requests\nfrom starlette.background import BackgroundTask\nfrom starlette.responses import StreamingResponse\n\nfrom neurons.base import BaseNeuron, NeuronType\nfrom targon.epistula import verify_signature\nfrom targon.utils import print_info\nimport uvicorn\nimport bittensor as bt\n\n\nclass Miner(BaseNeuron):\n    neuron_type = NeuronType.Miner\n    fast_api: FastAPIThreadedServer\n\n    def shutdown(self):\n        if self.fast_api:\n            self.fast_api.stop()\n\n    def log_on_block(self, block):\n        print_info(\n            self.metagraph,\n            self.wallet.hotkey.ss58_address,\n            block,\n        )\n\n    def __init__(self, config=None):\n        super().__init__(config)\n        bt.logging.set_info()\n        ## Typesafety\n        assert self.config.netuid\n        assert self.config.logging\n        assert self.config.model_endpoint\n\n        # Register log callback\n        self.block_callbacks.append(self.log_on_block)\n\n        ## BITTENSOR INITIALIZATION\n        bt.logging.info(\n            \"\\N{grinning face with smiling eyes}\", \"Successfully Initialized!\"\n        )\n        bt.logging.info(self.config.model_endpoint)\n        self.client = httpx.AsyncClient(\n            base_url=self.config.model_endpoint,\n            headers={\"Authorization\": f\"Bearer {self.config.api_key}\"},\n        )\n\n    async def create_chat_completion(self, request: Request):\n        bt.logging.info(\n            \"\\u2713\",\n            f\"Getting Chat Completion request from {request.headers.get('Epistula-Signed-By', '')[:8]}!\",\n        )\n        req = self.client.build_request(\n            \"POST\", \"/chat/completions\", content=await request.body()\n        )\n        r = await self.client.send(req, stream=True)\n        return StreamingResponse(\n            r.aiter_raw(), background=BackgroundTask(r.aclose), headers=r.headers\n        )\n\n    async def create_completion(self, request: Request):\n        bt.logging.info(\n            \"\\u2713\",\n            f\"Getting Completion request from {request.headers.get('Epistula-Signed-By', '')[:8]}!\",\n        )\n        req = self.client.build_request(\n            \"POST\", \"/completions\", content=await request.body()\n        )\n        r = await self.client.send(req, stream=True)\n        return StreamingResponse(\n            r.aiter_raw(), background=BackgroundTask(r.aclose), headers=r.headers\n        )\n\n    async def receive_models(self, request: Request):\n        models = await request.json()\n        bt.logging.info(\n            \"\\u2713\",\n            f\"Received model list from {request.headers.get('Epistula-Signed-By', '')[:8]}: {models}\",\n        )\n\n        #\n        # Add extra logic here for how your miner should handle the model list.\n        #\n\n        return \"\"\n\n    async def list_models(self):\n        #\n        # Return models the miner is running\n        #\n\n        return []\n\n    async def determine_epistula_version_and_verify(self, request: Request):\n        version = request.headers.get(\"Epistula-Version\")\n        if version == \"2\":\n            await self.verify_request(request)\n            return\n        raise HTTPException(status_code=400, detail=\"Unknown Epistula version\")\n\n    async def verify_request(\n        self,\n        request: Request,\n    ):\n        # We do this as early as possible so that now has a lesser chance\n        # of causing a stale request\n        now = round(time.time() * 1000)\n\n        # We need to check the signature of the body as bytes\n        # But use some specific fields from the body\n        signed_by = request.headers.get(\"Epistula-Signed-By\")\n        signed_for = request.headers.get(\"Epistula-Signed-For\")\n        if signed_for != self.wallet.hotkey.ss58_address:\n            raise HTTPException(\n                status_code=400, detail=\"Bad Request, message is not intended for self\"\n            )\n        if signed_by not in self.metagraph.hotkeys:\n            raise HTTPException(status_code=401, detail=\"Signer not in metagraph\")\n\n        uid = self.metagraph.hotkeys.index(signed_by)\n        stake = self.metagraph.S[uid].item()\n        if not self.config.no_force_validator_permit and stake < 10000:\n            bt.logging.warning(\n                f\"Blacklisting request from {signed_by} [uid={uid}], not enough stake -- {stake}\"\n            )\n            raise HTTPException(status_code=401, detail=\"Stake below minimum: {stake}\")\n\n        # If anything is returned here, we can throw\n        body = await request.body()\n        err = verify_signature(\n            request.headers.get(\"Epistula-Request-Signature\"),\n            body,\n            request.headers.get(\"Epistula-Timestamp\"),\n            request.headers.get(\"Epistula-Uuid\"),\n            signed_for,\n            signed_by,\n            now,\n        )\n        if err:\n            bt.logging.error(err)\n            raise HTTPException(status_code=400, detail=err)\n\n    def run(self):\n        assert self.config.netuid\n        assert self.config.subtensor\n        assert self.config.axon\n\n        # Serve passes the axon information to the network + netuid we are hosting on.\n        # This will auto-update if the axon port of external ip have changed.\n        external_ip = self.config.axon.external_ip or self.config.axon.ip\n        if not external_ip or external_ip == \"[::]\":\n            try:\n                external_ip = requests.get(\"https://checkip.amazonaws.com\").text.strip()\n                netaddr.IPAddress(external_ip)\n            except Exception:\n                bt.logging.error(\"Failed to get external IP\")\n\n        bt.logging.info(\n            f\"Serving miner endpoint {external_ip}:{self.config.axon.port} on network: {self.config.subtensor.chain_endpoint} with netuid: {self.config.netuid}\"\n        )\n\n        serve_success = serve_extrinsic(\n            subtensor=self.subtensor,\n            wallet=self.wallet,\n            ip=external_ip,\n            port=self.config.axon.port,\n            protocol=4,\n            netuid=self.config.netuid,\n        )\n        if not serve_success:\n            bt.logging.error(\"Failed to serve endpoint\")\n            return\n\n        # Start  starts the miner's endpoint, making it active on the network.\n        # change the config in the axon\n        app = FastAPI()\n        router = APIRouter()\n        router.add_api_route(\n            \"/v1/chat/completions\",\n            self.create_chat_completion,\n            dependencies=[Depends(self.determine_epistula_version_and_verify)],\n            methods=[\"POST\"],\n        )\n        router.add_api_route(\n            \"/v1/completions\",\n            self.create_completion,\n            dependencies=[Depends(self.determine_epistula_version_and_verify)],\n            methods=[\"POST\"],\n        )\n        router.add_api_route(\n            \"/models\",\n            self.receive_models,\n            dependencies=[Depends(self.determine_epistula_version_and_verify)],\n            methods=[\"POST\"],\n        )\n        router.add_api_route(\n            \"/models\",\n            self.list_models,\n            dependencies=[Depends(self.determine_epistula_version_and_verify)],\n            methods=[\"GET\"],\n        )\n        app.include_router(router)\n        fast_config = uvicorn.Config(\n            app,\n            host=\"0.0.0.0\",\n            port=self.config.axon.port,\n            log_level=\"info\",\n            loop=\"asyncio\",\n        )\n        self.fast_api = FastAPIThreadedServer(config=fast_config)\n        self.fast_api.start()\n\n        bt.logging.info(f\"Miner starting at block: {self.subtensor.block}\")\n\n        # This loop maintains the miner's operations until intentionally stopped.\n        try:\n            while not self.exit_context.isExiting:\n                time.sleep(1)\n        except Exception as e:\n            bt.logging.error(str(e))\n            bt.logging.error(traceback.format_exc())\n        self.shutdown()\n\n\nif __name__ == \"__main__\":\n    try:\n        miner = Miner()\n        miner.run()\n    except Exception as e:\n        bt.logging.error(str(e))\n        bt.logging.error(traceback.format_exc())\n    exit()\n"
          },
          {
            "fileName": "validator.py",
            "fileContents": "import json\nimport random\nimport asyncio\nimport sys\nfrom threading import Thread\nfrom time import sleep\n\nfrom asyncpg.connection import asyncpg\nfrom bittensor.core.settings import SS58_FORMAT, TYPE_REGISTRY\nimport httpx\nfrom substrateinterface import SubstrateInterface\nfrom neurons.base import BaseNeuron, NeuronType\nfrom targon.cache import load_cache\nfrom targon.config import (\n    AUTO_UPDATE,\n    HEARTBEAT,\n    IS_TESTNET,\n    SLIDING_WINDOW,\n    get_models_from_config,\n    get_models_from_endpoint,\n)\nfrom targon.dataset import download_dataset\nfrom targon.docker import load_docker, sync_output_checkers\nfrom targon.epistula import generate_header\nfrom targon.jugo import score_organics, send_organics_to_jugo, send_stats_to_jugo\nfrom targon.math import get_weights\nfrom targon.metagraph import (\n    create_set_weights,\n    get_miner_uids,\n    resync_hotkeys,\n    run_block_callback_thread,\n)\nfrom targon.request import check_tokens, generate_request, handle_inference\nfrom targon.updater import autoupdate\nfrom targon.utils import (\n    fail_with_none,\n    print_info,\n)\nfrom targon.types import Endpoints, InferenceStats\nimport traceback\nimport bittensor as bt\n\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom targon import (\n    __version__,\n    __spec_version__ as spec_version,\n)\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nclass Validator(BaseNeuron):\n    neuron_type = NeuronType.Validator\n    miner_tps: Dict[int, Dict[str, List[Optional[float]]]]\n    miner_models: Dict[int, List[str]]\n    db: Optional[asyncpg.Connection]\n    verification_ports: Dict[str, Dict[str, Any]]\n    models: List[str]\n    lock_waiting = False\n    lock_halt = False\n    is_runing = False\n    organics = {}\n    last_bucket_id = None\n    heartbeat_thread: Thread\n    step = 0\n    dataset = None\n\n    def __init__(self, config=None, run_init=True):\n        super().__init__(config)\n        ## Typesafety\n        self.set_weights = create_set_weights(spec_version, self.config.netuid)\n\n        ## CHECK IF REGG'D\n        if not self.metagraph.validator_permit[self.uid] and not IS_TESTNET:\n            bt.logging.error(\"Validator does not have vpermit\")\n            exit()\n        if run_init:\n            self.init()\n\n    def init(self):\n        assert self.config.netuid\n        assert self.config.cache_file\n        assert self.config.vpermit_tao_limit\n        assert self.config.database\n        assert self.config.subtensor\n        ## LOAD DOCKER\n        self.client = load_docker()\n\n        ## SET MISC PARAMS\n        self.next_forward_block = None\n        self.last_posted_weights = self.metagraph.last_update[self.uid]\n        bt.logging.info(f\"Last updated at block {self.last_posted_weights}\")\n\n        ## LOAD MINER SCORES CACHE\n        miners = get_miner_uids(self.metagraph, self.uid, self.config.vpermit_tao_limit)\n        self.miner_tps = load_cache(\n            self.config.cache_file, self.subtensor.block, miners\n        )\n\n        ## LOAD DATASET\n        bt.logging.info(\"⌛️\", \"Loading dataset\")\n        self.dataset = download_dataset()\n\n        ## CONNECT TO ORGANICS DB\n        try:\n            self.db = None\n            if self.config.database.url:\n                self.db = self.loop.run_until_complete(\n                    asyncpg.connect(self.config.database.url)\n                )\n        except Exception as e:\n            bt.logging.error(f\"Failed to initialize organics database: {e}\")\n\n        ## REGISTER BLOCK CALLBACKS\n        self.block_callbacks.extend(\n            [\n                self.log_on_block,\n                self.set_weights_on_interval,\n                self.sync_output_checkers_on_interval,\n                self.resync_hotkeys_on_interval,\n                self.send_models_to_miners_on_interval,\n                self.score_organics_on_block,\n            ]\n        )\n\n        # Setup heartbeat thread\n        if HEARTBEAT:\n            self.heartbeat_thread = Thread(name=\"heartbeat\", target=self.heartbeat)\n            self.heartbeat_thread.start()\n\n        ## DONE\n        bt.logging.info(\n            \"\\N{grinning face with smiling eyes}\", \"Successfully Initialized!\"\n        )\n\n    def heartbeat(self):\n        bt.logging.info(\"Starting Heartbeat\")\n        last_step = self.step\n        stuck_count = 0\n        while True:\n            sleep(60)\n            if last_step == self.step:\n                stuck_count += 1\n            if last_step != self.step:\n                stuck_count = 0\n            if stuck_count >= 5:\n                bt.logging.error(\n                    \"Heartbeat detecting main process hang, attempting restart\"\n                )\n                autoupdate(force=True)\n                sys.exit(0)\n            last_step = self.step\n            bt.logging.info(\"Heartbeat\")\n\n    def send_models_to_miners_on_interval(self, block):\n        assert self.config.vpermit_tao_limit\n        if block % self.config.epoch_length:\n            return\n\n        if block != 0 and not self.is_runing:\n            return\n        miner_uids = get_miner_uids(\n            self.metagraph, self.uid, self.config.vpermit_tao_limit\n        )\n        self.miner_models = {}\n        bt.logging.info(\"Broadcasting models to all miners\")\n        body = self.models\n        for uid in miner_uids:\n            bt.logging.info(f\"Broadcasting models {uid}\")\n            axon_info = self.metagraph.axons[uid]\n            headers = generate_header(self.wallet.hotkey, body, axon_info.hotkey)\n            headers[\"Content-Type\"] = \"application/json\"\n            try:\n                httpx.post(\n                    f\"http://{axon_info.ip}:{axon_info.port}/models\",\n                    headers=headers,\n                    json=body,\n                    timeout=3,\n                )\n                headers = generate_header(self.wallet.hotkey, b\"\", axon_info.hotkey)\n                res = httpx.get(\n                    f\"http://{axon_info.ip}:{axon_info.port}/models\",\n                    headers=headers,\n                    timeout=3,\n                )\n                if res.status_code != 200 or not isinstance(models := res.json(), list):\n                    models = []\n                self.miner_models[uid] = list(set(models))\n            except Exception:\n                self.miner_models[uid] = []\n        bt.logging.info(\"Miner models: \" + str(self.miner_models))\n\n    def resync_hotkeys_on_interval(self, block):\n        if not self.is_runing:\n            return\n        if block % self.config.epoch_length:\n            return\n        resync_hotkeys(self.metagraph, self.miner_tps)\n\n    def sync_output_checkers_on_interval(self, block):\n        if not self.is_runing:\n            return\n        if block % self.config.epoch_length:\n            return\n        self.lock_halt = True\n        while not self.lock_waiting:\n            sleep(1)\n        self.models = self.get_models()\n        self.verification_ports = sync_output_checkers(self.client, self.models)\n        self.lock_halt = False\n\n    def score_organics_on_block(self, block):\n        if not self.is_runing:\n            return\n        if block % 20:\n            return\n        res = asyncio.run(\n            score_organics(self.last_bucket_id, self.verification_ports, self.wallet)\n        )\n        if res == None:\n            return\n        bucket_id, organics, organic_stats = res\n        self.last_bucket_id = bucket_id\n        if organics == None or organic_stats == None:\n            return\n        self.organics = organics\n        asyncio.run(send_organics_to_jugo(self.wallet, organic_stats))\n\n    def set_weights_on_interval(self, block):\n        if block % self.config.epoch_length:\n            return\n        self.lock_halt = True\n        while not self.lock_waiting:\n            sleep(1)\n        self.set_weights(\n            self.wallet,\n            self.metagraph,\n            self.subtensor,\n            get_weights(\n                self.miner_models,\n                self.miner_tps,\n                self.organics,\n                self.models,\n            ),\n        )\n\n        # Only keep last 30 scores\n        for uid in self.miner_tps:\n            for model in self.miner_tps[uid]:\n                self.miner_tps[uid][model] = self.miner_tps[uid][model][-SLIDING_WINDOW:]\n        self.lock_halt = False\n\n    def log_on_block(self, block):\n        blocks_till = self.config.epoch_length - (block % self.config.epoch_length)\n        print_info(\n            self.metagraph,\n            self.wallet.hotkey.ss58_address,\n            block,\n        )\n        bt.logging.info(\n            f\"Forward Block: {self.subtensor.block} | Blocks till Set Weights: {blocks_till}\"\n        )\n\n    def run(self):\n        assert self.config.subtensor\n        assert self.config.neuron\n        assert self.config.database\n        assert self.config.vpermit_tao_limit\n        bt.logging.info(\n            f\"Running validator on network: {self.config.subtensor.chain_endpoint} with netuid: {self.config.netuid}\"\n        )\n\n        bt.logging.info(f\"Validator starting at block: {self.subtensor.block}\")\n\n        # This loop maintains the validator's operations until intentionally stopped.\n        miner_subset = 36\n\n        # Ensure everything is setup\n        self.models = self.get_models()\n        self.verification_ports = sync_output_checkers(self.client, self.models)\n        resync_hotkeys(self.metagraph, self.miner_tps)\n        self.send_models_to_miners_on_interval(0)\n\n        self.is_runing = True\n        while not self.exit_context.isExiting:\n            self.step += 1\n            if self.config.autoupdate and not AUTO_UPDATE:\n                autoupdate(branch=\"main\")\n            # Make sure our substrate thread is alive\n            if not self.substrate_thread.is_alive():\n                self.substrate = SubstrateInterface(\n                    ss58_format=SS58_FORMAT,\n                    use_remote_preset=True,\n                    url=self.config.subtensor.chain_endpoint,\n                    type_registry=TYPE_REGISTRY,\n                )\n                self.substrate_thread = run_block_callback_thread(\n                    self.substrate, self.run_callbacks\n                )\n\n            # Mutex for setting weights\n            if self.lock_halt:\n                self.lock_waiting = True\n                while self.lock_halt:\n                    sleep(1)\n                self.lock_waiting = False\n\n            # Random model, but every three is a model we are verifying for sure\n            model_name = random.choice(self.models)\n            if self.step % 3 == 0:\n                model_name = random.choice(list(self.verification_ports.keys()))\n\n            endpoint_model = list(self.verification_ports.keys())[0]\n            if self.verification_ports.get(model_name) != None:\n                endpoint = random.choice(\n                    self.verification_ports[model_name][\"endpoints\"]\n                )\n                generator_model_name = model_name\n            else:\n                endpoint = random.choice(\n                    self.verification_ports[endpoint_model][\"endpoints\"]\n                )\n                generator_model_name = endpoint_model\n            uids = get_miner_uids(\n                self.metagraph, self.uid, self.config.vpermit_tao_limit\n            )\n            random.shuffle(uids)\n            miner_uids = []\n            for uid in uids:\n                if len(miner_uids) > miner_subset:\n                    break\n\n                # Make sure tps array exists\n                if self.miner_tps[uid].get(model_name) is None:\n                    self.miner_tps[uid][model_name] = []\n\n                if model_name not in self.miner_models.get(uid, []):\n                    self.miner_tps[uid][model_name].append(None)\n                    continue\n                miner_uids.append(uid)\n\n            # Skip if no miners running this model\n            if not len(miner_uids):\n                bt.logging.info(\"No miners for this model\")\n                continue\n\n            res = self.loop.run_until_complete(\n                self.query_miners(\n                    miner_uids, model_name, endpoint, generator_model_name\n                )\n            )\n            self.save_scores()\n            if res is not None:\n                self.loop.run_until_complete(\n                    send_stats_to_jugo(\n                        self.metagraph,\n                        self.subtensor,\n                        self.wallet,\n                        *res,\n                        spec_version,\n                        self.models,\n                        self.miner_tps,\n                    )\n                )\n\n        # Exiting\n        self.shutdown()\n\n    async def verify_response(self, uid, request, endpoint, stat: InferenceStats):\n        if stat.error or stat.cause:\n            return uid, stat\n        # We do this out of the handle_inference loop to not block other requests\n        verification_port = self.verification_ports.get(\n            request[\"model\"], {\"port\": None}\n        ).get(\"port\")\n        if verification_port is None:\n            bt.logging.error(\n                \"Send request to a miner without verification port for model\"\n            )\n            return uid, None\n        verified = await check_tokens(\n            request, stat.tokens, uid, endpoint=endpoint, port=verification_port\n        )\n        if verified is None:\n            return uid, None\n        stat.verified = (\n            verified.get(\"verified\", False) if verified is not None else False\n        )\n        if stat.error is None and not stat.verified:\n            stat.error = verified.get(\"error\")\n            stat.cause = verified.get(\"cause\")\n        return uid, stat\n\n    async def query_miners(\n        self,\n        miner_uids: List[int],\n        model_name: str,\n        endpoint: Endpoints,\n        generator_model_name: str,\n    ):\n        assert self.config.database\n\n        verification_port: Optional[int] = self.verification_ports.get(\n            generator_model_name, {\"port\": None}\n        ).get(\"port\")\n        if verification_port is None:\n            bt.logging.error(\n                f\"No generator / verifier found for {generator_model_name}\"\n            )\n            return None\n        request = generate_request(\n            self.dataset, generator_model_name, endpoint, verification_port\n        )\n        if not request:\n            bt.logging.info(\"No request was generated\")\n            return None\n\n        bt.logging.info(f\"{model_name} - {endpoint}: {request}\")\n\n        # We do these in separate groups for better response timings\n        tasks = []\n        try:\n            for uid in miner_uids:\n                tasks.append(\n                    asyncio.create_task(\n                        handle_inference(\n                            self.metagraph, self.wallet, request, uid, endpoint\n                        )\n                    )\n                )\n            responses: List[Tuple[int, InferenceStats]] = await asyncio.gather(*tasks)\n\n            # Skip scoring if we arent running that model\n            if generator_model_name != model_name:\n                return None\n\n            tasks = []\n            for uid, stat in responses:\n                tasks.append(\n                    asyncio.create_task(\n                        self.verify_response(uid, request, endpoint, stat)\n                    )\n                )\n            stats: List[Tuple[int, Optional[InferenceStats]]] = await asyncio.gather(\n                *tasks\n            )\n        except Exception:\n            bt.logging.error(f\"Failed sending requests: {traceback.format_exc()}\")\n            stats = []\n        processed_stats = []\n        for uid, stat in stats:\n            if not stat:\n                continue\n            processed_stats.append((uid, stat))\n            bt.logging.info(f\"{uid}: {stat.verified} | {stat.total_time}\")\n            if not stat.verified and stat.error:\n                bt.logging.info(str(stat.cause))\n\n            # UID is not in our miner tps list\n            if self.miner_tps.get(uid) is None:\n                self.miner_tps[uid] = {request[\"model\"]: []}\n            # This uid doesnt have reccords of this model\n            if self.miner_tps[uid].get(request[\"model\"]) is None:\n                self.miner_tps[uid][request[\"model\"]] = []\n\n            if stat.verified and stat.total_time != 0:\n                self.miner_tps[uid][request[\"model\"]].append(stat.tps)\n                continue\n            self.miner_tps[uid][request[\"model\"]].append(None)\n        return (processed_stats, request, endpoint)\n\n    @fail_with_none(\"Failed writing to cache file\")\n    def save_scores(self):\n        assert self.config.cache_file\n        if self.exit_context.isExiting:\n            return\n        try:\n            with open(self.config.cache_file, \"w\") as file:\n                bt.logging.info(\"Caching scores...\")\n                json.dump(\n                    {\n                        \"miner_tps\": self.miner_tps,\n                        \"block_saved\": self.subtensor.block,\n                        \"version\": spec_version,\n                    },\n                    file,\n                )\n                file.flush()\n                bt.logging.info(\"Cached\")\n        except Exception as e:\n            bt.logging.error(f\"Failed writing to cache file: {e}\")\n\n    def shutdown(self):\n        if self.db:\n            bt.logging.info(\"Closing organics db connection\")\n            self.loop.run_until_complete(self.db.close())\n\n    def get_models(self) -> List[str]:\n        \"\"\"\n        List of models and sizes of models\n        Miners are scored based\n        - How large is the model\n        - How many models are we testing them on\n        - How fast\n\n        Ask miners what models they are running\n        score based on what models valis want\n        Let valis inspect what most popular models are\n        - Top valis manually decide via model leasing\n        - Minor valis follow along for consensus\n        \"\"\"\n        assert self.config.models\n\n        match self.config.models.mode:\n            case \"config\":\n                models = get_models_from_config()\n                if not models:\n                    raise Exception(\"No models\")\n                return models\n            case _:\n                models = get_models_from_endpoint(self.config.models.endpoint)\n                if not models:\n                    raise Exception(\"No models\")\n                return models\n\n\nif __name__ == \"__main__\":\n    try:\n        validator = Validator()\n        validator.run()\n    except Exception as e:\n        bt.logging.error(str(e))\n        bt.logging.error(traceback.format_exc())\n    exit()\n"
          },
          {
            "fileName": "requirements.txt",
            "fileContents": "bittensor==8.5.1\npandas==2.2.2\ndatasets==3.2.0\nhuggingface_hub==0.27.0\nnumpy==2.0.1\nsentencepiece==0.2.0\nplotext==5.2.8\nopenai==1.44.1\nasyncpg==0.29.0\nfastparquet==2024.5.0\nnanoid==2.0.0\naccelerate==0.34.2\ndocker==7.1.0\npython-dotenv==1.0.1\ntransformers==4.46.2\n"
          },
          {
            "fileName": "scripts",
            "fileContents": ""
          },
          {
            "fileName": "check_response.py",
            "fileContents": "import asyncio\nfrom enum import Enum\nfrom requests import post\n\n\nclass Endpoints(Enum):\n    CHAT = \"CHAT\"\n    COMPLETION = \"COMPLETION\"\n\n\nasync def check_tokens(\n    request, responses, uid, endpoint: Endpoints, port: int, url=\"http://localhost\"\n):\n    try:\n        result = post(\n            f\"{url}:{port}/verify\",\n            headers={\"Content-Type\": \"application/json\"},\n            json={\n                \"model\": request.get(\"model\"),\n                \"request_type\": endpoint.value,\n                \"request_params\": request,\n                \"output_sequence\": responses,\n            },\n        ).json()\n        if err := result.get(\"error\") is not None:\n            print(str(err))\n            return None\n        return result\n    except Exception as e:\n        print(f\"{uid}: \" + str(e))\n        return None\n\n\nasync def main():\n    request = {}\n    responses = []\n    uid = -1\n    endpoint = Endpoints.COMPLETION\n    port = 7777\n    res = await check_tokens(request, responses, uid, endpoint, port)\n    print(res)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"
          },
          {
            "fileName": "check_response_2.py",
            "fileContents": "import asyncio\nfrom enum import Enum\nfrom requests import post\n\n\nclass Endpoints(Enum):\n    CHAT = \"CHAT\"\n    COMPLETION = \"COMPLETION\"\n\n\nasync def check_tokens(\n    request, responses, uid, endpoint: Endpoints, port: int, url=\"http://localhost\"\n):\n    try:\n        result = post(\n            f\"{url}:{port}/verify\",\n            headers={\"Content-Type\": \"application/json\"},\n            json={\n                \"model\": request.get(\"model\"),\n                \"request_type\": endpoint.value,\n                \"request_params\": request,\n                \"output_sequence\": responses,\n            },\n        ).json()\n        if err := result.get(\"error\") is not None:\n            print(str(err))\n            return None\n        return result\n    except Exception as e:\n        print(f\"{uid}: \" + str(e))\n        return None\n\n\nasync def main():\n    request = {}\n    responses = []\n    uid = -1\n    endpoint = Endpoints.COMPLETION\n    port = 7777\n    res = await check_tokens(request, responses, uid, endpoint, port)\n    print(res)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"
          },
          {
            "fileName": "get_responses.py",
            "fileContents": "import json\nfrom neurons.validator import Validator\nfrom targon.dataset import download_dataset\nfrom targon.types import Endpoints\n\nminers = []\nmodel_name = \"NTQAI/Nxcode-CQ-7B-orpo\"\nendpoint = Endpoints.CHAT\n\nif __name__ == \"__main__\":\n    validator = Validator(run_init=False)\n    validator.dataset = download_dataset(True)\n    res = validator.loop.run_until_complete(\n        validator.query_miners(miners, model_name, endpoint)\n    )\n    with open(\"results.json\", \"w\") as file:\n        json.dump(\n            res,\n            file,\n        )\n        file.flush()\n    exit()\n"
          },
          {
            "fileName": "test_miner.py",
            "fileContents": "from typing import List\nfrom httpx import Timeout\nimport traceback\nimport httpx\nimport openai\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom neurons.validator import Validator\nfrom targon.epistula import generate_header\nfrom targon.protocol import Endpoints\n\n\nMINER_UID = -1\n\nmessages: List[ChatCompletionMessageParam] = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n        \"role\": \"user\",\n        \"content\": f\"What is the deffinition of the x y problem \",\n    },\n]\nmodel = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\nprompt = \"def print_hello_world():\"\n\n\ndef create_header_hook(hotkey, axon_hotkey):\n    def add_headers(request: httpx.Request):\n        for key, header in generate_header(hotkey, request.read(), axon_hotkey).items():\n            request.headers[key] = header\n\n    return add_headers\n\n\ndef main():\n    try:\n        validator = Validator(load_dataset=False)\n        axon_info = validator.metagraph.axons[MINER_UID]\n        miner = openai.OpenAI(\n            base_url=f\"http://{axon_info.ip}:{axon_info.port}/v1\",\n            api_key=\"sn4\",\n            max_retries=0,\n            timeout=Timeout(12, connect=5, read=5),\n            http_client=openai.DefaultHttpxClient(\n                event_hooks={\n                    \"request\": [\n                        create_header_hook(validator.wallet.hotkey, axon_info.hotkey)\n                    ]\n                }\n            ),\n        )\n        res = miner.chat.completions.create(\n            messages=messages, model=model, stream=True, logprobs=True, max_tokens=200\n        )\n        tokens = []\n        for chunk in res:\n            if chunk.choices[0].delta.content is None:\n                continue\n            choice = chunk.choices[0]\n            if choice.model_extra is None:\n                continue\n            token_ids = choice.model_extra.get(\"token_ids\") or []\n            token_id = token_ids[0] if len(token_ids) > 0 else -1\n            tokens.append(\n                (\n                    choice.delta.content or \"\",\n                    token_id,\n                )\n            )\n            print(choice.delta.content, token_id)\n        print(\n            validator.check_tokens({\"messages\": messages[:20]}, tokens, Endpoints.CHAT)\n        )\n        print(\n            validator.check_tokens({\"messages\": messages[:20]}, tokens, Endpoints.CHAT)\n        )\n        print(\n            validator.check_tokens({\"messages\": messages[:20]}, tokens, Endpoints.CHAT)\n        )\n        print(\n            validator.check_tokens({\"messages\": messages[:20]}, tokens, Endpoints.CHAT)\n        )\n        print(validator.check_tokens({\"messages\": messages}, tokens, Endpoints.CHAT))\n        print(validator.check_tokens({\"messages\": messages}, tokens, Endpoints.CHAT))\n        print(validator.check_tokens({\"messages\": messages}, tokens, Endpoints.CHAT))\n        print(validator.check_tokens({\"messages\": messages}, tokens, Endpoints.CHAT))\n        print(validator.check_tokens({\"messages\": messages}, tokens, Endpoints.CHAT))\n    except Exception as e:\n        print(e)\n        print(traceback.format_exc())\n\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          {
            "fileName": "view_weights.py",
            "fileContents": "from neurons.validator import Validator\nimport plotext as plt\n\nMINER_UIDS = []\n\nif __name__ == \"__main__\":\n    validator = Validator()\n    uids, weights = validator.get_weights()\n    weights = sorted(weights)\n    plt.scatter(weights)\n    plt.title(\"Weights\")  # to apply a title\n    plt.show()\n"
          },
          {
            "fileName": "setup.py",
            "fileContents": "from setuptools import setup, find_packages\n\n# Define the version directly here instead of importing\n__version__ = [line.strip() for line in open(\"VERSION\").readlines()][0]\n\nsetup(\n    name=\"targon\",\n    version=__version__,\n    author=\"Manifold Labs\",\n    author_email=\"devs@manifold.inc\",\n    description=\"The code for SN4 on bittensor\",\n    long_description_content_type=\"text/markdown\",\n    url=\"http://manifold.inc\",\n    packages=find_packages(),\n    install_requires=[line.strip() for line in open(\"requirements.txt\").readlines()],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.10\",\n)\n"
          },
          {
            "fileName": "targon",
            "fileContents": ""
          },
          {
            "fileName": "__init__.py",
            "fileContents": "from .config import *\nfrom .dataset import *\n\n__version__ = \"4.4.2\"\n\nversion_split = __version__.split(\".\")\n__spec_version__ = (\n    (100000 * int(version_split[0]))\n    + (1000 * int(version_split[1]))\n    + (10 * int(version_split[2]))\n)\n"
          },
          {
            "fileName": "cache.py",
            "fileContents": "import json\nimport traceback\nfrom typing import Any, Dict, List\nimport bittensor as bt\n\n\ndef load_cache(file_name: str, block: int, miners: List[int]):\n    miner_tps = {}\n    try:\n        with open(file_name, \"r\") as file:\n            loaded_data: Dict[str, Any] = json.load(file)\n            # Only load cache if fresh\n            if loaded_data.get(\"version\", 0) < 400000:\n                raise Exception(\"Cache file from older targon version\")\n            if loaded_data.get(\"block_saved\", 0) > block - 360:\n                miner_cache: Dict[str, Any] = loaded_data.get(\"miner_tps\", {})\n                miner_tps = dict([(int(k), v) for k, v in miner_cache.items()])\n    except IOError:\n        bt.logging.info(\"No cache file found\")\n    except EOFError:\n        bt.logging.warning(\"Curropted pickle file\")\n    except Exception as e:\n        bt.logging.error(f\"Failed reading cache file: {e}\")\n        bt.logging.error(traceback.format_exc())\n\n    for miner in miners:\n        if miner_tps.get(miner) is None:\n            miner_tps[miner] = {}\n    bt.logging.info(\"Loading cached data\")\n    return miner_tps\n"
          },
          {
            "fileName": "config.py",
            "fileContents": "# The MIT License (MIT)\n# Copyright © 2023 Yuma Rao\n# Copyright © 2023 Opentensor Foundation\n# Copyright © 2024 Manifold Labs\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the “Software”), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\n# the Software.\n\n# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\n# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nimport os\nimport bittensor as bt\n\nimport requests\nimport dotenv\n\n\ndef str2bool(v):\n    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n\n\ndotenv.load_dotenv()\nAUTO_UPDATE = not str2bool(os.getenv(\"NO_AUTO_UPDATE\", \"False\"))\nIMAGE_TAG = os.getenv(\"IMAGE_TAG\", \"latest\")\nHEARTBEAT = str2bool(os.getenv(\"HEARTBEAT\", \"False\"))\nIS_TESTNET = str2bool(os.getenv(\"IS_TESTNET\", \"False\"))\n\nSLIDING_WINDOW = 30\n\n\ndef validate_config_and_neuron_path(config):\n    r\"\"\"Checks/validates the config namespace object.\"\"\"\n    full_path = os.path.expanduser(\n        \"{}/{}/{}/netuid{}/{}\".format(\n            config.logging.logging_dir,\n            config.wallet.name,\n            config.wallet.hotkey,\n            config.netuid,\n            config.neuron.name,\n        )\n    )\n    bt.logging.info(f\"Logging path: {full_path}\")\n    config.neuron.full_path = os.path.expanduser(full_path)\n    if not os.path.exists(config.neuron.full_path):\n        os.makedirs(config.neuron.full_path, exist_ok=True)\n    return config\n\n\ndef add_args(parser):\n    \"\"\"\n    Adds relevant arguments to the parser for operation.\n    \"\"\"\n    # Netuid Arg: The netuid of the subnet to connect to.\n    parser.add_argument(\"--netuid\", type=int, help=\"Subnet netuid\", default=4)\n\n    parser.add_argument(\n        \"--neuron.name\",\n        type=str,\n        help=\"Neuron Name\",\n        default=\"targon\",\n    )\n\n    parser.add_argument(\n        \"--epoch-length\",\n        type=int,\n        dest=\"epoch_length\",\n        help=\"The default epoch length (how often we set weights, measured in 12 second blocks).\",\n        default=360,\n    )\n\n    parser.add_argument(\n        \"--mock\",\n        action=\"store_true\",\n        help=\"Run in mock mode\",\n        default=False,\n    )\n\n    parser.add_argument(\n        \"--autoupdate-off\",\n        action=\"store_false\",\n        dest=\"autoupdate\",\n        help=\"Disable automatic updates to Targon on latest version on Main.\",\n        default=True,\n    )\n\n\ndef add_miner_args(parser):\n    \"\"\"Add miner specific arguments to the parser.\"\"\"\n\n    parser.add_argument(\n        \"--model-endpoint\",\n        dest=\"model_endpoint\",\n        type=str,\n        help=\"The endpoint to use for the OpenAI Compatible client.\",\n        default=\"http://127.0.0.1:8000/v1\",\n    )\n\n    parser.add_argument(\n        \"--no-force-validator-permit\",\n        dest=\"no_force_validator_permit\",\n        action=\"store_true\",\n        help=\"If set, we will not force incoming requests to have a permit.\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--api-key\",\n        dest=\"api_key\",\n        type=str,\n        help=\"API key for openai compatable api\",\n        default=\"12345\",\n    )\n\n\ndef add_validator_args(parser):\n    \"\"\"Add validator specific arguments to the parser.\"\"\"\n\n    parser.add_argument(\n        \"--cache-file\",\n        dest=\"cache_file\",\n        type=str,\n        help=\"File to save scores, and other misc data that can persist through validator restarts\",\n        default=\"cache.json\",\n    )\n\n    parser.add_argument(\n        \"--miner-timeout\",\n        dest=\"miner_timeout\",\n        type=float,\n        help=\"The timeout for each forward call in seconds.\",\n        default=12,\n    )\n\n    parser.add_argument(\n        \"--vpermit-tao-limit\",\n        dest=\"vpermit_tao_limit\",\n        type=int,\n        help=\"The maximum number of TAO allowed to query a validator with a vpermit.\",\n        default=4096,\n    )\n\n    parser.add_argument(\n        \"--database.url\",\n        dest=\"database.url\",\n        type=str,\n        help=\"Database URL to score organic queries\",\n        default=None,\n    )\n\n    parser.add_argument(\n        \"--models.mode\",\n        dest=\"models.mode\",\n        type=str,\n        help=\"Which method to use when fetching models\",\n        choices=[\"endpoint\", \"config\", \"default\"],\n        default=\"default\",\n    )\n    parser.add_argument(\n        \"--models.endpoint\",\n        dest=\"models.endpoint\",\n        type=str,\n        help=\"Endpoint to query for models\",\n        default=\"https://targon.sybil.com/api/models\",\n    )\n\n\ndef get_models_from_endpoint(endpoint: str):\n    try:\n        res = requests.get(endpoint)\n        bt.logging.info(res.text)\n        res = res.json()\n        if not isinstance(res, list):\n            raise Exception(\n                f\"Unexpected type received from endpoint. Must be type list. got {res}\"\n            )\n        return res\n    except Exception as e:\n        bt.logging.error(f\"Failed to get models from {endpoint}: {str(e)}\")\n    return None\n\n\ndef get_models_from_config():\n    filename = \"./models.txt\"\n    try:\n        with open(filename, \"r\") as file:\n            models = file.read().strip().split(\"\\n\")\n            if not len(models):\n                bt.logging.error(\"No models in models file\")\n            else:\n                bt.logging.info(f\"Found models {str(models)}\")\n            return models\n    except IOError:\n        bt.logging.info(\"No model file found\")\n    except EOFError:\n        bt.logging.warning(\"Curropted models file\")\n    except Exception as e:\n        bt.logging.error(f\"Failed reading model file: {e}\")\n    return None\n"
          },
          {
            "fileName": "data",
            "fileContents": ""
          },
          {
            "fileName": "countries.txt",
            "fileContents": "Afghanistan\nAlbania\nAlgeria\nAndorra\nAngola\nAntigua and Barbuda\nArgentina\nArmenia\nAustralia\nAustria\nAzerbaijan\nThe Bahamas\nBahrain\nBangladesh\nBarbados\nBelarus\nBelgium\nBelize\nBenin\nBhutan\nBolivia\nBosnia and Herzegovina\nBotswana\nBrazil\nBrunei\nBulgaria\nBurkina Faso\nBurundi\nCabo Verde\nCambodia\nCameroon\nCanada\nCentral African Republic\nChad\nChile\nChina\nColombia\nComoros\nCongo, Democratic Republic of the\nCongo, Republic of the\nCosta Rica\nCôte d’Ivoire\nCroatia\nCuba\nCyprus\nCzech Republic\nDenmark\nDjibouti\nDominica\nDominican Republic\nEast Timor (Timor-Leste)\nEcuador\nEgypt\nEl Salvador\nEquatorial Guinea\nEritrea\nEstonia\nEswatini\nEthiopia\nFiji\nFinland\nFrance\nGabon\nThe Gambia\nGeorgia\nGermany\nGhana\nGreece\nGrenada\nGuatemala\nGuinea\nGuinea-Bissau\nGuyana\nHaiti\nHonduras\nHungary\nIceland\nIndia\nIndonesia\nIran\nIraq\nIreland\nIsrael\nItaly\nJamaica\nJapan\nJordan\nKazakhstan\nKenya\nKiribati\nKorea, North\nKorea, South\nKosovo\nKuwait\nKyrgyzstan\nLaos\nLatvia\nLebanon\nLesotho\nLiberia\nLibya\nLiechtenstein\nLithuania\nLuxembourg\nMadagascar\nMalawi\nMalaysia\nMaldives\nMali\nMalta\nMarshall Islands\nMauritania\nMauritius\nMexico\nMicronesia, Federated States of\nMoldova\nMonaco\nMongolia\nMontenegro\nMorocco\nMozambique\nMyanmar (Burma)\nNamibia\nNauru\nNepal\nNetherlands\nNew Zealand\nNicaragua\nNiger\nNigeria\nNorth Macedonia\nNorway\nOman\nPakistan\nPalau\nPanama\nPapua New Guinea\nParaguay\nPeru\nPhilippines\nPoland\nPortugal\nQatar\nRomania\nRussia\nRwanda\nSaint Kitts and Nevis\nSaint Lucia\nSaint Vincent and the Grenadines\nSamoa\nSan Marino\nSao Tome and Principe\nSaudi Arabia\nSenegal\nSerbia\nSeychelles\nSierra Leone\nSingapore\nSlovakia\nSlovenia\nSolomon Islands\nSomalia\nSouth Africa\nSpain\nSri Lanka\nSudan\nSudan, South\nSuriname\nSweden\nSwitzerland\nSyria\nTaiwan\nTajikistan\nTanzania\nThailand\nTogo\nTonga\nTrinidad and Tobago\nTunisia\nTurkey\nTurkmenistan\nTuvalu\nUganda\nUkraine\nUnited Arab Emirates\nUnited Kingdom\nUnited States\nUruguay\nUzbekistan\nVanuatu\nVatican City\nVenezuela\nVietnam\nYemen\nZambia\nZimbabwe\n"
          },
          {
            "fileName": "names.txt",
            "fileContents": "Aaren\nAarika\nAbagael\nAbagail\nAbbe\nAbbey\nAbbi\nAbbie\nAbby\nAbbye\nAbigael\nAbigail\nAbigale\nAbra\nAda\nAdah\nAdaline\nAdan\nAdara\nAdda\nAddi\nAddia\nAddie\nAddy\nAdel\nAdela\nAdelaida\nAdelaide\nAdele\nAdelheid\nAdelice\nAdelina\nAdelind\nAdeline\nAdella\nAdelle\nAdena\nAdey\nAdi\nAdiana\nAdina\nAdora\nAdore\nAdoree\nAdorne\nAdrea\nAdria\nAdriaens\nAdrian\nAdriana\nAdriane\nAdrianna\nAdrianne\nAdriena\nAdrienne\nAeriel\nAeriela\nAeriell\nAfton\nAg\nAgace\nAgata\nAgatha\nAgathe\nAggi\nAggie\nAggy\nAgna\nAgnella\nAgnes\nAgnese\nAgnesse\nAgneta\nAgnola\nAgretha\nAida\nAidan\nAigneis\nAila\nAile\nAilee\nAileen\nAilene\nAiley\nAili\nAilina\nAilis\nAilsun\nAilyn\nAime\nAimee\nAimil\nAindrea\nAinslee\nAinsley\nAinslie\nAjay\nAlaine\nAlameda\nAlana\nAlanah\nAlane\nAlanna\nAlayne\nAlberta\nAlbertina\nAlbertine\nAlbina\nAlecia\nAleda\nAleece\nAleen\nAlejandra\nAlejandrina\nAlena\nAlene\nAlessandra\nAleta\nAlethea\nAlex\nAlexa\nAlexandra\nAlexandrina\nAlexi\nAlexia\nAlexina\nAlexine\nAlexis\nAlfi\nAlfie\nAlfreda\nAlfy\nAli\nAlia\nAlica\nAlice\nAlicea\nAlicia\nAlida\nAlidia\nAlie\nAlika\nAlikee\nAlina\nAline\nAlis\nAlisa\nAlisha\nAlison\nAlissa\nAlisun\nAlix\nAliza\nAlla\nAlleen\nAllegra\nAllene\nAlli\nAllianora\nAllie\nAllina\nAllis\nAllison\nAllissa\nAllix\nAllsun\nAllx\nAlly\nAllyce\nAllyn\nAllys\nAllyson\nAlma\nAlmeda\nAlmeria\nAlmeta\nAlmira\nAlmire\nAloise\nAloisia\nAloysia\nAlta\nAlthea\nAlvera\nAlverta\nAlvina\nAlvinia\nAlvira\nAlyce\nAlyda\nAlys\nAlysa\nAlyse\nAlysia\nAlyson\nAlyss\nAlyssa\nAmabel\nAmabelle\nAmalea\nAmalee\nAmaleta\nAmalia\nAmalie\nAmalita\nAmalle\nAmanda\nAmandi\nAmandie\nAmandy\nAmara\nAmargo\nAmata\nAmber\nAmberly\nAmbur\nAme\nAmelia\nAmelie\nAmelina\nAmeline\nAmelita\nAmi\nAmie\nAmii\nAmil\nAmitie\nAmity\nAmmamaria\nAmy\nAmye\nAna\nAnabal\nAnabel\nAnabella\nAnabelle\nAnaliese\nAnalise\nAnallese\nAnallise\nAnastasia\nAnastasie\nAnastassia\nAnatola\nAndee\nAndeee\nAnderea\nAndi\nAndie\nAndra\nAndrea\nAndreana\nAndree\nAndrei\nAndria\nAndriana\nAndriette\nAndromache\nAndy\nAnestassia\nAnet\nAnett\nAnetta\nAnette\nAnge\nAngel\nAngela\nAngele\nAngelia\nAngelica\nAngelika\nAngelina\nAngeline\nAngelique\nAngelita\nAngelle\nAngie\nAngil\nAngy\nAnia\nAnica\nAnissa\nAnita\nAnitra\nAnjanette\nAnjela\nAnn\nAnn-Marie\nAnna\nAnna-Diana\nAnna-Diane\nAnna-Maria\nAnnabal\nAnnabel\nAnnabela\nAnnabell\nAnnabella\nAnnabelle\nAnnadiana\nAnnadiane\nAnnalee\nAnnaliese\nAnnalise\nAnnamaria\nAnnamarie\nAnne\nAnne-Corinne\nAnne-Marie\nAnnecorinne\nAnneliese\nAnnelise\nAnnemarie\nAnnetta\nAnnette\nAnni\nAnnice\nAnnie\nAnnis\nAnnissa\nAnnmaria\nAnnmarie\nAnnnora\nAnnora\nAnny\nAnselma\nAnsley\nAnstice\nAnthe\nAnthea\nAnthia\nAnthiathia\nAntoinette\nAntonella\nAntonetta\nAntonia\nAntonie\nAntonietta\nAntonina\nAnya\nAppolonia\nApril\nAprilette\nAra\nArabel\nArabela\nArabele\nArabella\nArabelle\nArda\nArdath\nArdeen\nArdelia\nArdelis\nArdella\nArdelle\nArden\nArdene\nArdenia\nArdine\nArdis\nArdisj\nArdith\nArdra\nArdyce\nArdys\nArdyth\nAretha\nAriadne\nAriana\nAridatha\nAriel\nAriela\nAriella\nArielle\nArlana\nArlee\nArleen\nArlen\nArlena\nArlene\nArleta\nArlette\nArleyne\nArlie\nArliene\nArlina\nArlinda\nArline\nArluene\nArly\nArlyn\nArlyne\nAryn\nAshely\nAshia\nAshien\nAshil\nAshla\nAshlan\nAshlee\nAshleigh\nAshlen\nAshley\nAshli\nAshlie\nAshly\nAsia\nAstra\nAstrid\nAstrix\nAtalanta\nAthena\nAthene\nAtlanta\nAtlante\nAuberta\nAubine\nAubree\nAubrette\nAubrey\nAubrie\nAubry\nAudi\nAudie\nAudra\nAudre\nAudrey\nAudrie\nAudry\nAudrye\nAudy\nAugusta\nAuguste\nAugustina\nAugustine\nAundrea\nAura\nAurea\nAurel\nAurelea\nAurelia\nAurelie\nAuria\nAurie\nAurilia\nAurlie\nAuroora\nAurora\nAurore\nAustin\nAustina\nAustine\nAva\nAveline\nAveril\nAveryl\nAvie\nAvis\nAviva\nAvivah\nAvril\nAvrit\nAyn\nBab\nBabara\nBabb\nBabbette\nBabbie\nBabette\nBabita\nBabs\nBambi\nBambie\nBamby\nBarb\nBarbabra\nBarbara\nBarbara-Anne\nBarbaraanne\nBarbe\nBarbee\nBarbette\nBarbey\nBarbi\nBarbie\nBarbra\nBarby\nBari\nBarrie\nBarry\nBasia\nBathsheba\nBatsheva\nBea\nBeatrice\nBeatrisa\nBeatrix\nBeatriz\nBebe\nBecca\nBecka\nBecki\nBeckie\nBecky\nBee\nBeilul\nBeitris\nBekki\nBel\nBelia\nBelicia\nBelinda\nBelita\nBell\nBella\nBellanca\nBelle\nBellina\nBelva\nBelvia\nBendite\nBenedetta\nBenedicta\nBenedikta\nBenetta\nBenita\nBenni\nBennie\nBenny\nBenoite\nBerenice\nBeret\nBerget\nBerna\nBernadene\nBernadette\nBernadina\nBernadine\nBernardina\nBernardine\nBernelle\nBernete\nBernetta\nBernette\nBerni\nBernice\nBernie\nBernita\nBerny\nBerri\nBerrie\nBerry\nBert\nBerta\nBerte\nBertha\nBerthe\nBerti\nBertie\nBertina\nBertine\nBerty\nBeryl\nBeryle\nBess\nBessie\nBessy\nBeth\nBethanne\nBethany\nBethena\nBethina\nBetsey\nBetsy\nBetta\nBette\nBette-Ann\nBetteann\nBetteanne\nBetti\nBettina\nBettine\nBetty\nBettye\nBeulah\nBev\nBeverie\nBeverlee\nBeverley\nBeverlie\nBeverly\nBevvy\nBianca\nBianka\nBibbie\nBibby\nBibbye\nBibi\nBiddie\nBiddy\nBidget\nBili\nBill\nBilli\nBillie\nBilly\nBillye\nBinni\nBinnie\nBinny\nBird\nBirdie\nBirgit\nBirgitta\nBlair\nBlaire\nBlake\nBlakelee\nBlakeley\nBlanca\nBlanch\nBlancha\nBlanche\nBlinni\nBlinnie\nBlinny\nBliss\nBlisse\nBlithe\nBlondell\nBlondelle\nBlondie\nBlondy\nBlythe\nBobbe\nBobbee\nBobbette\nBobbi\nBobbie\nBobby\nBobbye\nBobette\nBobina\nBobine\nBobinette\nBonita\nBonnee\nBonni\nBonnibelle\nBonnie\nBonny\nBrana\nBrandais\nBrande\nBrandea\nBrandi\nBrandice\nBrandie\nBrandise\nBrandy\nBreanne\nBrear\nBree\nBreena\nBren\nBrena\nBrenda\nBrenn\nBrenna\nBrett\nBria\nBriana\nBrianna\nBrianne\nBride\nBridget\nBridgette\nBridie\nBrier\nBrietta\nBrigid\nBrigida\nBrigit\nBrigitta\nBrigitte\nBrina\nBriney\nBrinn\nBrinna\nBriny\nBrit\nBrita\nBritney\nBritni\nBritt\nBritta\nBrittan\nBrittaney\nBrittani\nBrittany\nBritte\nBritteny\nBrittne\nBrittney\nBrittni\nBrook\nBrooke\nBrooks\nBrunhilda\nBrunhilde\nBryana\nBryn\nBryna\nBrynn\nBrynna\nBrynne\nBuffy\nBunni\nBunnie\nBunny\nCacilia\nCacilie\nCahra\nCairistiona\nCaitlin\nCaitrin\nCal\nCalida\nCalla\nCalley\nCalli\nCallida\nCallie\nCally\nCalypso\nCam\nCamala\nCamel\nCamella\nCamellia\nCami\nCamila\nCamile\nCamilla\nCamille\nCammi\nCammie\nCammy\nCandace\nCandi\nCandice\nCandida\nCandide\nCandie\nCandis\nCandra\nCandy\nCaprice\nCara\nCaralie\nCaren\nCarena\nCaresa\nCaressa\nCaresse\nCarey\nCari\nCaria\nCarie\nCaril\nCarilyn\nCarin\nCarina\nCarine\nCariotta\nCarissa\nCarita\nCaritta\nCarla\nCarlee\nCarleen\nCarlen\nCarlene\nCarley\nCarlie\nCarlin\nCarlina\nCarline\nCarlita\nCarlota\nCarlotta\nCarly\nCarlye\nCarlyn\nCarlynn\nCarlynne\nCarma\nCarmel\nCarmela\nCarmelia\nCarmelina\nCarmelita\nCarmella\nCarmelle\nCarmen\nCarmencita\nCarmina\nCarmine\nCarmita\nCarmon\nCaro\nCarol\nCarol-Jean\nCarola\nCarolan\nCarolann\nCarole\nCarolee\nCarolin\nCarolina\nCaroline\nCaroljean\nCarolyn\nCarolyne\nCarolynn\nCaron\nCarree\nCarri\nCarrie\nCarrissa\nCarroll\nCarry\nCary\nCaryl\nCaryn\nCasandra\nCasey\nCasi\nCasie\nCass\nCassandra\nCassandre\nCassandry\nCassaundra\nCassey\nCassi\nCassie\nCassondra\nCassy\nCatarina\nCate\nCaterina\nCatha\nCatharina\nCatharine\nCathe\nCathee\nCatherin\nCatherina\nCatherine\nCathi\nCathie\nCathleen\nCathlene\nCathrin\nCathrine\nCathryn\nCathy\nCathyleen\nCati\nCatie\nCatina\nCatlaina\nCatlee\nCatlin\nCatrina\nCatriona\nCaty\nCaye\nCayla\nCecelia\nCecil\nCecile\nCeciley\nCecilia\nCecilla\nCecily\nCeil\nCele\nCelene\nCelesta\nCeleste\nCelestia\nCelestina\nCelestine\nCelestyn\nCelestyna\nCelia\nCelie\nCelina\nCelinda\nCeline\nCelinka\nCelisse\nCelka\nCelle\nCesya\nChad\nChanda\nChandal\nChandra\nChanna\nChantal\nChantalle\nCharil\nCharin\nCharis\nCharissa\nCharisse\nCharita\nCharity\nCharla\nCharlean\nCharleen\nCharlena\nCharlene\nCharline\nCharlot\nCharlotta\nCharlotte\nCharmain\nCharmaine\nCharmane\nCharmian\nCharmine\nCharmion\nCharo\nCharyl\nChastity\nChelsae\nChelsea\nChelsey\nChelsie\nChelsy\nCher\nChere\nCherey\nCheri\nCherianne\nCherice\nCherida\nCherie\nCherilyn\nCherilynn\nCherin\nCherise\nCherish\nCherlyn\nCherri\nCherrita\nCherry\nChery\nCherye\nCheryl\nCheslie\nChiarra\nChickie\nChicky\nChiquia\nChiquita\nChlo\nChloe\nChloette\nChloris\nChris\nChrissie\nChrissy\nChrista\nChristabel\nChristabella\nChristal\nChristalle\nChristan\nChristean\nChristel\nChristen\nChristi\nChristian\nChristiana\nChristiane\nChristie\nChristin\nChristina\nChristine\nChristy\nChristye\nChristyna\nChrysa\nChrysler\nChrystal\nChryste\nChrystel\nCicely\nCicily\nCiel\nCilka\nCinda\nCindee\nCindelyn\nCinderella\nCindi\nCindie\nCindra\nCindy\nCinnamon\nCissiee\nCissy\nClair\nClaire\nClara\nClarabelle\nClare\nClaresta\nClareta\nClaretta\nClarette\nClarey\nClari\nClaribel\nClarice\nClarie\nClarinda\nClarine\nClarissa\nClarisse\nClarita\nClary\nClaude\nClaudelle\nClaudetta\nClaudette\nClaudia\nClaudie\nClaudina\nClaudine\nClea\nClem\nClemence\nClementia\nClementina\nClementine\nClemmie\nClemmy\nCleo\nCleopatra\nClerissa\nClio\nClo\nCloe\nCloris\nClotilda\nClovis\nCodee\nCodi\nCodie\nCody\nColeen\nColene\nColetta\nColette\nColleen\nCollen\nCollete\nCollette\nCollie\nColline\nColly\nCon\nConcettina\nConchita\nConcordia\nConni\nConnie\nConny\nConsolata\nConstance\nConstancia\nConstancy\nConstanta\nConstantia\nConstantina\nConstantine\nConsuela\nConsuelo\nCookie\nCora\nCorabel\nCorabella\nCorabelle\nCoral\nCoralie\nCoraline\nCoralyn\nCordelia\nCordelie\nCordey\nCordi\nCordie\nCordula\nCordy\nCoreen\nCorella\nCorenda\nCorene\nCoretta\nCorette\nCorey\nCori\nCorie\nCorilla\nCorina\nCorine\nCorinna\nCorinne\nCoriss\nCorissa\nCorliss\nCorly\nCornela\nCornelia\nCornelle\nCornie\nCorny\nCorrena\nCorrey\nCorri\nCorrianne\nCorrie\nCorrina\nCorrine\nCorrinne\nCorry\nCortney\nCory\nCosetta\nCosette\nCostanza\nCourtenay\nCourtnay\nCourtney\nCrin\nCris\nCrissie\nCrissy\nCrista\nCristabel\nCristal\nCristen\nCristi\nCristie\nCristin\nCristina\nCristine\nCristionna\nCristy\nCrysta\nCrystal\nCrystie\nCthrine\nCyb\nCybil\nCybill\nCymbre\nCynde\nCyndi\nCyndia\nCyndie\nCyndy\nCynthea\nCynthia\nCynthie\nCynthy\nDacey\nDacia\nDacie\nDacy\nDael\nDaffi\nDaffie\nDaffy\nDagmar\nDahlia\nDaile\nDaisey\nDaisi\nDaisie\nDaisy\nDale\nDalenna\nDalia\nDalila\nDallas\nDaloris\nDamara\nDamaris\nDamita\nDana\nDanell\nDanella\nDanette\nDani\nDania\nDanica\nDanice\nDaniela\nDaniele\nDaniella\nDanielle\nDanika\nDanila\nDanit\nDanita\nDanna\nDanni\nDannie\nDanny\nDannye\nDanya\nDanyelle\nDanyette\nDaphene\nDaphna\nDaphne\nDara\nDarb\nDarbie\nDarby\nDarcee\nDarcey\nDarci\nDarcie\nDarcy\nDarda\nDareen\nDarell\nDarelle\nDari\nDaria\nDarice\nDarla\nDarleen\nDarlene\nDarline\nDarlleen\nDaron\nDarrelle\nDarryl\nDarsey\nDarsie\nDarya\nDaryl\nDaryn\nDasha\nDasi\nDasie\nDasya\nDatha\nDaune\nDaveen\nDaveta\nDavida\nDavina\nDavine\nDavita\nDawn\nDawna\nDayle\nDayna\nDdene\nDe\nDeana\nDeane\nDeanna\nDeanne\nDeb\nDebbi\nDebbie\nDebby\nDebee\nDebera\nDebi\nDebor\nDebora\nDeborah\nDebra\nDede\nDedie\nDedra\nDee\nDee Dee\nDeeann\nDeeanne\nDeedee\nDeena\nDeerdre\nDeeyn\nDehlia\nDeidre\nDeina\nDeirdre\nDel\nDela\nDelcina\nDelcine\nDelia\nDelila\nDelilah\nDelinda\nDell\nDella\nDelly\nDelora\nDelores\nDeloria\nDeloris\nDelphine\nDelphinia\nDemeter\nDemetra\nDemetria\nDemetris\nDena\nDeni\nDenice\nDenise\nDenna\nDenni\nDennie\nDenny\nDeny\nDenys\nDenyse\nDeonne\nDesdemona\nDesirae\nDesiree\nDesiri\nDeva\nDevan\nDevi\nDevin\nDevina\nDevinne\nDevon\nDevondra\nDevonna\nDevonne\nDevora\nDi\nDiahann\nDian\nDiana\nDiandra\nDiane\nDiane-Marie\nDianemarie\nDiann\nDianna\nDianne\nDiannne\nDidi\nDido\nDiena\nDierdre\nDina\nDinah\nDinnie\nDinny\nDion\nDione\nDionis\nDionne\nDita\nDix\nDixie\nDniren\nDode\nDodi\nDodie\nDody\nDoe\nDoll\nDolley\nDolli\nDollie\nDolly\nDolores\nDolorita\nDoloritas\nDomeniga\nDominga\nDomini\nDominica\nDominique\nDona\nDonella\nDonelle\nDonetta\nDonia\nDonica\nDonielle\nDonna\nDonnamarie\nDonni\nDonnie\nDonny\nDora\nDoralia\nDoralin\nDoralyn\nDoralynn\nDoralynne\nDore\nDoreen\nDorelia\nDorella\nDorelle\nDorena\nDorene\nDoretta\nDorette\nDorey\nDori\nDoria\nDorian\nDorice\nDorie\nDorine\nDoris\nDorisa\nDorise\nDorita\nDoro\nDorolice\nDorolisa\nDorotea\nDoroteya\nDorothea\nDorothee\nDorothy\nDorree\nDorri\nDorrie\nDorris\nDorry\nDorthea\nDorthy\nDory\nDosi\nDot\nDoti\nDotti\nDottie\nDotty\nDre\nDreddy\nDredi\nDrona\nDru\nDruci\nDrucie\nDrucill\nDrucy\nDrusi\nDrusie\nDrusilla\nDrusy\nDulce\nDulcea\nDulci\nDulcia\nDulciana\nDulcie\nDulcine\nDulcinea\nDulcy\nDulsea\nDusty\nDyan\nDyana\nDyane\nDyann\nDyanna\nDyanne\nDyna\nDynah\nEachelle\nEada\nEadie\nEadith\nEalasaid\nEartha\nEaster\nEba\nEbba\nEbonee\nEbony\nEda\nEddi\nEddie\nEddy\nEde\nEdee\nEdeline\nEden\nEdi\nEdie\nEdin\nEdita\nEdith\nEditha\nEdithe\nEdiva\nEdna\nEdwina\nEdy\nEdyth\nEdythe\nEffie\nEileen\nEilis\nEimile\nEirena\nEkaterina\nElaina\nElaine\nElana\nElane\nElayne\nElberta\nElbertina\nElbertine\nEleanor\nEleanora\nEleanore\nElectra\nEleen\nElena\nElene\nEleni\nElenore\nEleonora\nEleonore\nElfie\nElfreda\nElfrida\nElfrieda\nElga\nElianora\nElianore\nElicia\nElie\nElinor\nElinore\nElisa\nElisabet\nElisabeth\nElisabetta\nElise\nElisha\nElissa\nElita\nEliza\nElizabet\nElizabeth\nElka\nElke\nElla\nElladine\nElle\nEllen\nEllene\nEllette\nElli\nEllie\nEllissa\nElly\nEllyn\nEllynn\nElmira\nElna\nElnora\nElnore\nEloisa\nEloise\nElonore\nElora\nElsa\nElsbeth\nElse\nElset\nElsey\nElsi\nElsie\nElsinore\nElspeth\nElsy\nElva\nElvera\nElvina\nElvira\nElwira\nElyn\nElyse\nElysee\nElysha\nElysia\nElyssa\nEm\nEma\nEmalee\nEmalia\nEmelda\nEmelia\nEmelina\nEmeline\nEmelita\nEmelyne\nEmera\nEmilee\nEmili\nEmilia\nEmilie\nEmiline\nEmily\nEmlyn\nEmlynn\nEmlynne\nEmma\nEmmalee\nEmmaline\nEmmalyn\nEmmalynn\nEmmalynne\nEmmeline\nEmmey\nEmmi\nEmmie\nEmmy\nEmmye\nEmogene\nEmyle\nEmylee\nEngracia\nEnid\nEnrica\nEnrichetta\nEnrika\nEnriqueta\nEolanda\nEolande\nEran\nErda\nErena\nErica\nEricha\nEricka\nErika\nErin\nErina\nErinn\nErinna\nErma\nErmengarde\nErmentrude\nErmina\nErminia\nErminie\nErna\nErnaline\nErnesta\nErnestine\nErtha\nEryn\nEsma\nEsmaria\nEsme\nEsmeralda\nEssa\nEssie\nEssy\nEsta\nEstel\nEstele\nEstell\nEstella\nEstelle\nEster\nEsther\nEstrella\nEstrellita\nEthel\nEthelda\nEthelin\nEthelind\nEtheline\nEthelyn\nEthyl\nEtta\nEtti\nEttie\nEtty\nEudora\nEugenia\nEugenie\nEugine\nEula\nEulalie\nEunice\nEuphemia\nEustacia\nEva\nEvaleen\nEvangelia\nEvangelin\nEvangelina\nEvangeline\nEvania\nEvanne\nEve\nEveleen\nEvelina\nEveline\nEvelyn\nEvey\nEvie\nEvita\nEvonne\nEvvie\nEvvy\nEvy\nEyde\nEydie\nEzmeralda\nFae\nFaina\nFaith\nFallon\nFan\nFanchette\nFanchon\nFancie\nFancy\nFanechka\nFania\nFanni\nFannie\nFanny\nFanya\nFara\nFarah\nFarand\nFarica\nFarra\nFarrah\nFarrand\nFaun\nFaunie\nFaustina\nFaustine\nFawn\nFawne\nFawnia\nFay\nFaydra\nFaye\nFayette\nFayina\nFayre\nFayth\nFaythe\nFederica\nFedora\nFelecia\nFelicdad\nFelice\nFelicia\nFelicity\nFelicle\nFelipa\nFelisha\nFelita\nFeliza\nFenelia\nFeodora\nFerdinanda\nFerdinande\nFern\nFernanda\nFernande\nFernandina\nFerne\nFey\nFiann\nFianna\nFidela\nFidelia\nFidelity\nFifi\nFifine\nFilia\nFilide\nFilippa\nFina\nFiona\nFionna\nFionnula\nFiorenze\nFleur\nFleurette\nFlo\nFlor\nFlora\nFlorance\nFlore\nFlorella\nFlorence\nFlorencia\nFlorentia\nFlorenza\nFlorette\nFlori\nFloria\nFlorida\nFlorie\nFlorina\nFlorinda\nFloris\nFlorri\nFlorrie\nFlorry\nFlory\nFlossi\nFlossie\nFlossy\nFlss\nFran\nFrancene\nFrances\nFrancesca\nFrancine\nFrancisca\nFranciska\nFrancoise\nFrancyne\nFrank\nFrankie\nFranky\nFranni\nFrannie\nFranny\nFrayda\nFred\nFreda\nFreddi\nFreddie\nFreddy\nFredelia\nFrederica\nFredericka\nFrederique\nFredi\nFredia\nFredra\nFredrika\nFreida\nFrieda\nFriederike\nFulvia\nGabbey\nGabbi\nGabbie\nGabey\nGabi\nGabie\nGabriel\nGabriela\nGabriell\nGabriella\nGabrielle\nGabriellia\nGabrila\nGaby\nGae\nGael\nGail\nGale\nGalina\nGarland\nGarnet\nGarnette\nGates\nGavra\nGavrielle\nGay\nGaye\nGayel\nGayla\nGayle\nGayleen\nGaylene\nGaynor\nGelya\nGena\nGene\nGeneva\nGenevieve\nGenevra\nGenia\nGenna\nGenni\nGennie\nGennifer\nGenny\nGenovera\nGenvieve\nGeorge\nGeorgeanna\nGeorgeanne\nGeorgena\nGeorgeta\nGeorgetta\nGeorgette\nGeorgia\nGeorgiana\nGeorgianna\nGeorgianne\nGeorgie\nGeorgina\nGeorgine\nGeralda\nGeraldine\nGerda\nGerhardine\nGeri\nGerianna\nGerianne\nGerladina\nGermain\nGermaine\nGermana\nGerri\nGerrie\nGerrilee\nGerry\nGert\nGerta\nGerti\nGertie\nGertrud\nGertruda\nGertrude\nGertrudis\nGerty\nGiacinta\nGiana\nGianina\nGianna\nGigi\nGilberta\nGilberte\nGilbertina\nGilbertine\nGilda\nGilemette\nGill\nGillan\nGilli\nGillian\nGillie\nGilligan\nGilly\nGina\nGinelle\nGinevra\nGinger\nGinni\nGinnie\nGinnifer\nGinny\nGiorgia\nGiovanna\nGipsy\nGiralda\nGisela\nGisele\nGisella\nGiselle\nGiuditta\nGiulia\nGiulietta\nGiustina\nGizela\nGlad\nGladi\nGladys\nGleda\nGlen\nGlenda\nGlenine\nGlenn\nGlenna\nGlennie\nGlennis\nGlori\nGloria\nGloriana\nGloriane\nGlory\nGlyn\nGlynda\nGlynis\nGlynnis\nGnni\nGodiva\nGolda\nGoldarina\nGoldi\nGoldia\nGoldie\nGoldina\nGoldy\nGrace\nGracia\nGracie\nGrata\nGratia\nGratiana\nGray\nGrayce\nGrazia\nGreer\nGreta\nGretal\nGretchen\nGrete\nGretel\nGrethel\nGretna\nGretta\nGrier\nGriselda\nGrissel\nGuendolen\nGuenevere\nGuenna\nGuglielma\nGui\nGuillema\nGuillemette\nGuinevere\nGuinna\nGunilla\nGus\nGusella\nGussi\nGussie\nGussy\nGusta\nGusti\nGustie\nGusty\nGwen\nGwendolen\nGwendolin\nGwendolyn\nGweneth\nGwenette\nGwenneth\nGwenni\nGwennie\nGwenny\nGwenora\nGwenore\nGwyn\nGwyneth\nGwynne\nGypsy\nHadria\nHailee\nHaily\nHaleigh\nHalette\nHaley\nHali\nHalie\nHalimeda\nHalley\nHalli\nHallie\nHally\nHana\nHanna\nHannah\nHanni\nHannie\nHannis\nHanny\nHappy\nHarlene\nHarley\nHarli\nHarlie\nHarmonia\nHarmonie\nHarmony\nHarri\nHarrie\nHarriet\nHarriett\nHarrietta\nHarriette\nHarriot\nHarriott\nHatti\nHattie\nHatty\nHayley\nHazel\nHeath\nHeather\nHeda\nHedda\nHeddi\nHeddie\nHedi\nHedvig\nHedvige\nHedwig\nHedwiga\nHedy\nHeida\nHeidi\nHeidie\nHelaina\nHelaine\nHelen\nHelen-Elizabeth\nHelena\nHelene\nHelenka\nHelga\nHelge\nHelli\nHeloise\nHelsa\nHelyn\nHendrika\nHenka\nHenrie\nHenrieta\nHenrietta\nHenriette\nHenryetta\nHephzibah\nHermia\nHermina\nHermine\nHerminia\nHermione\nHerta\nHertha\nHester\nHesther\nHestia\nHetti\nHettie\nHetty\nHilary\nHilda\nHildagard\nHildagarde\nHilde\nHildegaard\nHildegarde\nHildy\nHillary\nHilliary\nHinda\nHolli\nHollie\nHolly\nHolly-Anne\nHollyanne\nHoney\nHonor\nHonoria\nHope\nHoratia\nHortense\nHortensia\nHulda\nHyacinth\nHyacintha\nHyacinthe\nHyacinthia\nHyacinthie\nHynda\nIanthe\nIbbie\nIbby\nIda\nIdalia\nIdalina\nIdaline\nIdell\nIdelle\nIdette\nIleana\nIleane\nIlene\nIlise\nIlka\nIlla\nIlsa\nIlse\nIlysa\nIlyse\nIlyssa\nImelda\nImogen\nImogene\nImojean\nIna\nIndira\nInes\nInesita\nInessa\nInez\nInga\nIngaberg\nIngaborg\nInge\nIngeberg\nIngeborg\nInger\nIngrid\nIngunna\nInna\nIolande\nIolanthe\nIona\nIormina\nIra\nIrena\nIrene\nIrina\nIris\nIrita\nIrma\nIsa\nIsabel\nIsabelita\nIsabella\nIsabelle\nIsadora\nIsahella\nIseabal\nIsidora\nIsis\nIsobel\nIssi\nIssie\nIssy\nIvett\nIvette\nIvie\nIvonne\nIvory\nIvy\nIzabel\nJacenta\nJacinda\nJacinta\nJacintha\nJacinthe\nJackelyn\nJacki\nJackie\nJacklin\nJacklyn\nJackquelin\nJackqueline\nJacky\nJaclin\nJaclyn\nJacquelin\nJacqueline\nJacquelyn\nJacquelynn\nJacquenetta\nJacquenette\nJacquetta\nJacquette\nJacqui\nJacquie\nJacynth\nJada\nJade\nJaime\nJaimie\nJaine\nJami\nJamie\nJamima\nJammie\nJan\nJana\nJanaya\nJanaye\nJandy\nJane\nJanean\nJaneczka\nJaneen\nJanel\nJanela\nJanella\nJanelle\nJanene\nJanenna\nJanessa\nJanet\nJaneta\nJanetta\nJanette\nJaneva\nJaney\nJania\nJanice\nJanie\nJanifer\nJanina\nJanine\nJanis\nJanith\nJanka\nJanna\nJannel\nJannelle\nJanot\nJany\nJaquelin\nJaquelyn\nJaquenetta\nJaquenette\nJaquith\nJasmin\nJasmina\nJasmine\nJayme\nJaymee\nJayne\nJaynell\nJazmin\nJean\nJeana\nJeane\nJeanelle\nJeanette\nJeanie\nJeanine\nJeanna\nJeanne\nJeannette\nJeannie\nJeannine\nJehanna\nJelene\nJemie\nJemima\nJemimah\nJemmie\nJemmy\nJen\nJena\nJenda\nJenelle\nJeni\nJenica\nJeniece\nJenifer\nJeniffer\nJenilee\nJenine\nJenn\nJenna\nJennee\nJennette\nJenni\nJennica\nJennie\nJennifer\nJennilee\nJennine\nJenny\nJeralee\nJere\nJeri\nJermaine\nJerrie\nJerrilee\nJerrilyn\nJerrine\nJerry\nJerrylee\nJess\nJessa\nJessalin\nJessalyn\nJessamine\nJessamyn\nJesse\nJesselyn\nJessi\nJessica\nJessie\nJessika\nJessy\nJewel\nJewell\nJewelle\nJill\nJillana\nJillane\nJillayne\nJilleen\nJillene\nJilli\nJillian\nJillie\nJilly\nJinny\nJo\nJo Ann\nJo-Ann\nJo-Anne\nJoan\nJoana\nJoane\nJoanie\nJoann\nJoanna\nJoanne\nJoannes\nJobey\nJobi\nJobie\nJobina\nJoby\nJobye\nJobyna\nJocelin\nJoceline\nJocelyn\nJocelyne\nJodee\nJodi\nJodie\nJody\nJoeann\nJoela\nJoelie\nJoell\nJoella\nJoelle\nJoellen\nJoelly\nJoellyn\nJoelynn\nJoete\nJoey\nJohanna\nJohannah\nJohna\nJohnath\nJohnette\nJohnna\nJoice\nJojo\nJolee\nJoleen\nJolene\nJoletta\nJoli\nJolie\nJoline\nJoly\nJolyn\nJolynn\nJonell\nJoni\nJonie\nJonis\nJordain\nJordan\nJordana\nJordanna\nJorey\nJori\nJorie\nJorrie\nJorry\nJoscelin\nJosee\nJosefa\nJosefina\nJosepha\nJosephina\nJosephine\nJosey\nJosi\nJosie\nJosselyn\nJosy\nJourdan\nJoy\nJoya\nJoyan\nJoyann\nJoyce\nJoycelin\nJoye\nJsandye\nJuana\nJuanita\nJudi\nJudie\nJudith\nJuditha\nJudy\nJudye\nJuieta\nJulee\nJuli\nJulia\nJuliana\nJuliane\nJuliann\nJulianna\nJulianne\nJulie\nJulienne\nJuliet\nJulieta\nJulietta\nJuliette\nJulina\nJuline\nJulissa\nJulita\nJune\nJunette\nJunia\nJunie\nJunina\nJustina\nJustine\nJustinn\nJyoti\nKacey\nKacie\nKacy\nKaela\nKai\nKaia\nKaila\nKaile\nKailey\nKaitlin\nKaitlyn\nKaitlynn\nKaja\nKakalina\nKala\nKaleena\nKali\nKalie\nKalila\nKalina\nKalinda\nKalindi\nKalli\nKally\nKameko\nKamila\nKamilah\nKamillah\nKandace\nKandy\nKania\nKanya\nKara\nKara-Lynn\nKaralee\nKaralynn\nKare\nKaree\nKarel\nKaren\nKarena\nKari\nKaria\nKarie\nKaril\nKarilynn\nKarin\nKarina\nKarine\nKariotta\nKarisa\nKarissa\nKarita\nKarla\nKarlee\nKarleen\nKarlen\nKarlene\nKarlie\nKarlotta\nKarlotte\nKarly\nKarlyn\nKarmen\nKarna\nKarol\nKarola\nKarole\nKarolina\nKaroline\nKaroly\nKaron\nKarrah\nKarrie\nKarry\nKary\nKaryl\nKarylin\nKaryn\nKasey\nKass\nKassandra\nKassey\nKassi\nKassia\nKassie\nKat\nKata\nKatalin\nKate\nKatee\nKaterina\nKaterine\nKatey\nKath\nKatha\nKatharina\nKatharine\nKatharyn\nKathe\nKatherina\nKatherine\nKatheryn\nKathi\nKathie\nKathleen\nKathlin\nKathrine\nKathryn\nKathryne\nKathy\nKathye\nKati\nKatie\nKatina\nKatine\nKatinka\nKatleen\nKatlin\nKatrina\nKatrine\nKatrinka\nKatti\nKattie\nKatuscha\nKatusha\nKaty\nKatya\nKay\nKaycee\nKaye\nKayla\nKayle\nKaylee\nKayley\nKaylil\nKaylyn\nKeeley\nKeelia\nKeely\nKelcey\nKelci\nKelcie\nKelcy\nKelila\nKellen\nKelley\nKelli\nKellia\nKellie\nKellina\nKellsie\nKelly\nKellyann\nKelsey\nKelsi\nKelsy\nKendra\nKendre\nKenna\nKeri\nKeriann\nKerianne\nKerri\nKerrie\nKerrill\nKerrin\nKerry\nKerstin\nKesley\nKeslie\nKessia\nKessiah\nKetti\nKettie\nKetty\nKevina\nKevyn\nKi\nKiah\nKial\nKiele\nKiersten\nKikelia\nKiley\nKim\nKimberlee\nKimberley\nKimberli\nKimberly\nKimberlyn\nKimbra\nKimmi\nKimmie\nKimmy\nKinna\nKip\nKipp\nKippie\nKippy\nKira\nKirbee\nKirbie\nKirby\nKiri\nKirsten\nKirsteni\nKirsti\nKirstin\nKirstyn\nKissee\nKissiah\nKissie\nKit\nKitti\nKittie\nKitty\nKizzee\nKizzie\nKlara\nKlarika\nKlarrisa\nKonstance\nKonstanze\nKoo\nKora\nKoral\nKoralle\nKordula\nKore\nKorella\nKoren\nKoressa\nKori\nKorie\nKorney\nKorrie\nKorry\nKris\nKrissie\nKrissy\nKrista\nKristal\nKristan\nKriste\nKristel\nKristen\nKristi\nKristien\nKristin\nKristina\nKristine\nKristy\nKristyn\nKrysta\nKrystal\nKrystalle\nKrystle\nKrystyna\nKyla\nKyle\nKylen\nKylie\nKylila\nKylynn\nKym\nKynthia\nKyrstin\nLa Verne\nLacee\nLacey\nLacie\nLacy\nLadonna\nLaetitia\nLaina\nLainey\nLana\nLanae\nLane\nLanette\nLaney\nLani\nLanie\nLanita\nLanna\nLanni\nLanny\nLara\nLaraine\nLari\nLarina\nLarine\nLarisa\nLarissa\nLark\nLaryssa\nLatashia\nLatia\nLatisha\nLatrena\nLatrina\nLaura\nLauraine\nLaural\nLauralee\nLaure\nLauree\nLaureen\nLaurel\nLaurella\nLauren\nLaurena\nLaurene\nLauretta\nLaurette\nLauri\nLaurianne\nLaurice\nLaurie\nLauryn\nLavena\nLaverna\nLaverne\nLavina\nLavinia\nLavinie\nLayla\nLayne\nLayney\nLea\nLeah\nLeandra\nLeann\nLeanna\nLeanor\nLeanora\nLebbie\nLeda\nLee\nLeeann\nLeeanne\nLeela\nLeelah\nLeena\nLeesa\nLeese\nLegra\nLeia\nLeigh\nLeigha\nLeila\nLeilah\nLeisha\nLela\nLelah\nLeland\nLelia\nLena\nLenee\nLenette\nLenka\nLenna\nLenora\nLenore\nLeodora\nLeoine\nLeola\nLeoline\nLeona\nLeonanie\nLeone\nLeonelle\nLeonie\nLeonora\nLeonore\nLeontine\nLeontyne\nLeora\nLeshia\nLesley\nLesli\nLeslie\nLesly\nLesya\nLeta\nLethia\nLeticia\nLetisha\nLetitia\nLetizia\nLetta\nLetti\nLettie\nLetty\nLexi\nLexie\nLexine\nLexis\nLexy\nLeyla\nLezlie\nLia\nLian\nLiana\nLiane\nLianna\nLianne\nLib\nLibbey\nLibbi\nLibbie\nLibby\nLicha\nLida\nLidia\nLiesa\nLil\nLila\nLilah\nLilas\nLilia\nLilian\nLiliane\nLilias\nLilith\nLilla\nLilli\nLillian\nLillis\nLilllie\nLilly\nLily\nLilyan\nLin\nLina\nLind\nLinda\nLindi\nLindie\nLindsay\nLindsey\nLindsy\nLindy\nLinea\nLinell\nLinet\nLinette\nLinn\nLinnea\nLinnell\nLinnet\nLinnie\nLinzy\nLira\nLisa\nLisabeth\nLisbeth\nLise\nLisetta\nLisette\nLisha\nLishe\nLissa\nLissi\nLissie\nLissy\nLita\nLiuka\nLiv\nLiva\nLivia\nLivvie\nLivvy\nLivvyy\nLivy\nLiz\nLiza\nLizabeth\nLizbeth\nLizette\nLizzie\nLizzy\nLoella\nLois\nLoise\nLola\nLoleta\nLolita\nLolly\nLona\nLonee\nLoni\nLonna\nLonni\nLonnie\nLora\nLorain\nLoraine\nLoralee\nLoralie\nLoralyn\nLoree\nLoreen\nLorelei\nLorelle\nLoren\nLorena\nLorene\nLorenza\nLoretta\nLorette\nLori\nLoria\nLorianna\nLorianne\nLorie\nLorilee\nLorilyn\nLorinda\nLorine\nLorita\nLorna\nLorne\nLorraine\nLorrayne\nLorri\nLorrie\nLorrin\nLorry\nLory\nLotta\nLotte\nLotti\nLottie\nLotty\nLou\nLouella\nLouisa\nLouise\nLouisette\nLoutitia\nLu\nLuce\nLuci\nLucia\nLuciana\nLucie\nLucienne\nLucila\nLucilia\nLucille\nLucina\nLucinda\nLucine\nLucita\nLucky\nLucretia\nLucy\nLudovika\nLuella\nLuelle\nLuisa\nLuise\nLula\nLulita\nLulu\nLura\nLurette\nLurleen\nLurlene\nLurline\nLusa\nLuz\nLyda\nLydia\nLydie\nLyn\nLynda\nLynde\nLyndel\nLyndell\nLyndsay\nLyndsey\nLyndsie\nLyndy\nLynea\nLynelle\nLynett\nLynette\nLynn\nLynna\nLynne\nLynnea\nLynnell\nLynnelle\nLynnet\nLynnett\nLynnette\nLynsey\nLyssa\nMab\nMabel\nMabelle\nMable\nMada\nMadalena\nMadalyn\nMaddalena\nMaddi\nMaddie\nMaddy\nMadel\nMadelaine\nMadeleine\nMadelena\nMadelene\nMadelin\nMadelina\nMadeline\nMadella\nMadelle\nMadelon\nMadelyn\nMadge\nMadlen\nMadlin\nMadonna\nMady\nMae\nMaegan\nMag\nMagda\nMagdaia\nMagdalen\nMagdalena\nMagdalene\nMaggee\nMaggi\nMaggie\nMaggy\nMahala\nMahalia\nMaia\nMaible\nMaiga\nMaighdiln\nMair\nMaire\nMaisey\nMaisie\nMaitilde\nMala\nMalanie\nMalena\nMalia\nMalina\nMalinda\nMalinde\nMalissa\nMalissia\nMallissa\nMallorie\nMallory\nMalorie\nMalory\nMalva\nMalvina\nMalynda\nMame\nMamie\nManda\nMandi\nMandie\nMandy\nManon\nManya\nMara\nMarabel\nMarcela\nMarcelia\nMarcella\nMarcelle\nMarcellina\nMarcelline\nMarchelle\nMarci\nMarcia\nMarcie\nMarcile\nMarcille\nMarcy\nMareah\nMaren\nMarena\nMaressa\nMarga\nMargalit\nMargalo\nMargaret\nMargareta\nMargarete\nMargaretha\nMargarethe\nMargaretta\nMargarette\nMargarita\nMargaux\nMarge\nMargeaux\nMargery\nMarget\nMargette\nMargi\nMargie\nMargit\nMargo\nMargot\nMargret\nMarguerite\nMargy\nMari\nMaria\nMariam\nMarian\nMariana\nMariann\nMarianna\nMarianne\nMaribel\nMaribelle\nMaribeth\nMarice\nMaridel\nMarie\nMarie-Ann\nMarie-Jeanne\nMarieann\nMariejeanne\nMariel\nMariele\nMarielle\nMariellen\nMarietta\nMariette\nMarigold\nMarijo\nMarika\nMarilee\nMarilin\nMarillin\nMarilyn\nMarin\nMarina\nMarinna\nMarion\nMariquilla\nMaris\nMarisa\nMariska\nMarissa\nMarita\nMaritsa\nMariya\nMarj\nMarja\nMarje\nMarji\nMarjie\nMarjorie\nMarjory\nMarjy\nMarketa\nMarla\nMarlane\nMarleah\nMarlee\nMarleen\nMarlena\nMarlene\nMarley\nMarlie\nMarline\nMarlo\nMarlyn\nMarna\nMarne\nMarney\nMarni\nMarnia\nMarnie\nMarquita\nMarrilee\nMarris\nMarrissa\nMarsha\nMarsiella\nMarta\nMartelle\nMartguerita\nMartha\nMarthe\nMarthena\nMarti\nMartica\nMartie\nMartina\nMartita\nMarty\nMartynne\nMary\nMarya\nMaryann\nMaryanna\nMaryanne\nMarybelle\nMarybeth\nMaryellen\nMaryjane\nMaryjo\nMaryl\nMarylee\nMarylin\nMarylinda\nMarylou\nMarylynne\nMaryrose\nMarys\nMarysa\nMasha\nMatelda\nMathilda\nMathilde\nMatilda\nMatilde\nMatti\nMattie\nMatty\nMaud\nMaude\nMaudie\nMaura\nMaure\nMaureen\nMaureene\nMaurene\nMaurine\nMaurise\nMaurita\nMaurizia\nMavis\nMavra\nMax\nMaxi\nMaxie\nMaxine\nMaxy\nMay\nMaybelle\nMaye\nMead\nMeade\nMeagan\nMeaghan\nMeara\nMechelle\nMeg\nMegan\nMegen\nMeggi\nMeggie\nMeggy\nMeghan\nMeghann\nMehetabel\nMei\nMel\nMela\nMelamie\nMelania\nMelanie\nMelantha\nMelany\nMelba\nMelesa\nMelessa\nMelicent\nMelina\nMelinda\nMelinde\nMelisa\nMelisande\nMelisandra\nMelisenda\nMelisent\nMelissa\nMelisse\nMelita\nMelitta\nMella\nMelli\nMellicent\nMellie\nMellisa\nMellisent\nMelloney\nMelly\nMelodee\nMelodie\nMelody\nMelonie\nMelony\nMelosa\nMelva\nMercedes\nMerci\nMercie\nMercy\nMeredith\nMeredithe\nMeridel\nMeridith\nMeriel\nMerilee\nMerilyn\nMeris\nMerissa\nMerl\nMerla\nMerle\nMerlina\nMerline\nMerna\nMerola\nMerralee\nMerridie\nMerrie\nMerrielle\nMerrile\nMerrilee\nMerrili\nMerrill\nMerrily\nMerry\nMersey\nMeryl\nMeta\nMia\nMicaela\nMichaela\nMichaelina\nMichaeline\nMichaella\nMichal\nMichel\nMichele\nMichelina\nMicheline\nMichell\nMichelle\nMicki\nMickie\nMicky\nMidge\nMignon\nMignonne\nMiguela\nMiguelita\nMikaela\nMil\nMildred\nMildrid\nMilena\nMilicent\nMilissent\nMilka\nMilli\nMillicent\nMillie\nMillisent\nMilly\nMilzie\nMimi\nMin\nMina\nMinda\nMindy\nMinerva\nMinetta\nMinette\nMinna\nMinnaminnie\nMinne\nMinni\nMinnie\nMinnnie\nMinny\nMinta\nMiof Mela\nMiquela\nMira\nMirabel\nMirabella\nMirabelle\nMiran\nMiranda\nMireielle\nMireille\nMirella\nMirelle\nMiriam\nMirilla\nMirna\nMisha\nMissie\nMissy\nMisti\nMisty\nMitzi\nModesta\nModestia\nModestine\nModesty\nMoina\nMoira\nMoll\nMollee\nMolli\nMollie\nMolly\nMommy\nMona\nMonah\nMonica\nMonika\nMonique\nMora\nMoreen\nMorena\nMorgan\nMorgana\nMorganica\nMorganne\nMorgen\nMoria\nMorissa\nMorna\nMoselle\nMoyna\nMoyra\nMozelle\nMuffin\nMufi\nMufinella\nMuire\nMureil\nMurial\nMuriel\nMurielle\nMyra\nMyrah\nMyranda\nMyriam\nMyrilla\nMyrle\nMyrlene\nMyrna\nMyrta\nMyrtia\nMyrtice\nMyrtie\nMyrtle\nNada\nNadean\nNadeen\nNadia\nNadine\nNadiya\nNady\nNadya\nNalani\nNan\nNana\nNananne\nNance\nNancee\nNancey\nNanci\nNancie\nNancy\nNanete\nNanette\nNani\nNanice\nNanine\nNannette\nNanni\nNannie\nNanny\nNanon\nNaoma\nNaomi\nNara\nNari\nNariko\nNat\nNata\nNatala\nNatalee\nNatalie\nNatalina\nNataline\nNatalya\nNatasha\nNatassia\nNathalia\nNathalie\nNatividad\nNatka\nNatty\nNeala\nNeda\nNedda\nNedi\nNeely\nNeila\nNeile\nNeilla\nNeille\nNelia\nNelie\nNell\nNelle\nNelli\nNellie\nNelly\nNerissa\nNerita\nNert\nNerta\nNerte\nNerti\nNertie\nNerty\nNessa\nNessi\nNessie\nNessy\nNesta\nNetta\nNetti\nNettie\nNettle\nNetty\nNevsa\nNeysa\nNichol\nNichole\nNicholle\nNicki\nNickie\nNicky\nNicol\nNicola\nNicole\nNicolea\nNicolette\nNicoli\nNicolina\nNicoline\nNicolle\nNikaniki\nNike\nNiki\nNikki\nNikkie\nNikoletta\nNikolia\nNina\nNinetta\nNinette\nNinnetta\nNinnette\nNinon\nNissa\nNisse\nNissie\nNissy\nNita\nNixie\nNoami\nNoel\nNoelani\nNoell\nNoella\nNoelle\nNoellyn\nNoelyn\nNoemi\nNola\nNolana\nNolie\nNollie\nNomi\nNona\nNonah\nNoni\nNonie\nNonna\nNonnah\nNora\nNorah\nNorean\nNoreen\nNorene\nNorina\nNorine\nNorma\nNorri\nNorrie\nNorry\nNovelia\nNydia\nNyssa\nOctavia\nOdele\nOdelia\nOdelinda\nOdella\nOdelle\nOdessa\nOdetta\nOdette\nOdilia\nOdille\nOfelia\nOfella\nOfilia\nOla\nOlenka\nOlga\nOlia\nOlimpia\nOlive\nOlivette\nOlivia\nOlivie\nOliy\nOllie\nOlly\nOlva\nOlwen\nOlympe\nOlympia\nOlympie\nOndrea\nOneida\nOnida\nOona\nOpal\nOpalina\nOpaline\nOphelia\nOphelie\nOra\nOralee\nOralia\nOralie\nOralla\nOralle\nOrel\nOrelee\nOrelia\nOrelie\nOrella\nOrelle\nOriana\nOrly\nOrsa\nOrsola\nOrtensia\nOtha\nOthelia\nOthella\nOthilia\nOthilie\nOttilie\nPage\nPaige\nPaloma\nPam\nPamela\nPamelina\nPamella\nPammi\nPammie\nPammy\nPandora\nPansie\nPansy\nPaola\nPaolina\nPapagena\nPat\nPatience\nPatrica\nPatrice\nPatricia\nPatrizia\nPatsy\nPatti\nPattie\nPatty\nPaula\nPaule\nPauletta\nPaulette\nPauli\nPaulie\nPaulina\nPauline\nPaulita\nPauly\nPavia\nPavla\nPearl\nPearla\nPearle\nPearline\nPeg\nPegeen\nPeggi\nPeggie\nPeggy\nPen\nPenelopa\nPenelope\nPenni\nPennie\nPenny\nPepi\nPepita\nPeri\nPeria\nPerl\nPerla\nPerle\nPerri\nPerrine\nPerry\nPersis\nPet\nPeta\nPetra\nPetrina\nPetronella\nPetronia\nPetronilla\nPetronille\nPetunia\nPhaedra\nPhaidra\nPhebe\nPhedra\nPhelia\nPhil\nPhilipa\nPhilippa\nPhilippe\nPhilippine\nPhilis\nPhillida\nPhillie\nPhillis\nPhilly\nPhilomena\nPhoebe\nPhylis\nPhyllida\nPhyllis\nPhyllys\nPhylys\nPia\nPier\nPierette\nPierrette\nPietra\nPiper\nPippa\nPippy\nPolly\nPollyanna\nPooh\nPoppy\nPortia\nPris\nPrisca\nPriscella\nPriscilla\nPrissie\nPru\nPrudence\nPrudi\nPrudy\nPrue\nQueenie\nQuentin\nQuerida\nQuinn\nQuinta\nQuintana\nQuintilla\nQuintina\nRachael\nRachel\nRachele\nRachelle\nRae\nRaeann\nRaf\nRafa\nRafaela\nRafaelia\nRafaelita\nRahal\nRahel\nRaina\nRaine\nRakel\nRalina\nRamona\nRamonda\nRana\nRanda\nRandee\nRandene\nRandi\nRandie\nRandy\nRanee\nRani\nRania\nRanice\nRanique\nRanna\nRaphaela\nRaquel\nRaquela\nRasia\nRasla\nRaven\nRay\nRaychel\nRaye\nRayna\nRaynell\nRayshell\nRea\nReba\nRebbecca\nRebe\nRebeca\nRebecca\nRebecka\nRebeka\nRebekah\nRebekkah\nRee\nReeba\nReena\nReeta\nReeva\nRegan\nReggi\nReggie\nRegina\nRegine\nReiko\nReina\nReine\nRemy\nRena\nRenae\nRenata\nRenate\nRene\nRenee\nRenell\nRenelle\nRenie\nRennie\nReta\nRetha\nRevkah\nRey\nReyna\nRhea\nRheba\nRheta\nRhetta\nRhiamon\nRhianna\nRhianon\nRhoda\nRhodia\nRhodie\nRhody\nRhona\nRhonda\nRiane\nRiannon\nRianon\nRica\nRicca\nRici\nRicki\nRickie\nRicky\nRiki\nRikki\nRina\nRisa\nRita\nRiva\nRivalee\nRivi\nRivkah\nRivy\nRoana\nRoanna\nRoanne\nRobbi\nRobbie\nRobbin\nRobby\nRobbyn\nRobena\nRobenia\nRoberta\nRobin\nRobina\nRobinet\nRobinett\nRobinetta\nRobinette\nRobinia\nRoby\nRobyn\nRoch\nRochell\nRochella\nRochelle\nRochette\nRoda\nRodi\nRodie\nRodina\nRois\nRomola\nRomona\nRomonda\nRomy\nRona\nRonalda\nRonda\nRonica\nRonna\nRonni\nRonnica\nRonnie\nRonny\nRoobbie\nRora\nRori\nRorie\nRory\nRos\nRosa\nRosabel\nRosabella\nRosabelle\nRosaleen\nRosalia\nRosalie\nRosalind\nRosalinda\nRosalinde\nRosaline\nRosalyn\nRosalynd\nRosamond\nRosamund\nRosana\nRosanna\nRosanne\nRose\nRoseann\nRoseanna\nRoseanne\nRoselia\nRoselin\nRoseline\nRosella\nRoselle\nRosemaria\nRosemarie\nRosemary\nRosemonde\nRosene\nRosetta\nRosette\nRoshelle\nRosie\nRosina\nRosita\nRoslyn\nRosmunda\nRosy\nRow\nRowe\nRowena\nRoxana\nRoxane\nRoxanna\nRoxanne\nRoxi\nRoxie\nRoxine\nRoxy\nRoz\nRozalie\nRozalin\nRozamond\nRozanna\nRozanne\nRoze\nRozele\nRozella\nRozelle\nRozina\nRubetta\nRubi\nRubia\nRubie\nRubina\nRuby\nRuperta\nRuth\nRuthann\nRuthanne\nRuthe\nRuthi\nRuthie\nRuthy\nRyann\nRycca\nSaba\nSabina\nSabine\nSabra\nSabrina\nSacha\nSada\nSadella\nSadie\nSadye\nSaidee\nSal\nSalaidh\nSallee\nSalli\nSallie\nSally\nSallyann\nSallyanne\nSaloma\nSalome\nSalomi\nSam\nSamantha\nSamara\nSamaria\nSammy\nSande\nSandi\nSandie\nSandra\nSandy\nSandye\nSapphira\nSapphire\nSara\nSara-Ann\nSaraann\nSarah\nSarajane\nSaree\nSarena\nSarene\nSarette\nSari\nSarina\nSarine\nSarita\nSascha\nSasha\nSashenka\nSaudra\nSaundra\nSavina\nSayre\nScarlet\nScarlett\nSean\nSeana\nSeka\nSela\nSelena\nSelene\nSelestina\nSelia\nSelie\nSelina\nSelinda\nSeline\nSella\nSelle\nSelma\nSena\nSephira\nSerena\nSerene\nShae\nShaina\nShaine\nShalna\nShalne\nShana\nShanda\nShandee\nShandeigh\nShandie\nShandra\nShandy\nShane\nShani\nShanie\nShanna\nShannah\nShannen\nShannon\nShanon\nShanta\nShantee\nShara\nSharai\nShari\nSharia\nSharity\nSharl\nSharla\nSharleen\nSharlene\nSharline\nSharon\nSharona\nSharron\nSharyl\nShaun\nShauna\nShawn\nShawna\nShawnee\nShay\nShayla\nShaylah\nShaylyn\nShaylynn\nShayna\nShayne\nShea\nSheba\nSheela\nSheelagh\nSheelah\nSheena\nSheeree\nSheila\nSheila-Kathryn\nSheilah\nShel\nShela\nShelagh\nShelba\nShelbi\nShelby\nShelia\nShell\nShelley\nShelli\nShellie\nShelly\nShena\nSher\nSheree\nSheri\nSherie\nSherill\nSherilyn\nSherline\nSherri\nSherrie\nSherry\nSherye\nSheryl\nShina\nShir\nShirl\nShirlee\nShirleen\nShirlene\nShirley\nShirline\nShoshana\nShoshanna\nSiana\nSianna\nSib\nSibbie\nSibby\nSibeal\nSibel\nSibella\nSibelle\nSibilla\nSibley\nSibyl\nSibylla\nSibylle\nSidoney\nSidonia\nSidonnie\nSigrid\nSile\nSileas\nSilva\nSilvana\nSilvia\nSilvie\nSimona\nSimone\nSimonette\nSimonne\nSindee\nSiobhan\nSioux\nSiouxie\nSisely\nSisile\nSissie\nSissy\nSiusan\nSofia\nSofie\nSondra\nSonia\nSonja\nSonni\nSonnie\nSonnnie\nSonny\nSonya\nSophey\nSophi\nSophia\nSophie\nSophronia\nSorcha\nSosanna\nStace\nStacee\nStacey\nStaci\nStacia\nStacie\nStacy\nStafani\nStar\nStarla\nStarlene\nStarlin\nStarr\nStefa\nStefania\nStefanie\nSteffane\nSteffi\nSteffie\nStella\nStepha\nStephana\nStephani\nStephanie\nStephannie\nStephenie\nStephi\nStephie\nStephine\nStesha\nStevana\nStevena\nStoddard\nStorm\nStormi\nStormie\nStormy\nSue\nSuellen\nSukey\nSuki\nSula\nSunny\nSunshine\nSusan\nSusana\nSusanetta\nSusann\nSusanna\nSusannah\nSusanne\nSusette\nSusi\nSusie\nSusy\nSuzann\nSuzanna\nSuzanne\nSuzette\nSuzi\nSuzie\nSuzy\nSybil\nSybila\nSybilla\nSybille\nSybyl\nSydel\nSydelle\nSydney\nSylvia\nTabatha\nTabbatha\nTabbi\nTabbie\nTabbitha\nTabby\nTabina\nTabitha\nTaffy\nTalia\nTallia\nTallie\nTallou\nTallulah\nTally\nTalya\nTalyah\nTamar\nTamara\nTamarah\nTamarra\nTamera\nTami\nTamiko\nTamma\nTammara\nTammi\nTammie\nTammy\nTamqrah\nTamra\nTana\nTandi\nTandie\nTandy\nTanhya\nTani\nTania\nTanitansy\nTansy\nTanya\nTara\nTarah\nTarra\nTarrah\nTaryn\nTasha\nTasia\nTate\nTatiana\nTatiania\nTatum\nTawnya\nTawsha\nTed\nTedda\nTeddi\nTeddie\nTeddy\nTedi\nTedra\nTeena\nTEirtza\nTeodora\nTera\nTeresa\nTerese\nTeresina\nTeresita\nTeressa\nTeri\nTeriann\nTerra\nTerri\nTerrie\nTerrijo\nTerry\nTerrye\nTersina\nTerza\nTess\nTessa\nTessi\nTessie\nTessy\nThalia\nThea\nTheadora\nTheda\nThekla\nThelma\nTheo\nTheodora\nTheodosia\nTheresa\nTherese\nTheresina\nTheresita\nTheressa\nTherine\nThia\nThomasa\nThomasin\nThomasina\nThomasine\nTiena\nTierney\nTiertza\nTiff\nTiffani\nTiffanie\nTiffany\nTiffi\nTiffie\nTiffy\nTilda\nTildi\nTildie\nTildy\nTillie\nTilly\nTim\nTimi\nTimmi\nTimmie\nTimmy\nTimothea\nTina\nTine\nTiphani\nTiphanie\nTiphany\nTish\nTisha\nTobe\nTobey\nTobi\nToby\nTobye\nToinette\nToma\nTomasina\nTomasine\nTomi\nTommi\nTommie\nTommy\nToni\nTonia\nTonie\nTony\nTonya\nTonye\nTootsie\nTorey\nTori\nTorie\nTorrie\nTory\nTova\nTove\nTracee\nTracey\nTraci\nTracie\nTracy\nTrenna\nTresa\nTrescha\nTressa\nTricia\nTrina\nTrish\nTrisha\nTrista\nTrix\nTrixi\nTrixie\nTrixy\nTruda\nTrude\nTrudey\nTrudi\nTrudie\nTrudy\nTrula\nTuesday\nTwila\nTwyla\nTybi\nTybie\nTyne\nUla\nUlla\nUlrica\nUlrika\nUlrikaumeko\nUlrike\nUmeko\nUna\nUrsa\nUrsala\nUrsola\nUrsula\nUrsulina\nUrsuline\nUta\nVal\nValaree\nValaria\nVale\nValeda\nValencia\nValene\nValenka\nValentia\nValentina\nValentine\nValera\nValeria\nValerie\nValery\nValerye\nValida\nValina\nValli\nVallie\nVally\nValma\nValry\nVan\nVanda\nVanessa\nVania\nVanna\nVanni\nVannie\nVanny\nVanya\nVeda\nVelma\nVelvet\nVenita\nVenus\nVera\nVeradis\nVere\nVerena\nVerene\nVeriee\nVerile\nVerina\nVerine\nVerla\nVerna\nVernice\nVeronica\nVeronika\nVeronike\nVeronique\nVevay\nVi\nVicki\nVickie\nVicky\nVictoria\nVida\nViki\nVikki\nVikky\nVilhelmina\nVilma\nVin\nVina\nVinita\nVinni\nVinnie\nVinny\nViola\nViolante\nViole\nViolet\nVioletta\nViolette\nVirgie\nVirgina\nVirginia\nVirginie\nVita\nVitia\nVitoria\nVittoria\nViv\nViva\nVivi\nVivia\nVivian\nViviana\nVivianna\nVivianne\nVivie\nVivien\nViviene\nVivienne\nViviyan\nVivyan\nVivyanne\nVonni\nVonnie\nVonny\nVyky\nWallie\nWallis\nWalliw\nWally\nWaly\nWanda\nWandie\nWandis\nWaneta\nWanids\nWenda\nWendeline\nWendi\nWendie\nWendy\nWendye\nWenona\nWenonah\nWhitney\nWileen\nWilhelmina\nWilhelmine\nWilie\nWilla\nWillabella\nWillamina\nWilletta\nWillette\nWilli\nWillie\nWillow\nWilly\nWillyt\nWilma\nWilmette\nWilona\nWilone\nWilow\nWindy\nWini\nWinifred\nWinna\nWinnah\nWinne\nWinni\nWinnie\nWinnifred\nWinny\nWinona\nWinonah\nWren\nWrennie\nWylma\nWynn\nWynne\nWynnie\nWynny\nXaviera\nXena\nXenia\nXylia\nXylina\nYalonda\nYasmeen\nYasmin\nYelena\nYetta\nYettie\nYetty\nYevette\nYnes\nYnez\nYoko\nYolanda\nYolande\nYolane\nYolanthe\nYoshi\nYoshiko\nYovonnda\nYsabel\nYvette\nYvonne\nZabrina\nZahara\nZandra\nZaneta\nZara\nZarah\nZaria\nZarla\nZea\nZelda\nZelma\nZena\nZenia\nZia\nZilvia\nZita\nZitella\nZoe\nZola\nZonda\nZondra\nZonnya\nZora\nZorah\nZorana\nZorina\nZorine\nZsa Zsa\nZsazsa\nZulema\nZuzana\n"
          },
          {
            "fileName": "dataset.py",
            "fileContents": "import os\nfrom datasets import load_dataset\nimport logging\nimport random\nfrom datetime import datetime\nfrom typing import Dict, Iterable, Union\nfrom openai.types.chat import ChatCompletionMessageParam\n\nfrom targon.types import Endpoints\n\nBASE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\")\nNAMES = [line.strip() for line in open(os.path.join(BASE_DIR, \"names.txt\")).readlines()]\nCOUNTRIES = [\n    line.strip() for line in open(os.path.join(BASE_DIR, \"countries.txt\")).readlines()\n]\n\n\ndef create_search_prompt(\n    query: str, endpoint: Endpoints\n) -> Dict[str, Union[str, Iterable[ChatCompletionMessageParam]]]:\n    # Format the current date for inclusion in the prompt\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n    system_message = f\"\"\"\n### Current Date: {date}\n### Instruction:\nYou are to take on the role of {random.choice(NAMES)}, an expert language model\ndeveloped in {random.choice(COUNTRIES)}, tasked with generating responses to user queries.\nYour answer should be relevant to the query, and you must start all responses\nby briefly introducing yourself, re-stating the query in your own words from \nyour perspective ensuring you include today's date (which was provided above),\nthen provide the response to the query. You should always respond in English.\n\"\"\"\n    # Compile the chat components into a structured format\n    match endpoint:\n        case Endpoints.CHAT:\n            messages: Iterable[ChatCompletionMessageParam] = [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": query},\n            ]\n            return {\"messages\": messages}\n        case Endpoints.COMPLETION:\n            prompt: str = f\"\"\"{system_message}\\n\\n{query}\"\"\"\n            return {\"prompt\": prompt}\n        case _:\n            raise Exception(\"Unknown Endpoint\")\n\n\ndef create_query_prompt(query: str) -> Iterable[ChatCompletionMessageParam]:\n    # Format the current date for inclusion in the prompt\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Construct the system message with dynamic content\n    system_message = f\"\"\"\n### Current Date: {date}\n### Instruction:\nYou are to take the query information that is passed from you and create a search query for the query data. \nDo not answer the information, just create a search query. The search query should not be longer than a sentence.\nAssistant should always start the response with \"Search query: \"\n\"\"\"\n\n    # Compile the chat components into a structured format\n    chats: Iterable[ChatCompletionMessageParam] = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": query},\n    ]\n\n    # Apply the chat template without tokenization\n    return chats\n\n\ndef download_dataset():\n    logger = logging.getLogger(\"huggingface_hub.utils._http\")\n    logger.setLevel(logging.CRITICAL + 1)\n    ds = load_dataset(\"manifoldlabs/Infinity-Instruct\", \"7M\")\n    return ds\n"
          },
          {
            "fileName": "docker.py",
            "fileContents": "from time import sleep\nimport random\nfrom typing import Any, Dict, List, Tuple\nimport re\nimport math\nimport docker\nimport bittensor as bt\nimport subprocess\nfrom accelerate.commands import estimate\n\nfrom docker.models.containers import Container\nfrom docker.types import DeviceRequest\nimport requests\n\nfrom targon.config import IMAGE_TAG\nfrom targon.types import Endpoints\n\n\ndef get_gpu_with_space(gpus: List[Tuple[int, int, int]], required: int):\n    \"[GPU_ID, free, total] in MB\"\n    bt.logging.info(f\"Need: {required}, have: {gpus}\")\n    \n    # find unsused GPUS\n    unused = [gpu for gpu in gpus if gpu[1] / gpu[2] > 0.9]\n\n    # find first gpu with enough space\n    for gpu in unused:\n        if gpu[1] >= required * 1.2:\n            return [gpu]\n    \n    # if we need multiple gpu, only used unused\n    total_free = 0\n    next_gpus = []\n    for gpu in unused:\n        total_free += gpu[1]\n        next_gpus.append(gpu)\n        if total_free > required * 1.2:\n            return next_gpus\n    return None\n\n\ndef bytes_to_mib(bytes_value):\n    mib_value = bytes_value / (1024**2)  # 1024^2 = 1,048,576\n    return math.ceil(mib_value)\n\n\ndef estimate_max_size(model_name):\n    \"Returns size in MiB, what nvidia smi prints\"\n    try:\n        model = estimate.create_empty_model(\n            model_name, library_name=\"transformers\", trust_remote_code=False\n        )\n    except (RuntimeError, OSError) as e:\n        library = estimate.check_has_model(e)\n        if library != \"unknown\":\n            raise RuntimeError(\n                f\"Tried to load `{model_name}` with `{library}` but a possible model to load was not found inside the repo.\"\n            )\n        return None\n\n    total_size, _ = estimate.calculate_maximum_sizes(model)\n    return bytes_to_mib(total_size)\n\n\nMANIFOLD_VERIFIER = \"manifoldlabs/sn4-verifier\"\n\n\ndef load_docker():\n    client = docker.from_env()\n    return client\n\n\ndef get_free_gpus() -> List[Tuple[int, int, int]]:\n    \"[GPU_ID, free, total] in MB\"\n    res = subprocess.run(\n        [\n            \"nvidia-smi\",\n            \"--query-gpu=memory.free,memory.total\",\n            \"--format=csv,noheader\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n    )\n    if res.returncode != 0:\n        bt.logging.error(res.stdout.decode(\"utf-8\"))\n        raise Exception(\"Failed to detect nvida gpus\")\n\n    lines = [line.split(\" \") for line in res.stdout.decode(\"utf-8\").strip().split(\"\\n\")]\n    gpus = [(i, int(line[0]), int(line[2])) for i, line in enumerate(lines)]\n    return gpus\n\n\ndef remove_containers(client):\n    containers: List[Container] = client.containers.list(  # type: ignore\n        filters={\"label\": \"model\"}\n    )\n    for container in containers:\n        model = container.labels.get(\"model\")\n        bt.logging.info(f\"Removing {container.name}: {model}\")\n        container.remove(force=True)\n\ndef sync_output_checkers(\n    client: docker.DockerClient, models: List[str]\n) -> Dict[str, Dict[str, Any]]:\n\n    # Get new image hash (if any)\n    image_name = f\"{MANIFOLD_VERIFIER}:{IMAGE_TAG}\"\n    try:\n        client.images.pull(image_name)  # type: ignore\n    except Exception as e:\n        bt.logging.error(str(e))\n    bt.logging.info(f\"Syncing {models}\")\n\n    # Remove all containers\n    remove_containers(client)\n    verification_ports = {}\n    used_ports = []\n    random.shuffle(models)\n    min_port = 5555\n\n    # Clear containers that arent running\n    client.containers.prune()\n\n    # Load all models\n    bt.logging.info(f\"Starting subset of {list(models)}\")\n    for model in models:\n        container_name = re.sub(r\"[\\W_]\", \"-\", model).lower()\n\n        # Delete if existing and out of date\n        existing_containers: List[Container] = client.containers.list(filters={\"name\": container_name})  # type: ignore\n        if len(existing_containers):\n            existing_containers[0].remove(force=True)\n\n        # Determine GPU free\n        free_gpus = get_free_gpus()\n        required_vram = estimate_max_size(model)\n        if required_vram is None:\n            bt.logging.error(f\"Failed to find model {model}\")\n            continue\n        gpus = get_gpu_with_space(free_gpus, required_vram)\n        if gpus is None:\n            bt.logging.info(f\"Not enough space to run {model}\")\n            continue\n\n        # Find Port\n        while min_port in used_ports:\n            min_port += 1\n        used_ports.append(min_port)\n\n        # Init new container\n        bt.logging.info(\n            f\"Loading {model} on gpu(s) {[gpu[0] for gpu in gpus]}\"\n        )\n        config: Dict[str, Any] = {\n            \"image\": image_name,\n            \"ports\": {f\"80/tcp\": min_port},\n            \"environment\": [\n                f\"MODEL={model}\",\n                f\"TENSOR_PARALLEL={len(gpus)}\",\n            ],\n            \"volumes\": [\"/var/targon/huggingface/cache:/root/.cache/huggingface\"],\n            \"runtime\": \"nvidia\",\n            \"detach\": True,\n            \"ipc_mode\": \"host\",\n            \"name\": container_name,\n            \"extra_hosts\": {\"host.docker.internal\": \"host-gateway\"},\n            \"labels\": {\"model\": str(model), \"port\": str(min_port)},\n            \"device_requests\": [\n                DeviceRequest(\n                    device_ids=[str(gpu[0]) for gpu in gpus], capabilities=[[\"gpu\"]]\n                )\n            ],\n        }\n        client.containers.run(**config)  # type: ignore\n        while True:\n            ready = True\n            std_model = re.sub(r\"[\\W_]\", \"-\", model).lower()\n            containers: List[Container] = client.containers.list(filters={\"name\": std_model}, all=True)  # type: ignore\n            if not len(containers):\n                bt.logging.info(\n                    f\"Failed starting container {std_model}: Removing from verifiers\"\n                )\n                break\n            (container,) = containers\n            if container.health == \"unhealthy\":\n                container_logs = container.logs()\n                bt.logging.error(\n                    f\"Failed starting container {std_model}: Removing from verifiers\"\n                )\n                bt.logging.error(\"---- Verifier Logs ----\")\n                bt.logging.error(container_logs)\n                bt.logging.error(\"-----------------------\")\n                break\n            if container.health != \"healthy\":\n                bt.logging.info(f\"{container.name}: {container.health}\")\n                ready = False\n            if ready:\n                verification_ports[model] = {\"port\": min_port}\n                endpoints = requests.get(\n                    f\"http://localhost:{min_port}/endpoints\"\n                ).json()\n                endpoints = [Endpoints(e.upper()) for e in endpoints]\n                verification_ports[model][\"endpoints\"] = endpoints\n                break\n            bt.logging.info(\"Checking again in 5 seconds\")\n            sleep(5)\n\n    bt.logging.info(\"Successfully started verifiers\")\n    bt.logging.info(str(verification_ports))\n    if len(list(verification_ports.keys())) == 0:\n        bt.logging.error(\"No verification ports\")\n        exit()\n    return verification_ports\n"
          },
          {
            "fileName": "epistula.py",
            "fileContents": "import json\nfrom hashlib import sha256\nfrom uuid import uuid4\nfrom math import ceil\nfrom typing import Annotated, Any, Dict, Optional\n\nimport time\nimport httpx\nfrom substrateinterface import Keypair\n\n\ndef generate_header(\n    hotkey: Keypair,\n    body: Any,\n    signed_for: Optional[str] = None,\n) -> Dict[str, Any]:\n    timestamp = round(time.time() * 1000)\n    timestampInterval = ceil(timestamp / 1e4) * 1e4\n    uuid = str(uuid4())\n    req_hash = None\n    if isinstance(body, bytes):\n        req_hash = sha256(body).hexdigest()\n    else:\n        req_hash = sha256(json.dumps(body).encode(\"utf-8\")).hexdigest()\n\n    headers = {\n        \"Epistula-Version\": str(2),\n        \"Epistula-Timestamp\": str(timestamp),\n        \"Epistula-Uuid\": uuid,\n        \"Epistula-Signed-By\": hotkey.ss58_address,\n        \"Epistula-Request-Signature\": \"0x\"\n        + hotkey.sign(f\"{req_hash}.{uuid}.{timestamp}.{signed_for or ''}\").hex(),\n    }\n    if signed_for:\n        headers[\"Epistula-Signed-For\"] = signed_for\n        headers[\"Epistula-Secret-Signature-0\"] = (\n            \"0x\" + hotkey.sign(str(timestampInterval - 1) + \".\" + signed_for).hex()\n        )\n        headers[\"Epistula-Secret-Signature-1\"] = (\n            \"0x\" + hotkey.sign(str(timestampInterval) + \".\" + signed_for).hex()\n        )\n        headers[\"Epistula-Secret-Signature-2\"] = (\n            \"0x\" + hotkey.sign(str(timestampInterval + 1) + \".\" + signed_for).hex()\n        )\n    return headers\n\n\ndef verify_signature(\n    signature, body: bytes, timestamp, uuid, signed_for, signed_by, now\n) -> Optional[Annotated[str, \"Error Message\"]]:\n    if not isinstance(signature, str):\n        return \"Invalid Signature\"\n    timestamp = int(timestamp)\n    if not isinstance(timestamp, int):\n        return \"Invalid Timestamp\"\n    if not isinstance(signed_by, str):\n        return \"Invalid Sender key\"\n    if not isinstance(signed_for, str):\n        return \"Invalid receiver key\"\n    if not isinstance(uuid, str):\n        return \"Invalid uuid\"\n    if not isinstance(body, bytes):\n        return \"Body is not of type bytes\"\n    ALLOWED_DELTA_MS = 8000\n    keypair = Keypair(ss58_address=signed_by)\n    if timestamp + ALLOWED_DELTA_MS < now:\n        return \"Request is too stale\"\n    message = f\"{sha256(body).hexdigest()}.{uuid}.{timestamp}.{signed_for}\"\n    verified = keypair.verify(message, signature)\n    if not verified:\n        return \"Signature Mismatch\"\n    return None\n\n\ndef create_header_hook(hotkey, axon_hotkey, model):\n    async def add_headers(request: httpx.Request):\n        for key, header in generate_header(hotkey, request.read(), axon_hotkey).items():\n            request.headers[key] = header\n        request.headers[\"X-Targon-Model\"] = model\n\n    return add_headers\n"
          },
          {
            "fileName": "jugo.py",
            "fileContents": "from typing import Any, Dict, List, Optional, Tuple\n\nimport aiohttp\nimport traceback\nfrom nanoid import generate\n\nfrom targon.epistula import generate_header\nfrom targon.request import check_tokens\nfrom targon.types import Endpoints, InferenceStats, OrganicStats\nimport bittensor as bt\n\nJUGO_URL = \"https://jugo.targon.com\"\n\n\nasync def send_organics_to_jugo(\n    wallet: \"bt.wallet\",\n    organics: List[OrganicStats],\n):\n    try:\n        body = {\"organics\": [organic.model_dump() for organic in organics]}\n        headers = generate_header(wallet.hotkey, body)\n        # Send request to the FastAPI server\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{JUGO_URL}/organics/scores\",\n                headers=headers,\n                json=body,\n                timeout=aiohttp.ClientTimeout(60),\n            ) as response:\n                if response.status == 200:\n                    bt.logging.info(\"Records sent successfully.\")\n                else:\n                    error_detail = await response.text()\n                    bt.logging.error(\n                        f\"Error sending records: {response.status} - {error_detail}\"\n                    )\n\n    except aiohttp.ClientConnectionError:\n        bt.logging.error(\"Error conecting to jugo, offline.\")\n    except Exception as e:\n        bt.logging.error(f\"Error in send_stats_to_jugo: {e}\")\n        bt.logging.error(traceback.format_exc())\n\n\nasync def send_stats_to_jugo(\n    metagraph: \"bt.metagraph\",\n    subtensor: \"bt.subtensor\",\n    wallet: \"bt.wallet\",\n    stats: List[Tuple[int, Optional[InferenceStats]]],\n    req: Dict[str, Any],\n    endpoint: Endpoints,\n    version: int,\n    models: List[str],\n    miner_tps: Dict[int, Dict[str, List[Optional[float]]]],\n):\n    try:\n        r_nanoid = generate(size=48)\n        responses = [\n            {\n                \"r_nanoid\": r_nanoid,\n                \"hotkey\": metagraph.axons[uid].hotkey,\n                \"coldkey\": metagraph.axons[uid].coldkey,\n                \"uid\": int(uid),\n                \"stats\": stat and stat.model_dump(),\n            }\n            for uid, stat in stats\n        ]\n        request = {\n            \"r_nanoid\": r_nanoid,\n            \"block\": subtensor.block,\n            \"request\": req,\n            \"request_endpoint\": str(endpoint),\n            \"version\": version,\n            \"hotkey\": wallet.hotkey.ss58_address,\n        }\n        # Prepare the data\n        body = {\n            \"request\": request,\n            \"responses\": responses,\n            \"models\": models,\n            \"scores\": miner_tps,\n        }\n        headers = generate_header(wallet.hotkey, body)\n        # Send request to the FastAPI server\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{JUGO_URL}/\",\n                headers=headers,\n                json=body,\n                timeout=aiohttp.ClientTimeout(60),\n            ) as response:\n                if response.status == 200:\n                    bt.logging.info(\"Records sent successfully.\")\n                else:\n                    error_detail = await response.text()\n                    bt.logging.error(\n                        f\"Error sending records: {response.status} - {error_detail}\"\n                    )\n\n    except aiohttp.ClientConnectionError:\n        bt.logging.error(\"Error conecting to jugo, offline.\")\n    except Exception as e:\n        bt.logging.error(f\"Error in send_stats_to_jugo: {e}\")\n        bt.logging.error(traceback.format_exc())\n\n\nasync def score_organics(last_bucket_id, ports, wallet):\n    try:\n        async with aiohttp.ClientSession() as session:\n            body = list(ports.keys())\n            headers = generate_header(wallet.hotkey, body)\n            async with session.post(\n                JUGO_URL + \"/organics\",\n                headers=headers,\n                json=body,\n                timeout=aiohttp.ClientTimeout(60),\n            ) as res:\n                if res.status != 200:\n                    bt.logging.info(f\"Error pinging jugo {res.text}\")\n                    return last_bucket_id, None, None\n                res_body = await res.json()\n        bucket_id = res_body.get(\"bucket_id\")\n        organics = res_body.get(\"organics\")\n        if last_bucket_id == bucket_id:\n            bt.logging.info(f\"Already seen this bucket id\")\n            return last_bucket_id, None, None\n        scores = {}\n        organic_stats = []\n        bt.logging.info(f\"Found {len(organics)} organics\")\n        for model, records in organics.items():\n            for record in records:\n                uid = record[\"uid\"]\n                if scores.get(uid) is None:\n                    scores[uid] = []\n                if not record[\"success\"]:\n                    scores[uid].append(-500)\n                    continue\n                tokens = []\n                for token in record[\"response\"]:\n                    choice = token.get(\"choices\", [{}])[0]\n                    text = \"\"\n                    logprob = -100\n                    match record[\"endpoint\"]:\n                        case \"CHAT\":\n                            text = choice.get(\"delta\", {}).get(\"content\")\n                            logprobs = choice.get(\"logprobs\")\n                            if logprobs is None:\n                                continue\n                            logprob = logprobs.get(\"content\", [{}])[0].get(\n                                \"logprob\", -100\n                            )\n                            token = logprobs.get(\"content\", [{}])[0].get(\"token\", None)\n                            if text is None or (text == \"\" and len(tokens) == 0):\n                                continue\n                        case \"COMPLETION\":\n                            text = choice.get(\"text\")\n                            logprobs = choice.get(\"logprobs\")\n                            if logprobs is None:\n                                continue\n                            logprob = logprobs.get(\"token_logprobs\", [-100])[0]\n                            token = logprobs.get(\"tokens\", [\"\"])[0]\n                            if text is None or (text == \"\" and len(tokens) == 0):\n                                continue\n\n                    token_id = -1\n                    if not token.startswith(\"token_id:\"):\n                        continue\n                    token_parts = token.split(\":\")\n                    if len(token_parts) > 1:\n                        token_id = int(token_parts[1])\n\n                    tokens.append(\n                        {\n                            \"text\": text,\n                            \"logprob\": logprob,\n                            \"token_id\": token_id,\n                        }\n                    )\n\n                # No response tokens\n                if len(tokens) == 0:\n                    scores[uid].append(-100)\n                    continue\n\n                port = ports.get(model, {}).get(\"port\")\n                if not port:\n                    continue\n                res = await check_tokens(\n                    record[\"request\"],\n                    tokens,\n                    record[\"uid\"],\n                    Endpoints(record[\"endpoint\"]),\n                    port,\n                )\n                bt.logging.info(str(res))\n                if res is None:\n                    continue\n                verified = res.get(\"verified\", False)\n                tps = 0\n                if verified:\n                    try:\n                        response_tokens_count = int(record.get(\"response_tokens\", 0))\n\n                        # This shouldnt happen\n                        if response_tokens_count == 0:\n                            continue\n\n                        tps = min(\n                            response_tokens_count, record[\"request\"][\"max_tokens\"]\n                        ) / (int(record.get(\"total_time\")) / 1000)\n                        scores[uid].append(tps)\n                    except Exception as e:\n                        bt.logging.error(\"Error scoring record: \" + str(e))\n                        continue\n                organic_stats.append(\n                    OrganicStats(\n                        time_to_first_token=int(record.get(\"time_to_first_token\")),\n                        time_for_all_tokens=int(record.get(\"total_time\"))\n                        - int(record.get(\"time_to_first_token\")),\n                        total_time=int(record.get(\"total_time\")),\n                        tps=tps,\n                        tokens=[],\n                        verified=verified,\n                        error=res.get(\"error\"),\n                        cause=res.get(\"cause\"),\n                        model=model,\n                        max_tokens=record.get(\"request\").get(\"max_tokens\"),\n                        seed=record.get(\"request\").get(\"seed\"),\n                        temperature=record.get(\"request\").get(\"temperature\"),\n                        uid=uid,\n                        hotkey=record.get(\"hotkey\"),\n                        coldkey=record.get(\"coldkey\"),\n                        endpoint=record.get(\"endpoint\"),\n                        total_tokens=record.get(\"response_tokens\"),\n                    )\n                )\n        bt.logging.info(f\"{bucket_id}: {scores}\")\n        return bucket_id, scores, organic_stats\n    except Exception as e:\n        bt.logging.error(str(e))\n        return None\n"
          },
          {
            "fileName": "math.py",
            "fileContents": "from math import exp\nimport bittensor as bt\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\n\nfrom targon.config import SLIDING_WINDOW\nfrom targon.utils import fail_with_none\n\n\ndef normalize(arr: List[float], t_min=0, t_max=1) -> List[float]:\n    norm_arr = []\n    diff = t_max - t_min\n    diff_arr = max(arr) - min(arr)\n    for i in arr:\n        temp = (((i - min(arr)) * diff) / diff_arr) + t_min\n        norm_arr.append(temp)\n    return norm_arr\n\n\ndef sigmoid(num):\n    return 1 / (1 + exp(-((num - 0.5) / 0.1)))\n\n\ndef safe_mean_score(data):\n    clean_data = [x for x in data if x is not None]\n    if len(clean_data) == 0:\n        return 0.0\n    mean_value = np.mean(clean_data)\n    if np.isnan(mean_value) or np.isinf(mean_value):\n        return 0.0\n    return float(mean_value) * sigmoid(len(clean_data) / len(data))\n\n\n@fail_with_none(\"Failed getting Weights\")\ndef get_weights(\n    miner_models: Dict[int, List[str]],\n    miner_tps: Dict[int, Dict[str, List[Optional[float]]]],\n    organics: Dict[int, list[int]],\n    models: List[str],\n) -> Tuple[List[int], List[float]]:\n    # Mean and sigmoid of tps scores from each model. Since all miners are queried with\n    # All models, more models served = higher score. *then* it becomes a speed game.\n    tps = {}\n    for uid in miner_tps:\n        tps[uid] = 0\n        if (organic := organics.get(uid)) is not None:\n            tps[uid] = safe_mean_score(organic)\n        for model in miner_models.get(uid, []):\n            if model not in models:\n                continue\n            if miner_tps.get(uid) is None:\n                continue\n            if miner_tps[uid].get(model) is None:\n                continue\n\n            tps[uid] += safe_mean_score(miner_tps[uid][model][-SLIDING_WINDOW:])\n\n    tps_list = list(tps.values())\n    if len(tps_list) == 0:\n        bt.logging.warning(\"Not setting weights, no responses from miners\")\n        return [], []\n    uids: List[int] = sorted(tps.keys())\n    rewards = [tps[uid] for uid in uids]\n\n    bt.logging.info(f\"All wps: {tps}\")\n    if sum(rewards) < 1 / 1e9:\n        bt.logging.warning(\"No one gave responses worth scoring\")\n        return [], []\n    raw_weights = normalize(rewards)\n    bt.logging.info(f\"Raw Weights: {raw_weights}\")\n    return uids, raw_weights\n"
          },
          {
            "fileName": "metagraph.py",
            "fileContents": "import time\nfrom typing import Callable, Dict, List, Tuple\nimport numpy as np\nimport bittensor as bt\nfrom bittensor.utils.weight_utils import process_weights_for_netuid\n\nfrom targon.utils import fail_with_none\n\nimport threading\n\n\ndef get_miner_uids(\n    metagraph: \"bt.metagraph\", self_uid: int, vpermit_tao_limit: int\n) -> List[int]:\n    available_uids = []\n    for uid in range(int(metagraph.n.item())):\n        if uid == self_uid:\n            continue\n\n        # Filter non serving axons.\n        if not metagraph.axons[uid].is_serving:\n            continue\n        # Filter validator permit > 1024 stake.\n        if metagraph.validator_permit[uid]:\n            if metagraph.S[uid] > vpermit_tao_limit:\n                continue\n        available_uids.append(uid)\n        continue\n    return available_uids\n\n\n@fail_with_none(\"Failed resyncing hotkeys\")\ndef resync_hotkeys(metagraph: \"bt.metagraph\", miner_tps: Dict):\n    bt.logging.info(\"re-syncing hotkeys\")\n    # Zero out all hotkeys that have been replaced.\n    for uid, hotkey in enumerate(metagraph.hotkeys):\n        if miner_tps.get(uid) is None:\n            miner_tps[uid] = {}\n        if hotkey != metagraph.hotkeys[uid]:\n            miner_tps[uid] = {}\n\n\ndef create_set_weights(version: int, netuid):\n    @fail_with_none(\"Failed setting weights\")\n    def set_weights(\n        wallet: \"bt.wallet\",\n        metagraph: \"bt.metagraph\",\n        subtensor: \"bt.subtensor\",\n        weights: Tuple[List[int], List[float]],\n    ):\n        if weights is None:\n            return None\n        uids, raw_weights = weights\n        if not len(uids):\n            bt.logging.info(\"No UIDS to score\")\n            return\n\n        # Set the weights on chain via our subtensor connection.\n        (\n            processed_weight_uids,\n            processed_weights,\n        ) = process_weights_for_netuid(\n            uids=np.asarray(uids),\n            weights=np.asarray(raw_weights),\n            netuid=netuid,\n            subtensor=subtensor,\n            metagraph=metagraph,\n        )\n\n        bt.logging.info(\"Setting Weights: \" + str(processed_weights))\n        bt.logging.info(\"Weight Uids: \" + str(processed_weight_uids))\n        for _ in range(3):\n            result, message = subtensor.set_weights(\n                wallet=wallet,\n                netuid=netuid,\n                uids=processed_weight_uids,  # type: ignore\n                weights=processed_weights,\n                wait_for_finalization=False,\n                wait_for_inclusion=False,\n                version_key=version,\n                max_retries=1,\n            )\n            if result is True:\n                bt.logging.info(\"set_weights on chain successfully!\")\n                break\n            else:\n                bt.logging.error(f\"set_weights failed {message}\")\n            time.sleep(15)\n\n    return set_weights\n\n\ndef create_subscription_handler(substrate, callback: Callable):\n    def inner(obj, update_nr, _):\n        substrate.get_block(block_number=obj[\"header\"][\"number\"])\n\n        if update_nr >= 1:\n            return callback(obj[\"header\"][\"number\"])  # type: int\n\n    return inner\n\n\ndef start_subscription(substrate, callback: Callable):\n    return substrate.subscribe_block_headers(\n        create_subscription_handler(substrate, callback)\n    )\n\n\ndef run_block_callback_thread(substrate, callback: Callable):\n    subscription_thread = threading.Thread(\n        target=start_subscription, args=[substrate, callback], daemon=True\n    )\n    subscription_thread.start()\n    bt.logging.info(\"Block subscription started in background thread.\")\n    return subscription_thread\n"
          },
          {
            "fileName": "request.py",
            "fileContents": "import math\nfrom os import urandom\nimport time\nimport traceback\nfrom typing import Dict, List, Optional, Tuple\n\nfrom httpx import Timeout\nimport openai\nimport requests\nfrom targon.dataset import create_query_prompt, create_search_prompt\nfrom targon.epistula import create_header_hook\nfrom targon.types import Endpoints, InferenceStats\nfrom targon.utils import fail_with_none\nimport random\nimport bittensor as bt\n\n\n@fail_with_none(\"Error generating dataset\")\ndef generate_request(dataset, model_name, endpoint: Endpoints, port: int):\n    # Generate a random seed for reproducibility in sampling and text generation\n    random.seed(urandom(100))\n    seed = random.randint(10000, 10000000)\n    temperature = random.random()\n    max_tokens = random.randint(512, 1920)\n\n    total_rows = len(dataset[\"train\"])\n    random_row_text = dataset[\"train\"][random.randint(0, total_rows - 1)][\n        \"conversations\"\n    ][0][\"value\"]\n    # Generate a query from the sampled text and perform text generation\n    messages = create_query_prompt(random_row_text)\n    res: Optional[str] = None\n    response = None\n    for _ in range(3):\n        try:\n            response = requests.post(\n                f\"http://localhost:{port}/generate\",\n                headers={\"Content-Type\": \"application/json\"},\n                json={\n                    \"messages\": messages,\n                    \"sampling_params\": {\n                        \"temperature\": 0.5,\n                        \"seed\": seed,\n                        \"max_tokens\": random.randint(16, 64),\n                    },\n                },\n            )\n            if response.status_code != 200:\n                bt.logging.error(f\"Failed to generate request for {model_name}\")\n                return None\n            res = response.json().get(\"text\")\n        except Exception:\n            bt.logging.error(f\"Failed to generate request for {model_name}\")\n        break\n    if res is None:\n        bt.logging.error(\n            f\"Failed to generate prompt for {model_name}: {endpoint}: {response}\"\n        )\n        return None\n\n    # Create sampling parameters using the generated seed and token limit\n    return {\n        \"seed\": seed,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"model\": model_name,\n        \"stream\": True,\n        \"logprobs\": True,\n        **create_search_prompt(res, endpoint),\n    }\n\n\nasync def handle_inference(\n    metagraph: \"bt.metagraph\",\n    wallet: \"bt.wallet\",\n    request,\n    uid: int,\n    endpoint: Endpoints,\n) -> Tuple[int, InferenceStats]:\n    stats = InferenceStats(\n        time_to_first_token=0,\n        time_for_all_tokens=0,\n        tps=0,\n        total_time=0,\n        tokens=[],\n        verified=False,\n    )\n    try:\n        axon_info = metagraph.axons[uid]\n        miner = openai.AsyncOpenAI(\n            base_url=f\"http://{axon_info.ip}:{axon_info.port}/v1\",\n            api_key=\"sn4\",\n            max_retries=0,\n            timeout=Timeout(12, connect=5, read=5),\n            http_client=openai.DefaultAsyncHttpxClient(\n                event_hooks={\n                    \"request\": [\n                        create_header_hook(\n                            wallet.hotkey, axon_info.hotkey, request[\"model\"]\n                        )\n                    ]\n                }\n            ),\n        )\n        start_token_time = 0\n        start_send_message_time = time.time()\n        token_times = []\n        try:\n            match endpoint:\n                case Endpoints.CHAT:\n                    chat = await miner.chat.completions.create(**request)\n                    async for chunk in chat:\n                        if chunk.choices[0].delta is None:\n                            continue\n                        if (\n                            chunk.choices[0].delta.content == \"\"\n                            or chunk.choices[0].delta.content is None\n                        ) and len(stats.tokens) == 0:\n                            continue\n                        if start_token_time == 0:\n                            start_token_time = time.time()\n                        choice = chunk.choices[0]\n                        logprob = -100\n                        token_id = -1\n                        choiceprobs = choice.logprobs\n                        if choiceprobs is not None:\n                            if choiceprobs.content:\n                                logprob = choiceprobs.content[0].logprob\n                                token = choiceprobs.content[0].token\n                                if token is None:\n                                    continue\n                                if not token.startswith(\"token_id:\"):\n                                    continue\n                                token_parts = token.split(\":\")\n                                if len(token_parts) > 1:\n                                    token_id = int(token_parts[1])\n                        stats.tokens.append(\n                            {\n                                \"text\": choice.delta.content or \"\",\n                                \"token_id\": token_id,\n                                \"logprob\": logprob,\n                            }\n                        )\n                        token_times.append(time.time())\n                case Endpoints.COMPLETION:\n                    comp = await miner.completions.create(**request)\n                    async for chunk in comp:\n                        if (\n                            chunk.choices[0].text == \"\" or chunk.choices[0].text is None\n                        ) and len(stats.tokens) == 0:\n                            continue\n                        if start_token_time == 0:\n                            start_token_time = time.time()\n                        choice = chunk.choices[0]\n                        if choice.logprobs is None:\n                            continue\n                        token_id = -1\n                        logprob = -100\n                        if choice.logprobs.token_logprobs:\n                            logprob = choice.logprobs.token_logprobs[0]\n                        if (\n                            choice.logprobs.tokens is not None\n                            and len(choice.logprobs.tokens) > 0\n                        ):\n                            token = choice.logprobs.tokens[0]\n                            if token is None:\n                                continue\n                            if not token.startswith(\"token_id:\"):\n                                continue\n                            token_parts = token.split(\":\")\n                            if len(token_parts) > 1:\n                                token_id = int(token_parts[1])\n                        stats.tokens.append(\n                            {\n                                \"text\": choice.text or \"\",\n                                \"token_id\": token_id,\n                                \"logprob\": logprob,\n                            }\n                        )\n                        token_times.append(time.time())\n        except openai.APIConnectionError as e:\n            bt.logging.trace(f\"Miner {uid} failed request: {e}\")\n            stats.error = str(e)\n            stats.cause = \"BAD_STREAM\"\n        except Exception as e:\n            bt.logging.trace(f\"Unknown Error when sending to miner {uid}: {e}\")\n            stats.error = str(e)\n            stats.cause = \"BAD_STREAM\"\n\n        if start_token_time == 0:\n            start_token_time = time.time()\n        end_token_time = time.time()\n        time_to_first_token = start_token_time - start_send_message_time\n        time_for_all_tokens = end_token_time - start_token_time\n        if stats.error:\n            return uid, stats\n        stats.time_to_first_token = time_to_first_token\n        stats.time_for_all_tokens = time_for_all_tokens\n        stats.total_time = end_token_time - start_send_message_time\n        stats.tps = min(len(stats.tokens), request[\"max_tokens\"]) / stats.total_time\n\n        # Detect when response was fully generated, then streamed, which leads to\n        # poor user experience (slow time to N tokens vs total time).\n        token_count = len(stats.tokens)\n        if token_count > 60:\n            time_to_5th_percent = (\n                token_times[math.ceil(token_count * 0.05)] - start_send_message_time\n            )\n            if time_to_5th_percent / stats.total_time >= 0.85:\n                stats.verified = False\n                stats.error = \"Likely non-streamed response\"\n                stats.cause = \"BAD_STREAM\"\n        return uid, stats\n    except Exception as e:\n        bt.logging.error(f\"{uid}: Error in forward for: {e}\")\n        bt.logging.error(traceback.format_exc())\n        return uid, stats\n\n\n@fail_with_none(\"Failed to check tokens\")\nasync def check_tokens(\n    request,\n    responses: List[Dict],\n    uid,\n    endpoint: Endpoints,\n    port: int,\n    url=\"http://localhost\",\n) -> Optional[Dict]:\n    try:\n        result = requests.post(\n            f\"{url}:{port}/verify\",\n            headers={\"Content-Type\": \"application/json\"},\n            json={\n                \"model\": request.get(\"model\"),\n                \"request_type\": endpoint.value,\n                \"request_params\": request,\n                \"output_sequence\": responses,\n            },\n        ).json()\n        if result.get(\"verified\") is None:\n            bt.logging.error(str(result))\n            return None\n        return result\n    except Exception as e:\n        bt.logging.error(f\"{uid}: \" + str(e))\n        return None\n"
          },
          {
            "fileName": "types.py",
            "fileContents": "# The MIT License (MIT)\n# Copyright © 2023 Yuma Rao\n# Copyright © 2024 Manifold Labs\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the “Software”), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\n# the Software.\n\n# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\n# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\n\nfrom enum import Enum\nfrom pydantic import BaseModel\nfrom typing import Any, List, Optional\n\n\nclass Endpoints(Enum):\n    CHAT = \"CHAT\"\n    COMPLETION = \"COMPLETION\"\n\nclass InferenceStats(BaseModel):\n    time_to_first_token: float\n    time_for_all_tokens: float\n    total_time: float\n    tps: float\n    tokens: List[Any]\n    verified: bool\n    error: Optional[str] = None\n    cause: Optional[str] = None\n\nclass OrganicStats(InferenceStats):\n    model: str\n    max_tokens: int\n    seed: int\n    temperature: float\n    uid: int\n    hotkey: str\n    coldkey: str\n    endpoint: str\n    total_tokens: int\n\n"
          },
          {
            "fileName": "updater.py",
            "fileContents": "# The MIT License (MIT)\n# Copyright © 2023 Yuma Rao\n# Copyright © 2024 Manifold Labs\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the “Software”), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\n# the Software.\n\n# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\n# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nimport time\nimport os\nimport sys\nimport requests\nimport bittensor as bt\nimport targon\n\n\ndef autoupdate(branch: str = \"main\", force=False):\n    \"\"\"\n    Automatically updates the Targon codebase to the latest version available on the specified branch.\n\n    This function checks the remote repository for the latest version of Targon by fetching the VERSION file from the specified branch.\n    If the local version is older than the remote version, it performs a git pull to update the local codebase to the latest version.\n    After successfully updating, it restarts the application with the updated code.\n\n    Args:\n    - branch (str): The name of the branch to check for updates. Defaults to \"main\".\n\n    Note:\n    - The function assumes that the local codebase is a git repository and has the same structure as the remote repository.\n    - It requires git to be installed and accessible from the command line.\n    - The function will restart the application using the same command-line arguments it was originally started with.\n    - If the update fails, manual intervention is required to resolve the issue and restart the application.\n    \"\"\"\n    bt.logging.info(\"Checking for updates...\")\n    try:\n        response = requests.get(\n            f\"https://raw.githubusercontent.com/manifold-inc/targon/{branch}/VERSION?token={time.time()}\",\n            headers={\"Cache-Control\": \"no-cache\"},\n        )\n        response.raise_for_status()\n        repo_version = response.content.decode()\n        latest_version = [int(v) for v in repo_version.split(\".\")]\n        local_version = [int(v) for v in targon.__version__.split(\".\")]\n\n        bt.logging.info(f\"Local version: {targon.__version__}\")\n        bt.logging.info(f\"Latest version: {repo_version}\")\n\n        if latest_version > local_version or force:\n            bt.logging.info(\"A newer version of Targon is available. Updating...\")\n            base_path = os.path.abspath(__file__)\n            while os.path.basename(base_path) != \"targon\":\n                base_path = os.path.dirname(base_path)\n            base_path = os.path.dirname(base_path)\n\n            os.system(f\"cd {base_path} && git pull && pip install -r requirements.txt\")\n\n            with open(os.path.join(base_path, \"VERSION\")) as f:\n                new_version = f.read().strip()\n                new_version = [int(v) for v in new_version.split(\".\")]\n\n                if new_version == latest_version:\n                    bt.logging.info(\"Targon updated successfully. Restarting...\")\n                    sys.exit(0)\n                else:\n                    bt.logging.error(\"Update failed. Manual update required.\")\n    except Exception as e:\n        bt.logging.error(f\"Update check failed: {e}\")\n"
          },
          {
            "fileName": "utils.py",
            "fileContents": "import traceback\nimport bittensor as bt\n\n\ndef print_info(metagraph, hotkey, block, isMiner=True):\n    uid = metagraph.hotkeys.index(hotkey)\n    log = f\"UID:{uid} | Block:{block} | Consensus:{metagraph.C[uid]} | \"\n    if isMiner:\n        bt.logging.info(\n            log\n            + f\"Stake:{metagraph.S[uid]} | Trust:{metagraph.T[uid]} | Incentive:{metagraph.I[uid]} | Emission:{metagraph.E[uid]}\"\n        )\n        return\n    bt.logging.info(log + f\"VTrust:{metagraph.Tv[uid]} | \")\n\n\ndef fail_with_none(message: str = \"\"):\n    def outer(func):\n        def inner(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                bt.logging.error(message)\n                bt.logging.error(str(e))\n                bt.logging.error(traceback.format_exc())\n                return None\n\n        return inner\n\n    return outer\n\n\nclass ExitContext:\n    \"\"\"\n    Using this as a class lets us pass this to other threads\n    \"\"\"\n    isExiting: bool = False\n\n    def startExit(self, *_):\n        if self.isExiting:\n            exit()\n        self.isExiting = True\n\n    def __bool__(self):\n        return self.isExiting\n"
          },
          {
            "fileName": "verifier",
            "fileContents": ""
          },
          {
            "fileName": "Dockerfile",
            "fileContents": "FROM python:3.9\nWORKDIR /app\n\nCOPY ./requirements.txt requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY ./verifier.py .\n\nHEALTHCHECK --interval=15s --timeout=5s --start-period=30s --start-interval=30s --retries=15 CMD curl --silent --fail http://localhost/ > /dev/null || exit 1\n\nENTRYPOINT [\"uvicorn\", \"verifier:app\", \"--port\", \"80\", \"--host\", \"0.0.0.0\"]\n"
          },
          {
            "fileName": "requirements.txt",
            "fileContents": "vllm==0.6.2\nfastapi==0.115.0\nopenai==1.44.1\nuvicorn==0.30.6\n"
          },
          {
            "fileName": "verifier.py",
            "fileContents": "import random\nimport math\nimport os\nimport asyncio\nimport traceback\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Tuple\nfrom vllm import LLM, SamplingParams\n\n# Load the model.\nMODEL_NAME = os.getenv(\"MODEL\", None)\nif MODEL_NAME is None:\n    print(\"No model name provided, exiting.\")\n    exit()\n# Constants.\nLOGPROB_LOG_THRESHOLD = 0.65\nLOGPROB_FAILURE_THRESHOLD = 0.75\nTENSOR_PARALLEL = int(os.getenv(\"TENSOR_PARALLEL\", 1))\nMODEL_WRAPPER = LLM(\n    model=MODEL_NAME,\n    enforce_eager=True,\n    gpu_memory_utilization=.9,\n    tensor_parallel_size=TENSOR_PARALLEL,\n)\nTOKENIZER = MODEL_WRAPPER.get_tokenizer()\nMODEL = MODEL_WRAPPER.llm_engine.model_executor.driver_worker.model_runner.model  # type: ignore\nMODEL_NUM_PARAMS = sum(1 for _ in MODEL.parameters())\n\n# Lock to ensure atomicity.\nLOCK = asyncio.Lock()\nLOCK_GENERATE = asyncio.Lock()\n\nENDPOINTS = [\"completion\"]\nif TOKENIZER.chat_template is not None:\n    ENDPOINTS.append(\"chat\")\n\n\nclass RequestParams(BaseModel):\n    messages: Optional[List[Dict[str, str]]] = None\n    prompt: Optional[str] = None\n    temperature: float = 0.0\n    seed: int = 42\n    max_tokens: int\n\n\nclass OutputItem(BaseModel):\n    text: str\n    logprob: float\n    token_id: int\n\n\nclass RequestType(Enum):\n    CHAT = \"CHAT\"\n    COMPLETION = \"COMPLETION\"\n\n\nclass VerificationRequest(BaseModel):\n    request_type: str\n    model: str = MODEL_NAME\n    request_params: RequestParams\n    output_sequence: List[OutputItem]\n\n\nclass RequestSamplingParams(BaseModel):\n    temperature: float = 0.0\n    seed: int = 42\n    max_tokens: int\n\n\nclass GenerateRequest(BaseModel):\n    messages: List[Dict[str, str]]\n    sampling_params: RequestSamplingParams\n\n\napp = FastAPI()\n\n\n@app.post(\"/generate\")\nasync def generate_question(req: GenerateRequest):\n    async with LOCK_GENERATE:\n        try:\n            if \"chat\" in ENDPOINTS:\n                output = (\n                    MODEL_WRAPPER.chat(\n                        messages=req.messages, sampling_params=SamplingParams(**req.sampling_params.model_dump()), use_tqdm=False  # type: ignore\n                    )[0]\n                    .outputs[0]\n                    .text\n                )\n            else:\n                prompt = \"\"\n                for message in req.messages:\n                    prompt += (\n                        message.get(\"role\", \"\")\n                        + \": \"\n                        + message.get(\"content\", \"\")\n                        + \"\\n\"\n                    )\n                prompt += \"\\nResponse: \"\n                output = (\n                    MODEL_WRAPPER.generate(\n                        prompts=prompt,\n                        sampling_params=SamplingParams(\n                            **req.sampling_params.model_dump()\n                        ),\n                        use_tqdm=False,\n                    )[0]\n                    .outputs[0]\n                    .text\n                )\n            return {\"text\": output}\n        except Exception as e:\n            print(\"Failed generate request\", str(e), traceback.format_exc())\n        return {\"text\": None}\n\n\ndef verify_logprobs_random(\n    request: VerificationRequest, input_text: str\n) -> Tuple[bool, str]:\n    \"\"\"\n    Generate a handful of random outputs to ensure the logprobs weren't generated after the fact.\n    \"\"\"\n    indices = list(range(1, len(request.output_sequence) - 1))\n    indices_to_check = list(\n        sorted(\n            [\n                0,  # always check first token\n                len(request.output_sequence) - 1,  # always check last token\n            ]\n            + random.sample(indices, min(len(indices), 3))\n        )\n    )\n\n    # Generate a single token at each index, comparing logprobs.\n    top_logprobs = int(request.request_params.temperature * 10) + 3\n    sampling_params = SamplingParams(\n        temperature=request.request_params.temperature,\n        seed=request.request_params.seed,\n        max_tokens=1,\n        logprobs=top_logprobs,\n    )\n    for idx in indices_to_check:\n        full_text = input_text + \"\".join(\n            [item.text for item in request.output_sequence[0:idx]]\n        )\n        output = MODEL_WRAPPER.generate([full_text], sampling_params, use_tqdm=False)[\n            0\n        ].outputs[0]\n\n        # The miner's output token should be in the logprobs...\n        top_tokens = []\n        if output.logprobs is None:\n            print(\"No log probs to check\")\n            continue\n        for lp in output.logprobs:\n            top_tokens += list(lp.keys())\n        if request.output_sequence[idx].token_id not in top_tokens:\n            message = f\"Token output at index {idx} [{TOKENIZER.decode([request.output_sequence[idx].token_id])}] not found in top {top_logprobs} logprobs: {[TOKENIZER.decode([token]) for token in top_tokens]}\"\n            return False, message\n    return (\n        True,\n        f\"Successfully verified {len(indices_to_check)} random logprobs: {indices_to_check}\",\n    )\n\n\ndef verify_logprobs(\n    request: VerificationRequest, input_text: str, input_tokens: List[int]\n) -> Optional[Tuple[bool, str, str]]:\n    \"\"\"\n    Compare the produced logprob values against the ground truth, or at least\n    the ground truth according to this particular GPU/software pairing.\n    \"\"\"\n\n    # Set up sampling parameters for the \"fast\" check, which just compares input logprobs against output logprobs.\n    top_logprobs = int(request.request_params.temperature * 10) + 6\n    sampling_params = SamplingParams(\n        temperature=request.request_params.temperature,\n        seed=request.request_params.seed,\n        max_tokens=1,\n        logprobs=top_logprobs,\n        prompt_logprobs=top_logprobs,\n    )\n\n    # Generate output for a single token, which will return input logprobs based on prompt_logprobs=1\n    output = None\n    for _ in range(5):\n        full_text = input_text + \"\".join(\n            [item.text for item in request.output_sequence]\n        )\n        output = MODEL_WRAPPER.generate([full_text], sampling_params, use_tqdm=False)[0]\n        if output.prompt_logprobs is not None:\n            break\n\n    if not output or output.prompt_logprobs is None:\n        return None\n\n    # The actual logprobs should be *very* close, but typically not 100% because of GPU/driver/etc. differences.\n    total_score = 0.0\n    idxs = min(\n        len(output.prompt_logprobs) - len(input_tokens) - 3,\n        len(request.output_sequence) - 1,\n    )\n    perfect_tokens = 0\n    eos_token_id = getattr(TOKENIZER, \"eos_token_id\", -1)\n    eot_token_id = TOKENIZER.get_vocab().get(\"<|eot_id|>\", -1)  # type: ignore\n    output_tokens = [item.token_id for item in request.output_sequence]\n    really_low_prob = 0\n    not_first = 0\n    for idx in range(idxs):\n        item = request.output_sequence[idx]\n        expected_logprob = output.prompt_logprobs[idx + len(input_tokens)]\n        assert expected_logprob is not None\n        eos_logprob = expected_logprob.get(eos_token_id)\n        eot_logprob = expected_logprob.get(eot_token_id)\n        if (\n            not eos_logprob\n            and eot_logprob\n            or (\n                eos_logprob\n                and eot_logprob\n                and eot_logprob.rank != None\n                and eos_logprob.rank != None\n                and eot_logprob.rank < eos_logprob.rank\n            )\n        ):\n            eos_logprob = eot_logprob\n        expected_logprob = expected_logprob.get(item.token_id)\n        if eos_logprob and (\n            not expected_logprob\n            or (\n                eos_logprob\n                and expected_logprob.rank != None\n                and eos_logprob.rank != None\n                and eos_logprob.rank < expected_logprob.rank\n                and expected_logprob.rank > 10\n            )\n        ):\n            return False, f\"Expected EOS/EOT token at index {idx}\", \"SKIPPED_EOS_EOT\"\n        if expected_logprob is None:\n            continue\n        rank = expected_logprob.rank\n        assert rank != None\n        if rank >= 75:\n            return (\n                False,\n                f\"Found extraordinarily improbable token '{TOKENIZER.decode([item.token_id])}' at index {idx}: {rank=}\",\n                \"UNLIKELY_TOKEN\",\n            )\n        elif rank >= 25:\n            really_low_prob += 1\n        elif rank > top_logprobs:\n            continue\n        if rank != 1:\n            not_first += 1\n        expected_logprob = expected_logprob.logprob\n        produced_logprob = item.logprob\n        score = 1.0 - min(\n            1.0, abs(math.exp(expected_logprob) - math.exp(produced_logprob))\n        )\n\n        # Prevents over fitting smaller models\n        if produced_logprob == 0:\n            perfect_tokens += 1\n\n        # To accomodate architectural difference and such, we'll give a perfect score if >= 0.9\n        if score >= 0.9:\n            score = 1.0\n\n        # Logprobs rarely match well for high temps so we can use rank instead.\n        if (\n            rank == 1\n            and request.request_params.temperature >= 0.9\n            and produced_logprob != 0\n        ):\n            score = 1.0\n\n        total_score += score\n\n    # Check if miner produced non-top ranking tokens more than top-ranking tokens.\n    ratio = not_first / len(output_tokens)\n    if ratio >= 0.5:\n        return (\n            False,\n            f\"{not_first} of {len(output_tokens)} [{ratio=}] tokens were not rank 1.\",\n            \"UNLIKELY_TOKENS\",\n        )\n\n    # Check if miner prematurely stopped generating, meaning the single output token generated\n    # from the \"throwaway\" above was NOT an EOS/EOT token.\n    if eos_token_id > 0 or eot_token_id > 0:\n        if len(output_tokens) < request.request_params.max_tokens:\n            last_token_probs = []\n            if output:\n                last_token_probs = output.outputs[0]\n                last_token_probs = (\n                    last_token_probs.logprobs[0]\n                    if last_token_probs and last_token_probs.logprobs\n                    else []\n                )\n            if (\n                eos_token_id not in last_token_probs\n                and eot_token_id not in last_token_probs\n                and len(last_token_probs) != 0\n            ):\n                return (\n                    False,\n                    \"Premature end of generation, EOS/EOT unlikely after last token.\",\n                    \"EARLY_END\",\n                )\n\n    # Calculate average score.\n    average_score = round(total_score / idxs, 5)\n    passes = average_score >= LOGPROB_FAILURE_THRESHOLD\n    perfect_avg = round(perfect_tokens / idxs, 5)\n    if passes and perfect_avg >= (\n        1 - min(request.request_params.temperature * 0.5, 0.6)\n    ):\n        return False, f\"Overfitted response tokens. {perfect_avg}% perfect\", \"OVERFIT\"\n    if really_low_prob >= 5:\n        return (\n            False,\n            f\"Found {really_low_prob} highly improbable tokens.\",\n            \"UNLIKELY_TOKEN\",\n        )\n\n    return True, \"\", \"\"\n\n\n@app.post(\"/verify\")\nasync def verify(request: VerificationRequest) -> Dict:\n    \"\"\"Verify a miner's output.\"\"\"\n\n    # If the miner didn't return any outputs, fail.\n    if len(request.output_sequence) < 3:\n        return {\n            \"verified\": False,\n            \"error\": \"Output sequence too short!\",\n            \"cause\": \"TOO_SHORT\",\n        }\n    if (\n        request.request_params.max_tokens\n        and len(request.output_sequence) > request.request_params.max_tokens\n    ):\n        return {\n            \"verified\": False,\n            \"error\": f\"Too many tokens produced: {request.request_params.max_tokens} < {len(request.output_sequence)}\",\n            \"cause\": \"TOO_LONG\",\n        }\n    if request.model != MODEL_NAME:\n        return {\n            \"error\": f\"Unable to verify model={request.model}, since we are using {MODEL_NAME}\",\n            \"cause\": \"INTERNAL_ERROR\",\n        }\n\n    # Tokenize the input sequence.\n    input_text = (\n        request.request_params.prompt\n        if request.request_type == RequestType.COMPLETION.value\n        else TOKENIZER.apply_chat_template(\n            request.request_params.messages,  # type: ignore\n            tokenize=False,\n            add_special_tokens=False,\n            add_generation_prompt=True,\n        )\n    )\n    assert isinstance(input_text, str)\n    if hasattr(TOKENIZER, \"bos_token\"):\n        if input_text.startswith(TOKENIZER.bos_token):  # type: ignore\n            input_text = input_text[len(TOKENIZER.bos_token) :]  # type: ignore\n    input_tokens = TOKENIZER(input_text).input_ids\n\n    # Verify!\n    async with LOCK:\n        return_value = {\n            \"verified\": False,\n            \"error\": None,\n        }\n\n        # Logprob checks.\n        res = verify_logprobs(request, str(input_text), input_tokens)\n        if res is None:\n            return {\"error\": \"Failed to check log probs\", \"cause\": \"INTERNAL_ERROR\"}\n        result, message, cause = res\n        return_value.update(\n            {\n                \"verified\": result,\n                \"cause\": cause,\n                \"error\": message,\n            }\n        )\n        if not result:\n            return return_value\n\n        # Random logprob check.\n        if request.request_params.temperature > 0.75:\n            return {\"verified\": True}\n\n        res = verify_logprobs_random(request, str(input_text))\n        if res is None:\n            return {\n                \"error\": \"Failed to check log probs\",\n                \"cause\": \"INTERNAL_ERROR\",\n            }\n        result, message = res\n        return_value.update(\n            {\n                \"verified\": result,\n                \"cause\": \"LOGPROB_RANDOM\",\n                \"error\": message,\n            }\n        )\n        if not result:\n            return return_value\n\n        return {\"verified\": True}\n\n\n@app.get(\"/endpoints\")\ndef endpoints():\n    return ENDPOINTS\n\n\n@app.get(\"/\")\ndef ping():\n    return \"\", 200\n"
          }
        ],
        "repo_data": {
          "id": 697482210,
          "node_id": "R_kgDOKZK74g",
          "name": "targon",
          "full_name": "manifold-inc/targon",
          "private": false,
          "owner": {
            "login": "manifold-inc",
            "id": 146253941,
            "node_id": "O_kgDOCLeodQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/146253941?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/manifold-inc",
            "html_url": "https://github.com/manifold-inc",
            "followers_url": "https://api.github.com/users/manifold-inc/followers",
            "following_url": "https://api.github.com/users/manifold-inc/following{/other_user}",
            "gists_url": "https://api.github.com/users/manifold-inc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/manifold-inc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/manifold-inc/subscriptions",
            "organizations_url": "https://api.github.com/users/manifold-inc/orgs",
            "repos_url": "https://api.github.com/users/manifold-inc/repos",
            "events_url": "https://api.github.com/users/manifold-inc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/manifold-inc/received_events",
            "type": "Organization",
            "user_view_type": "public",
            "site_admin": false
          },
          "html_url": "https://github.com/manifold-inc/targon",
          "description": "A library for building subnets with the manifold reward stack",
          "fork": false,
          "url": "https://api.github.com/repos/manifold-inc/targon",
          "forks_url": "https://api.github.com/repos/manifold-inc/targon/forks",
          "keys_url": "https://api.github.com/repos/manifold-inc/targon/keys{/key_id}",
          "collaborators_url": "https://api.github.com/repos/manifold-inc/targon/collaborators{/collaborator}",
          "teams_url": "https://api.github.com/repos/manifold-inc/targon/teams",
          "hooks_url": "https://api.github.com/repos/manifold-inc/targon/hooks",
          "issue_events_url": "https://api.github.com/repos/manifold-inc/targon/issues/events{/number}",
          "events_url": "https://api.github.com/repos/manifold-inc/targon/events",
          "assignees_url": "https://api.github.com/repos/manifold-inc/targon/assignees{/user}",
          "branches_url": "https://api.github.com/repos/manifold-inc/targon/branches{/branch}",
          "tags_url": "https://api.github.com/repos/manifold-inc/targon/tags",
          "blobs_url": "https://api.github.com/repos/manifold-inc/targon/git/blobs{/sha}",
          "git_tags_url": "https://api.github.com/repos/manifold-inc/targon/git/tags{/sha}",
          "git_refs_url": "https://api.github.com/repos/manifold-inc/targon/git/refs{/sha}",
          "trees_url": "https://api.github.com/repos/manifold-inc/targon/git/trees{/sha}",
          "statuses_url": "https://api.github.com/repos/manifold-inc/targon/statuses/{sha}",
          "languages_url": "https://api.github.com/repos/manifold-inc/targon/languages",
          "stargazers_url": "https://api.github.com/repos/manifold-inc/targon/stargazers",
          "contributors_url": "https://api.github.com/repos/manifold-inc/targon/contributors",
          "subscribers_url": "https://api.github.com/repos/manifold-inc/targon/subscribers",
          "subscription_url": "https://api.github.com/repos/manifold-inc/targon/subscription",
          "commits_url": "https://api.github.com/repos/manifold-inc/targon/commits{/sha}",
          "git_commits_url": "https://api.github.com/repos/manifold-inc/targon/git/commits{/sha}",
          "comments_url": "https://api.github.com/repos/manifold-inc/targon/comments{/number}",
          "issue_comment_url": "https://api.github.com/repos/manifold-inc/targon/issues/comments{/number}",
          "contents_url": "https://api.github.com/repos/manifold-inc/targon/contents/{+path}",
          "compare_url": "https://api.github.com/repos/manifold-inc/targon/compare/{base}...{head}",
          "merges_url": "https://api.github.com/repos/manifold-inc/targon/merges",
          "archive_url": "https://api.github.com/repos/manifold-inc/targon/{archive_format}{/ref}",
          "downloads_url": "https://api.github.com/repos/manifold-inc/targon/downloads",
          "issues_url": "https://api.github.com/repos/manifold-inc/targon/issues{/number}",
          "pulls_url": "https://api.github.com/repos/manifold-inc/targon/pulls{/number}",
          "milestones_url": "https://api.github.com/repos/manifold-inc/targon/milestones{/number}",
          "notifications_url": "https://api.github.com/repos/manifold-inc/targon/notifications{?since,all,participating}",
          "labels_url": "https://api.github.com/repos/manifold-inc/targon/labels{/name}",
          "releases_url": "https://api.github.com/repos/manifold-inc/targon/releases{/id}",
          "deployments_url": "https://api.github.com/repos/manifold-inc/targon/deployments",
          "created_at": "2023-09-27T20:23:15Z",
          "updated_at": "2025-01-22T17:41:40Z",
          "pushed_at": "2025-01-30T03:07:41Z",
          "git_url": "git://github.com/manifold-inc/targon.git",
          "ssh_url": "git@github.com:manifold-inc/targon.git",
          "clone_url": "https://github.com/manifold-inc/targon.git",
          "svn_url": "https://github.com/manifold-inc/targon",
          "homepage": null,
          "size": 11356,
          "stargazers_count": 36,
          "watchers_count": 36,
          "language": "Python",
          "has_issues": true,
          "has_projects": true,
          "has_downloads": true,
          "has_wiki": true,
          "has_pages": false,
          "has_discussions": false,
          "forks_count": 35,
          "mirror_url": null,
          "archived": false,
          "disabled": false,
          "open_issues_count": 3,
          "license": {
            "key": "mit",
            "name": "MIT License",
            "spdx_id": "MIT",
            "url": "https://api.github.com/licenses/mit",
            "node_id": "MDc6TGljZW5zZTEz"
          },
          "allow_forking": true,
          "is_template": false,
          "web_commit_signoff_required": false,
          "topics": [],
          "visibility": "public",
          "forks": 35,
          "open_issues": 3,
          "watchers": 36,
          "default_branch": "main",
          "permissions": {
            "admin": false,
            "maintain": false,
            "push": false,
            "triage": false,
            "pull": true
          },
          "temp_clone_token": "",
          "custom_properties": {},
          "organization": {
            "login": "manifold-inc",
            "id": 146253941,
            "node_id": "O_kgDOCLeodQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/146253941?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/manifold-inc",
            "html_url": "https://github.com/manifold-inc",
            "followers_url": "https://api.github.com/users/manifold-inc/followers",
            "following_url": "https://api.github.com/users/manifold-inc/following{/other_user}",
            "gists_url": "https://api.github.com/users/manifold-inc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/manifold-inc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/manifold-inc/subscriptions",
            "organizations_url": "https://api.github.com/users/manifold-inc/orgs",
            "repos_url": "https://api.github.com/users/manifold-inc/repos",
            "events_url": "https://api.github.com/users/manifold-inc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/manifold-inc/received_events",
            "type": "Organization",
            "user_view_type": "public",
            "site_admin": false
          },
          "network_count": 35,
          "subscribers_count": 9
        },
        "config": {
          "limit": 50,
          "batch_size": 10
        }
      }
    ]
  },
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "3d188d5e6776608372babb5a38f0e018bf857063e63b63b6556840b24e736e3d"
  }
}